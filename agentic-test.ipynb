{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search\n",
    "pip install -qU faiss-cpu pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Environmental Varaibales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLAMA_MODEL: llama3.2\n",
      "LANGCHAIN_PROJECT: AIE1 - LangGraph - 266127a1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "from uuid import uuid4\n",
    "import os\n",
    "\n",
    "# Memuat variabel dari file .env\n",
    "load_dotenv()\n",
    "\n",
    "# Konfigurasi untuk Llama 3.2\n",
    "os.environ[\"LLAMA_MODEL\"] = \"llama3.2\" \n",
    "\n",
    "# Konfigurasi LangChain\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - LangGraph - {uuid4().hex[0:8]}\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\", \"\")\n",
    "\n",
    "print(\"LLAMA_MODEL:\", os.environ[\"LLAMA_MODEL\"])\n",
    "print(\"LANGCHAIN_PROJECT:\", os.environ[\"LANGCHAIN_PROJECT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a Simple Retrieval Chain using LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\62812\\AppData\\Local\\Temp\\ipykernel_12864\\1638102623.py:18: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "# Load the document pertaining to a particular topic\n",
    "docs = ArxivLoader(query=\"Retrieval Augmented Generation\", load_max_docs=5).load()\n",
    "\n",
    "# Split the document into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")   \n",
    "\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Instantiate the Embedding Model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Create Index- Load document chunks into the vectorstore\n",
    "faiss_vectorstore = FAISS.from_documents(\n",
    "    documents=chunked_documents,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = faiss_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate RAG prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I don't know'.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\62812\\AppData\\Local\\Temp\\ipykernel_12864\\1979328059.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llama_chat_model = ChatOllama(model=\"llama3.2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llama_chat_model = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LCEL RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(itemgetter('question'))\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F7803C6B40>, search_kwargs={}),\n",
       "  question: RunnableLambda(itemgetter('question'))\n",
       "}\n",
       "| RunnableAssign(mapper={\n",
       "    context: RunnableLambda(itemgetter('context'))\n",
       "  })\n",
       "| {\n",
       "    response: ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I don't know'.\\n\\nQuestion:\\n{question}\\n\\nContext:\\n{context}\\n\"), additional_kwargs={})])\n",
       "              | ChatOllama(model='llama3.2'),\n",
       "    context: RunnableLambda(itemgetter('context'))\n",
       "  }"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "# from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Gunakan model LLaMA 3.2 melalui Ollama\n",
    "llama_chat_model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "retrieval_augmented_generation_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\n",
    "        \"response\": rag_prompt | llama_chat_model,\n",
    "        \"context\": itemgetter(\"context\")\n",
    "    }\n",
    ")\n",
    "\n",
    "retrieval_augmented_generation_chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': AIMessage(content='Retrieval Augmented Generation is a text generation paradigm that combines deep learning technology with traditional retrieval technology. It has achieved state-of-the-art performance in many NLP tasks by explicitly acquiring knowledge in a plug-and-play manner, leading to scalability and potentially alleviating the difficulty of text generation. It involves generating text from retrieved human-written references rather than generating from scratch.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 2186, 'total_tokens': 2259}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}),\n",
       " 'context': [Document(metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}, page_content='grating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications\\nof retrieval-augmented generation in other genera-\\ntion tasks such as abstractive summarization (Peng\\net al., 2019), code generation (Hashimoto et al.,\\n2018), paraphrase (Kazemnejad et al., 2020; Su\\net al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2\\nRetrieval-Augmented Paradigm\\nIn this section, we ﬁrst give a general formulation\\nof retrieval-augmented text generation. Then, we\\ndiscuss three major components of the retrieval-\\naugmented generation paradigm, including the re-\\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\\nInput\\nSources \\n(Sec. 2.2):\\nTraining \\nCorpus\\nExternal Data\\nUnsupervised \\nData\\nMetrics\\n(Sec. 2.3):\\nSparse-vector \\nRetrieval\\nDense-vector \\nRetrieval\\nTask-specific \\nRetrieval\\nRetrieval Memory\\nGeneration Model\\nSec. 4: Machine \\nTranslation\\nSec. 5: Other \\nTasks\\nData \\nAugmentation\\nAttention \\nMechanism\\nSkeleton & \\nTemplates\\nInformation Retrieval'),\n",
       "  Document(metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}, page_content='augmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about\\nretrieval-augmented generation, which are orga-\\nnized with respect to different tasks. Speciﬁcally,\\non the dialogue response generation task, exem-\\nplar/template retrieval as an intermediate step has\\nbeen shown beneﬁcial to informative response gen-\\neration (Weston et al., 2018; Wu et al., 2019; Cai\\net al., 2019a,b). In addition, there has been growing\\ninterest in knowledge-grounded generation explor-\\ning different forms of knowledge such as knowl-\\nedge bases and external documents (Dinan et al.,\\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\\n2021). On the machine translation task, we summa-\\nrize the early work on how the retrieved sentences\\n(called translation memory) are used to improve\\nstatistical machine translation (SMT) (Koehn et al.,\\n2003) models (Simard and Isabelle, 2009; Koehn\\nand Senellart, 2010) and in particular, we inten-\\nsively highlight several popular methods to inte-\\ngrating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications'),\n",
       "  Document(metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}, page_content='recent methods to facilitate future research.\\n1\\nIntroduction\\nRetrieval-augmented text generation, as a new\\ntext generation paradigm that fuses emerging deep\\nlearning technology and traditional retrieval tech-\\nnology, has achieved state-of-the-art (SOTA) per-\\nformance in many NLP tasks and attracted the at-\\ntention of the computational linguistics community\\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\\n2021). Compared with generation-based counter-\\npart, this new paradigm has some remarkable ad-\\nvantages: 1) The knowledge is not necessary to be\\nimplicitly stored in model parameters, but is explic-\\nitly acquired in a plug-and-play manner, leading\\nto great scalibility; 2) Instead of generating from\\nscratch, the paradigm generating text from some re-\\ntrieved human-written reference, which potentially\\nalleviates the difﬁculty of text generation.\\nThis paper aims to review many representative\\napproaches for retrieval-augmented text generation\\ntasks including dialogue response generation (We-\\nston et al., 2018), machine translation (Gu et al.,\\n2018) and others (Hashimoto et al., 2018). We\\n∗All authors contributed equally.\\nﬁrstly present the generic paradigm of retrieval-\\naugmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about'),\n",
       "  Document(metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}, page_content='A Survey on Retrieval-Augmented Text Generation\\nHuayang Li♥,∗\\nYixuan Su♠,∗\\nDeng Cai♦,∗\\nYan Wang♣,∗\\nLemao Liu♣,∗\\n♥Nara Institute of Science and Technology\\n♠University of Cambridge\\n♦The Chinese University of Hong Kong\\n♣Tencent AI Lab\\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\\nthisisjcykcd@gmail.com, brandenwang@tencent.com\\nlemaoliu@gmail.com\\nAbstract\\nRecently, retrieval-augmented text generation\\nattracted increasing attention of the compu-\\ntational linguistics community.\\nCompared\\nwith conventional generation models, retrieval-\\naugmented text generation has remarkable ad-\\nvantages and particularly has achieved state-of-\\nthe-art performance in many NLP tasks. This\\npaper aims to conduct a survey about retrieval-\\naugmented text generation. It ﬁrstly highlights\\nthe generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable ap-\\nproaches according to different tasks including\\ndialogue response generation, machine trans-\\nlation, and other generation tasks. Finally, it\\npoints out some promising directions on top of\\nrecent methods to facilitate future research.\\n1\\nIntroduction\\nRetrieval-augmented text generation, as a new\\ntext generation paradigm that fuses emerging deep\\nlearning technology and traditional retrieval tech-')]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import AIMessage, Document\n",
    "\n",
    "await retrieval_augmented_generation_chain.ainvoke({\"question\" : \"What is Retrieval Augmented Generation?\"})\n",
    "\n",
    "##################### REPONSE #############################\n",
    "{'response': AIMessage(content='Retrieval Augmented Generation is a text generation paradigm that combines deep learning technology with traditional retrieval technology. It has achieved state-of-the-art performance in many NLP tasks by explicitly acquiring knowledge in a plug-and-play manner, leading to scalability and potentially alleviating the difficulty of text generation. It involves generating text from retrieved human-written references rather than generating from scratch.', response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 2186, 'total_tokens': 2259}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}),\n",
    " 'context': [Document(page_content='grating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications\\nof retrieval-augmented generation in other genera-\\ntion tasks such as abstractive summarization (Peng\\net al., 2019), code generation (Hashimoto et al.,\\n2018), paraphrase (Kazemnejad et al., 2020; Su\\net al., 2021b), and knowledge-intensive generation\\n(Lewis et al., 2020b). Finally, we also point out\\nsome promising directions on retrieval-augmented\\ngeneration to push forward the future research.\\n2\\nRetrieval-Augmented Paradigm\\nIn this section, we ﬁrst give a general formulation\\nof retrieval-augmented text generation. Then, we\\ndiscuss three major components of the retrieval-\\naugmented generation paradigm, including the re-\\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\\nInput\\nSources \\n(Sec. 2.2):\\nTraining \\nCorpus\\nExternal Data\\nUnsupervised \\nData\\nMetrics\\n(Sec. 2.3):\\nSparse-vector \\nRetrieval\\nDense-vector \\nRetrieval\\nTask-specific \\nRetrieval\\nRetrieval Memory\\nGeneration Model\\nSec. 4: Machine \\nTranslation\\nSec. 5: Other \\nTasks\\nData \\nAugmentation\\nAttention \\nMechanism\\nSkeleton & \\nTemplates\\nInformation Retrieval', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}),\n",
    "  Document(page_content='augmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about\\nretrieval-augmented generation, which are orga-\\nnized with respect to different tasks. Speciﬁcally,\\non the dialogue response generation task, exem-\\nplar/template retrieval as an intermediate step has\\nbeen shown beneﬁcial to informative response gen-\\neration (Weston et al., 2018; Wu et al., 2019; Cai\\net al., 2019a,b). In addition, there has been growing\\ninterest in knowledge-grounded generation explor-\\ning different forms of knowledge such as knowl-\\nedge bases and external documents (Dinan et al.,\\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\\n2021). On the machine translation task, we summa-\\nrize the early work on how the retrieved sentences\\n(called translation memory) are used to improve\\nstatistical machine translation (SMT) (Koehn et al.,\\n2003) models (Simard and Isabelle, 2009; Koehn\\nand Senellart, 2010) and in particular, we inten-\\nsively highlight several popular methods to inte-\\ngrating translation memory to NMT models (Gu\\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\\nHe et al., 2021). We also review the applications', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}),\n",
    "  Document(page_content='recent methods to facilitate future research.\\n1\\nIntroduction\\nRetrieval-augmented text generation, as a new\\ntext generation paradigm that fuses emerging deep\\nlearning technology and traditional retrieval tech-\\nnology, has achieved state-of-the-art (SOTA) per-\\nformance in many NLP tasks and attracted the at-\\ntention of the computational linguistics community\\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\\n2021). Compared with generation-based counter-\\npart, this new paradigm has some remarkable ad-\\nvantages: 1) The knowledge is not necessary to be\\nimplicitly stored in model parameters, but is explic-\\nitly acquired in a plug-and-play manner, leading\\nto great scalibility; 2) Instead of generating from\\nscratch, the paradigm generating text from some re-\\ntrieved human-written reference, which potentially\\nalleviates the difﬁculty of text generation.\\nThis paper aims to review many representative\\napproaches for retrieval-augmented text generation\\ntasks including dialogue response generation (We-\\nston et al., 2018), machine translation (Gu et al.,\\n2018) and others (Hashimoto et al., 2018). We\\n∗All authors contributed equally.\\nﬁrstly present the generic paradigm of retrieval-\\naugmented generation as well as three key com-\\nponents under this paradigm, which are retrieval\\nsources, retrieval metrics and generation models.\\nThen, we introduce notable methods about', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'}),\n",
    "  Document(page_content='A Survey on Retrieval-Augmented Text Generation\\nHuayang Li♥,∗\\nYixuan Su♠,∗\\nDeng Cai♦,∗\\nYan Wang♣,∗\\nLemao Liu♣,∗\\n♥Nara Institute of Science and Technology\\n♠University of Cambridge\\n♦The Chinese University of Hong Kong\\n♣Tencent AI Lab\\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\\nthisisjcykcd@gmail.com, brandenwang@tencent.com\\nlemaoliu@gmail.com\\nAbstract\\nRecently, retrieval-augmented text generation\\nattracted increasing attention of the compu-\\ntational linguistics community.\\nCompared\\nwith conventional generation models, retrieval-\\naugmented text generation has remarkable ad-\\nvantages and particularly has achieved state-of-\\nthe-art performance in many NLP tasks. This\\npaper aims to conduct a survey about retrieval-\\naugmented text generation. It ﬁrstly highlights\\nthe generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable ap-\\nproaches according to different tasks including\\ndialogue response generation, machine trans-\\nlation, and other generation tasks. Finally, it\\npoints out some promising directions on top of\\nrecent methods to facilitate future research.\\n1\\nIntroduction\\nRetrieval-augmented text generation, as a new\\ntext generation paradigm that fuses emerging deep\\nlearning technology and traditional retrieval tech-', metadata={'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\\nof the computational linguistics community. Compared with conventional\\ngeneration models, retrieval-augmented text generation has remarkable\\nadvantages and particularly has achieved state-of-the-art performance in many\\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\\ngeneration, and then it reviews notable approaches according to different tasks\\nincluding dialogue response generation, machine translation, and other\\ngeneration tasks. Finally, it points out some important directions on top of\\nrecent methods to facilitate future research.'})]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_belt = [\n",
    "    DuckDuckGoSearchRun(),\n",
    "    ArxivQueryRun()\n",
    "]\n",
    "\n",
    "tool_executor = ToolNode(tools=tool_belt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Llama Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, atau Artificial Intelligence (Inteligensi Buatan), adalah teknologi yang digunakan untuk membuat komputer dapat berpikir dan berperilaku seperti manusia. AI menggunakan algoritma dan data untuk mencapai tujuan tertentu, seperti mengenali wajah, mengerjakan tugas-tugas kompleks, atau bahkan berkomunikasi dengan manusia.\n",
      "\n",
      "AI telah berkembang pesat dalam beberapa dekade terakhir dan telah menyebar ke berbagai bidang, termasuk:\n",
      "\n",
      "1. **Pengenalan Wajah**: AI digunakan untuk mengenali wajah dan mengklasifikasikan orang-orang.\n",
      "2. **Robotik**: AI digunakan untuk membuat robot yang dapat bergerak dan berinteraksi dengan lingkungan sekitar.\n",
      "3. **Perawatan Kesehatan**: AI digunakan untuk menganalisis data kesehatan dan memberikan saran perawatan.\n",
      "4. **Pengembangan Perangkat Lunak**: AI digunakan untuk membuat perangkat lunak yang dapat berpikir dan berinteraksi dengan manusia.\n",
      "\n",
      "Beberapa contoh aplikasi AI yang populer adalah:\n",
      "\n",
      "1. Siri dan Google Assistant\n",
      "2. Virtual Assistant (VA) seperti Alexa dan Cortana\n",
      "3. Model PREDIKSI MELIHIT (ML) untuk memprediksi cuaca dan harga saham\n",
      "4. Aplikasi pengenalan wajah untuk keamanan dan keamanan\n",
      "\n",
      "Namun, perlu diingat bahwa AI masih dalam tahap perkembangan dan memiliki potensi untuk mengubah dunia kita.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "# Inisialisasi model Ollama dengan LLaMA 3.2\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "# Konversi tools ke format OpenAI (jika ingin tetap digunakan)\n",
    "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
    "\n",
    "# Buat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Chain model dengan prompt\n",
    "chain = prompt | model\n",
    "\n",
    "# Fungsi untuk memproses permintaan\n",
    "def process_request(user_input):\n",
    "    response = chain.invoke({\"input\": user_input})\n",
    "    return response.content  # Mengambil teks hasil respon\n",
    "\n",
    "# Contoh penggunaan\n",
    "result = process_request(\"Apa itu AI?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leverage LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "  messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "def call_model(state):\n",
    "  messages = state[\"messages\"]\n",
    "  response = model.invoke(messages)\n",
    "  return {\"messages\" : [response]}\n",
    "\n",
    "def call_tool(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  action = ToolInvocation(\n",
    "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "      tool_input=json.loads(\n",
    "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "      )\n",
    "  )\n",
    "\n",
    "  response = tool_executor.invoke(action)\n",
    "\n",
    "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "\n",
    "  return {\"messages\" : [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': RunnableLambda(call_model), 'action': RunnableLambda(call_tool)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "workflow.nodes\n",
    "########################RESPONSE############################\n",
    "{'agent': RunnableLambda(call_model), 'action': RunnableLambda(call_tool)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f78093d7f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Conditional Edge for Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f78093d7f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def should_continue(state):\n",
    "  last_message = state[\"messages\"][-1]\n",
    "\n",
    "  if \"function_call\" not in last_message.additional_kwargs:\n",
    "    return \"end\"\n",
    "\n",
    "  return \"continue\"\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\" : \"action\",\n",
    "        \"end\" : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally connect the conditional edge to the agent node and action node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f78093d7f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(\"action\", \"agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"nodes\": {\n",
      "    \"__start__\": \"<langgraph.pregel.read.PregelNode object at 0x000001F780644F50>\",\n",
      "    \"agent\": \"<langgraph.pregel.read.PregelNode object at 0x000001F780644E60>\",\n",
      "    \"action\": \"<langgraph.pregel.read.PregelNode object at 0x000001F780644830>\"\n",
      "  },\n",
      "  \"channels\": {\n",
      "    \"messages\": \"<langgraph.channels.binop.BinaryOperatorAggregate object at 0x000001F78088A600>\",\n",
      "    \"__start__\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F78092E100>\",\n",
      "    \"agent\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F780630C00>\",\n",
      "    \"action\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F78092B340>\",\n",
      "    \"branch:__start__:__self__:agent\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F78092BE00>\",\n",
      "    \"branch:__start__:__self__:action\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F78092B7C0>\",\n",
      "    \"branch:agent:__self__:agent\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F78092A680>\",\n",
      "    \"branch:agent:__self__:action\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F780928740>\",\n",
      "    \"branch:action:__self__:agent\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F78092A700>\",\n",
      "    \"branch:action:__self__:action\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F78092ABC0>\",\n",
      "    \"start:agent\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F78092A380>\",\n",
      "    \"branch:agent:should_continue:action\": \"<langgraph.channels.ephemeral_value.EphemeralValue object at 0x000001F78092A140>\"\n",
      "  },\n",
      "  \"stream_mode\": \"updates\",\n",
      "  \"stream_eager\": false,\n",
      "  \"output_channels\": [\n",
      "    \"messages\"\n",
      "  ],\n",
      "  \"stream_channels\": [\n",
      "    \"messages\"\n",
      "  ],\n",
      "  \"interrupt_after_nodes\": [],\n",
      "  \"interrupt_before_nodes\": [],\n",
      "  \"input_channels\": \"__start__\",\n",
      "  \"step_timeout\": null,\n",
      "  \"debug\": false,\n",
      "  \"checkpointer\": null,\n",
      "  \"store\": null,\n",
      "  \"retry_policy\": null,\n",
      "  \"config_type\": null,\n",
      "  \"config\": null,\n",
      "  \"name\": \"LangGraph\",\n",
      "  \"builder\": \"<langgraph.graph.state.StateGraph object at 0x000001F78093D7F0>\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Compile workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "# Cetak isi dari app dengan format JSON untuk kejelasan\n",
    "print(json.dumps(app.__dict__, indent=2, default=str))\n",
    "\n",
    "# ################### RESPONSE ###############################\n",
    "# CompiledGraph(nodes={'agent': ChannelInvoke(bound=RunnableLambda(call_model)\n",
    "# | ChannelWrite(channels=[ChannelWriteEntry(channel='agent', value=None, skip_none=False), ChannelWriteEntry(channel='messages', value=RunnableLambda(...), skip_none=False)]), config={'tags': []}, channels={'messages': 'messages'}, triggers=['agent:inbox'], mapper=functools.partial(<function _coerce_state at 0x7de64d4c9ab0>, <class '__main__.AgentState'>)), 'action': ChannelInvoke(bound=RunnableLambda(call_tool)\n",
    "# | ChannelWrite(channels=[ChannelWriteEntry(channel='action', value=None, skip_none=False), ChannelWriteEntry(channel='messages', value=RunnableLambda(...), skip_none=False)]), config={'tags': []}, channels={'messages': 'messages'}, triggers=['action:inbox'], mapper=functools.partial(<function _coerce_state at 0x7de64d4c9ab0>, <class '__main__.AgentState'>)), 'agent:edges': ChannelInvoke(bound=RunnableLambda(runnable), config={'tags': ['langsmith:hidden']}, channels={'messages': 'messages'}, triggers=['agent']), 'action:edges': ChannelInvoke(bound=ChannelWrite(channels=[ChannelWriteEntry(channel='agent:inbox', value='action', skip_none=True)]), config={'tags': ['langsmith:hidden']}, channels={'messages': 'messages'}, triggers=['action']), '__start__': ChannelInvoke(bound=ChannelWrite(channels=[ChannelWriteEntry(channel='__start__', value=None, skip_none=False), ChannelWriteEntry(channel='messages', value=RunnableLambda(...), skip_none=False)]), config={'tags': ['langsmith:hidden']}, channels={None: '__start__:inbox'}, triggers=['__start__:inbox']), '__start__:edges': ChannelInvoke(bound=ChannelWrite(channels=[ChannelWriteEntry(channel='agent:inbox', value=None, skip_none=False)]), config={'tags': ['langsmith:hidden']}, channels={'messages': 'messages'}, triggers=['__start__'])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7de64de19e70>, 'agent:inbox': <langgraph.channels.any_value.AnyValue object at 0x7de64de19fc0>, 'action:inbox': <langgraph.channels.any_value.AnyValue object at 0x7de64de1a080>, '__start__:inbox': <langgraph.channels.any_value.AnyValue object at 0x7de64de1ad10>, 'agent': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7de64de1b2b0>, 'action': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7de64de1b880>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7de64de18340>, '__end__': <langgraph.channels.last_value.LastValue object at 0x7de64de18490>, <ReservedChannels.is_last_step: 'is_last_step'>: <langgraph.channels.last_value.LastValue object at 0x7de64de1b670>}, output='__end__', hidden=['agent:inbox', 'action:inbox', '__start__', 'messages'], snapshot_channels=['messages'], input='__start__:inbox', graph=<langgraph.graph.state.StateGraph object at 0x7de64de18880>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the LangGraph- Ask Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?', additional_kwargs={}, response_metadata={}), AIMessage(content='In the context of Large Language Models (LLMs), RAG stands for \"Ragdoll\" which is an acronym for a popular library used to build and train LLMs. Ragdoll is an open-source, lightweight framework designed specifically for training large-scale language models.\\n\\nRagdoll was first released in 2021 by Meta AI researchers, making it one of the earliest and most influential libraries in the field of LLM development.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-01-31T04:07:20.5316433Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1146087700, 'load_duration': 20375800, 'prompt_eval_count': 45, 'prompt_eval_duration': 56000000, 'eval_count': 89, 'eval_duration': 1067000000}, id='run-c371b825-3e58-4500-9afe-a907ab04422f-0')]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"RAG in the context of Large Language Models\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 171, 'total_tokens': 196}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'function_call', 'logprobs': None}),\n",
       "  FunctionMessage(content=\"Large language models (LLMs) are incredibly powerful tools for processing and generating text. However, they inherently struggle to understand the broader context of information, especially when dealing with lengthy conversations or complex tasks. This is where large context windows and Retrieval-Augmented Generation (RAG) come into play. These advanced, generalized language models are trained on vast datasets, enabling them to understand and generate human-like text. In the context of RAG, LLMs are used to generate fully formed responses based on the user query and contextual information retrieved from the vector DBs during user queries. Querying In the rapidly evolving landscape of language models, the debate between Retrieval-Augmented Generation (RAG) and Long Context Large Language Models (LLMs) has garnered significant attention. Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM's internal representation of information. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has ... RAG stands for R etrieval- A ugmented G eneration. RAG enables large language models (LLM) to access and utilize up-to-date information. Hence, it improves the quality of relevance of the response from LLM. Below is a simple diagram of how RAG is implemented.\", additional_kwargs={}, response_metadata={}, name='duckduckgo_search'),\n",
       "  AIMessage(content=\"RAG stands for Retrieval-Augmented Generation in the context of Large Language Models (LLMs). It is an AI framework that improves the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM's internal representation of information. RAG enables LLMs to access and utilize up-to-date information, thereby improving the relevance and quality of the responses generated by the model. RAG broke onto the scene in the rapidly evolving landscape of language models as a way to enhance the capabilities of LLMs in understanding and generating human-like text.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 491, 'total_tokens': 608}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
    "\n",
    "response = app.invoke(inputs)\n",
    "print(response)\n",
    "############################RESPOSNE #########################\n",
    "{'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?'),\n",
    "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"RAG in the context of Large Language Models\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 171, 'total_tokens': 196}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'function_call', 'logprobs': None}),\n",
    "  FunctionMessage(content=\"Large language models (LLMs) are incredibly powerful tools for processing and generating text. However, they inherently struggle to understand the broader context of information, especially when dealing with lengthy conversations or complex tasks. This is where large context windows and Retrieval-Augmented Generation (RAG) come into play. These advanced, generalized language models are trained on vast datasets, enabling them to understand and generate human-like text. In the context of RAG, LLMs are used to generate fully formed responses based on the user query and contextual information retrieved from the vector DBs during user queries. Querying In the rapidly evolving landscape of language models, the debate between Retrieval-Augmented Generation (RAG) and Long Context Large Language Models (LLMs) has garnered significant attention. Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM's internal representation of information. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has ... RAG stands for R etrieval- A ugmented G eneration. RAG enables large language models (LLM) to access and utilize up-to-date information. Hence, it improves the quality of relevance of the response from LLM. Below is a simple diagram of how RAG is implemented.\", name='duckduckgo_search'),\n",
    "  AIMessage(content=\"RAG stands for Retrieval-Augmented Generation in the context of Large Language Models (LLMs). It is an AI framework that improves the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM's internal representation of information. RAG enables LLMs to access and utilize up-to-date information, thereby improving the relevance and quality of the responses generated by the model. RAG broke onto the scene in the rapidly evolving landscape of language models as a way to enhance the capabilities of LLMs in understanding and generating human-like text.\", response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 491, 'total_tokens': 608}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context of Large Language Models (LLMs), RAG stands for \"Randomized Attention Generative\" model. It is a type of attention-based model that uses a random sampling approach to generate text.\\n\\nRAG was introduced in 2019 by researchers at Google, specifically Shiyu Chang and others [1]. The model was designed to be a flexible and efficient way to generate text, with the ability to produce coherent and diverse outputs.\\n\\nThe key innovation of RAG is its use of a probabilistic sampling mechanism to generate text. Instead of using traditional beam search or greedy search methods, RAG samples from a probability distribution over possible words in the vocabulary at each step. This allows the model to explore a large space of possible words and combinations, resulting in more diverse and coherent outputs.\\n\\nRAG has been shown to be effective in various applications, including text generation, language translation, and question answering. Its flexibility and efficiency have made it a popular choice for many NLP tasks.\\n\\n[1] Chang, S., et al. (2019). \"Randomized Attention Generative Model for Text Generation.\" In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages'][-1].content\n",
    "\n",
    "####### RESPONSE #####################\n",
    "#RAG stands for Retrieval-Augmented Generation in the context of Large Language Models (LLMs). It is an AI framework that improves the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM's internal representation of information. RAG enables LLMs to access and utilize up-to-date information, thereby improving the relevance and quality of the responses generated by the model. RAG broke onto the scene in the rapidly evolving landscape of language models as a way to enhance the capabilities of LLMs in understanding and generating human-like text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am unable to verify who the authors of the paper \"Retrieval Augmented Generation\" are.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the main author on the Retrieval Augmented Generation paper - and what University did they attend?\"\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
    "#\n",
    "response = app.invoke(inputs)\n",
    "print(response['messages'][-1].content)\n",
    "################# RESPONSE ##############################\n",
    "#The main author on the \"Retrieval Augmented Generation\" paper is Huayang Li. Unfortunately, the University they attended is not mentioned in the summary provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot verify who the main author of the \"Retrieval Augmented Generation\" paper is. However, I can tell you that the concept of retrieval augmentation has been explored in various research papers across different fields. If you have any further information about the specific paper you're referring to or its context, I'd be happy to try and help you find more accurate information.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is the main author on the Retrieval Augmented Generation paper?\"\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=question)]}\n",
    "\n",
    "response = app.invoke(inputs)\n",
    "print(response['messages'][-1].content)\n",
    "################# RESPONSE ##############################\n",
    "#The main authors on the paper \"A Survey on Retrieval-Augmented Text Generation\" are Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and Lemao Liu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
