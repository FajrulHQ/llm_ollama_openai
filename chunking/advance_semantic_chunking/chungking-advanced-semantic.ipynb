{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - BASIC WITH GENSIM (LDA) \n",
    "Pada metode ini hanya menggunakan metode chungking berdasarkan topik menggunakan Gensim (LDA - Latent Dirichlet Allocation). Gensim menggunakan model berbasis bag-of-words seperti LDA (Latent Dirichlet Allocation) untuk mengekstrak topik dari teks.\n",
    "\n",
    "Cara Kerja:\n",
    "- Memetakan dokumen ke dalam ruang vektor berdasarkan frekuensi kata.\n",
    "- Menggunakan probabilitas untuk menemukan distribusi kata dalam berbagai topik.\n",
    "\n",
    "Kelebihan:\n",
    "- Cocok untuk analisis topik berbasis statistik.\n",
    "- Tidak memerlukan model berbasis pembelajaran mendalam.\n",
    "\n",
    "Kekurangan:\n",
    "- Tidak mempertimbangkan konteks urutan kata dalam kalimat.\n",
    "- Tidak menghasilkan representasi teks yang dapat digunakan untuk perbandingan semantik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-basic-gensim\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-basic-gensim\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-basic-gensim\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import PyPDF2\n",
    "import docx\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    topics = lda_model.print_topics()\n",
    "    return topics\n",
    "\n",
    "def advanced_chunk_text(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_entities = set()\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        entities = {ent.text for ent in sent.ents}\n",
    "        if len(current_chunk) + len(sent.text) < chunk_size and (not current_entities or entities & current_entities):\n",
    "            current_chunk += \" \" + sent.text\n",
    "            current_entities.update(entities)\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent.text\n",
    "            current_entities = entities\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext in \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = advanced_chunk_text(text)\n",
    "    topics = extract_topics(text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n\\n\")\n",
    "        \n",
    "        for topic in topics:\n",
    "            f.write(f\"--- Extracted Topics {i+1} ---\\n{topic}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-basic-gensim\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - BASIC WITH SENTENCETRANSFORMER (BERT)\n",
    "Pada metode ini hanya menggunakan metode chungking berdasarkan topik menggunakan SentenceTransformer (BERT-based Embeddings). SentenceTransformer menggunakan model berbasis Transformer (BERT, RoBERTa, dll.) untuk menghasilkan embedding kalimat yang lebih kontekstual.\n",
    "\n",
    "Cara Kerja:\n",
    "- Mengubah kalimat menjadi vektor berdimensi tinggi menggunakan model deep learning.\n",
    "- Mempertimbangkan makna keseluruhan teks dalam konteksnya.\n",
    "\n",
    "Kelebihan:\n",
    "- Menghasilkan embedding yang lebih kaya dan kontekstual.\n",
    "- Bisa digunakan untuk perbandingan kemiripan antar-kalimat.\n",
    "\n",
    "Kekurangan:\n",
    "- Memerlukan lebih banyak daya komputasi dibandingkan dengan Gensim LDA.\n",
    "- Model lebih besar dan memerlukan dependensi tambahan seperti PyTorch atau TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-basic-sentence-transformers\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-basic-sentence-transformers\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-basic-sentence-transformers\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import PyPDF2\n",
    "import docx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    sentences = [sent.text for sent in nlp(text).sents]\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings\n",
    "\n",
    "def advanced_chunk_text(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_entities = set()\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        entities = {ent.text for ent in sent.ents}\n",
    "        if len(current_chunk) + len(sent.text) < chunk_size and (not current_entities or entities & current_entities):\n",
    "            current_chunk += \" \" + sent.text\n",
    "            current_entities.update(entities)\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent.text\n",
    "            current_entities = entities\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = advanced_chunk_text(text)\n",
    "    topics = extract_topics(text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n\\n\")\n",
    "        \n",
    "        for topic in topics:\n",
    "            f.write(f\"--- Extracted Topics {i+1} ---\\n{topic}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-basic-sentence-transformers\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - NODE ONLY\n",
    "\n",
    "Metode ini hanya menggunakan teknik chunking berbasis graf tanpa perangkingan. Model ini membentuk node berdasarkan kemiripan semantik antar kalimat menggunakan SentenceTransformer dan graph-based clustering.\n",
    "\n",
    "Cara Kerja:\n",
    "- Teks dipecah menjadi kalimat.\n",
    "- Kalimat direpresentasikan sebagai vektor embedding menggunakan SentenceTransformer.\n",
    "- Graf dibentuk berdasarkan cosine similarity antar kalimat.\n",
    "- Komunitas dalam graf ditentukan menggunakan greedy modularity optimization, yang menghasilkan chunk berbasis hubungan semantik.\n",
    "\n",
    "Kelebihan:\n",
    "- Memanfaatkan hubungan semantik antar kalimat.\n",
    "- Cocok untuk dokumen dengan struktur naratif yang kuat.\n",
    "\n",
    "Kekurangan:\n",
    "- Tidak ada prioritas dalam hasil chunking.\n",
    "- Semua chunk dianggap memiliki bobot yang sama dalam informasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-graph-node-only\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-graph-node-only\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-graph-node-only\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import docx\n",
    "import PyPDF2\n",
    "import networkx as nx\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def graph_based_chunking(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i, j])\n",
    "    \n",
    "    partitions = nx.community.greedy_modularity_communities(G)\n",
    "    \n",
    "    chunks = []\n",
    "    for community in partitions:\n",
    "        chunk = \" \".join([sentences[i] for i in sorted(community)])\n",
    "        if len(chunk) > chunk_size:\n",
    "            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    topics = lda_model.print_topics()\n",
    "    return topics\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = graph_based_chunking(text)\n",
    "    topics = extract_topics(text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n\\n\")\n",
    "        \n",
    "        for topic in topics:\n",
    "            f.write(f\"--- Extracted Topics {i+1} ---\\n{topic}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-graph-node-only\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - NODE WITH CHUNK RANK\n",
    "\n",
    "Metode ini menggunakan chunking berbasis graf, namun dengan tambahan perangkingan chunk berdasarkan jumlah kata atau kepadatan informasi.\n",
    "\n",
    "Cara Kerja:\n",
    "- Proses chunking sama seperti metode \"Node Only\".\n",
    "- Setelah chunk terbentuk, setiap chunk diberi skor berdasarkan jumlah kata.\n",
    "- Chunk dengan informasi lebih padat diberikan peringkat lebih tinggi.\n",
    "\n",
    "Kelebihan:\n",
    "- Memungkinkan ekstraksi chunk yang lebih informatif.\n",
    "- Cocok untuk proses summarization berbasis chunk.\n",
    "\n",
    "Kekurangan:\n",
    "- Ranking berdasarkan jumlah kata belum tentu mencerminkan makna semantik yang lebih penting.\n",
    "- Bisa menyebabkan perubahan urutan asli dokumen jika digunakan untuk reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-graph-chunk-rank\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-graph-chunk-rank\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-graph-chunk-rank\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import docx\n",
    "import PyPDF2\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def graph_based_chunking(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i, j])\n",
    "    \n",
    "    partitions = nx.community.greedy_modularity_communities(G)\n",
    "    \n",
    "    chunks = []\n",
    "    for community in partitions:\n",
    "        chunk = \" \".join([sentences[i] for i in sorted(community)])\n",
    "        if len(chunk) > chunk_size:\n",
    "            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    topics = lda_model.print_topics()\n",
    "    return topics\n",
    "\n",
    "def rank_chunks(chunks):\n",
    "    # Rank chunk by word count\n",
    "    chunk_scores = [len(chunk.split()) for chunk in chunks]\n",
    "    ranked_chunks = sorted(zip(chunk_scores, chunks), reverse=True, key=lambda x: x[0])\n",
    "    return ranked_chunks\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = graph_based_chunking(text)\n",
    "    topics = extract_topics(text)\n",
    "    ranked_chunks = rank_chunks(chunks)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (score, chunk) in enumerate(ranked_chunks):\n",
    "            topics = extract_topics(chunk)\n",
    "            f.write(f\"--- Chunk {i+1} (Score: {score}) ---\\n{chunk}\\n\\n\")\n",
    "        \n",
    "        for topic in topics:\n",
    "            f.write(f\"--- Extracted Topics {i+1} ---\\n{topic}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-graph-chunk-rank\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - NODE WITH TOPIC RANK\n",
    "\n",
    "Metode ini menggunakan chunking berbasis graf dan menambahkan perangkingan berbasis topik menggunakan LDA (Latent Dirichlet Allocation).\n",
    "\n",
    "Cara Kerja:\n",
    "- Setelah chunk terbentuk, topik utama diekstrak dari setiap chunk menggunakan Gensim LDA.\n",
    "- Topik terbaik dipilih berdasarkan probabilitas tertinggi dalam distribusi topik chunk tersebut.\n",
    "\n",
    "Kelebihan:\n",
    "- Memungkinkan identifikasi topik utama dalam setiap chunk.\n",
    "- Berguna untuk analisis topik otomatis dari dokumen panjang.\n",
    "\n",
    "Kekurangan:\n",
    "- LDA berbasis bag-of-words, sehingga tidak mempertimbangkan urutan kata.\n",
    "- Hasil topik bisa kurang akurat jika jumlah topik tidak ditentukan dengan baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-graph-topic-rank\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-graph-topic-rank\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-graph-topic-rank\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import docx\n",
    "import PyPDF2\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def graph_based_chunking(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i, j])\n",
    "    \n",
    "    partitions = nx.community.greedy_modularity_communities(G)\n",
    "    \n",
    "    chunks = []\n",
    "    for community in partitions:\n",
    "        chunk = \" \".join([sentences[i] for i in sorted(community)])\n",
    "        if len(chunk) > chunk_size:\n",
    "            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    topics = lda_model.show_topics(formatted=False)\n",
    "    \n",
    "    # Rank topics by probability\n",
    "    ranked_topics = sorted(topics, key=lambda x: -sum(prob for _, prob in x[1]))\n",
    "    best_topic = ranked_topics[0] if ranked_topics else None\n",
    "    \n",
    "    return best_topic\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = graph_based_chunking(text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            best_topic = extract_topics(chunk)\n",
    "            topic_str = f\"Topic {best_topic[0]}: {[word for word, _ in best_topic[1]]}\" if best_topic else \"No topics found\"\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\nBest Topic: {topic_str}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-graph-topic-rank\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - LLAMA WITH EMBEDDING RANK\n",
    "\n",
    "Metode ini menggunakan pemrosesan bahasa alami (NLP) berbasis spaCy untuk chunking teks, serta peringkat berbasis embedding menggunakan model `HuggingFaceEmbeddings`. Model LDA (Latent Dirichlet Allocation) digunakan untuk mengekstrak topik utama dari setiap chunk.\n",
    "\n",
    "Cara Kerja:\n",
    "- Dokumen dipecah menjadi chunk menggunakan model NLP spaCy.\n",
    "- Embedding setiap chunk dihitung menggunakan `HuggingFaceEmbeddings`.\n",
    "- Chunk diberi peringkat berdasarkan norma embedding menggunakan `numpy.linalg.norm`.\n",
    "- Topik utama dari setiap chunk diekstrak menggunakan Gensim LDA.\n",
    "- Topik terbaik dipilih berdasarkan probabilitas tertinggi dalam distribusi topik chunk tersebut.\n",
    "\n",
    "Kelebihan:\n",
    "- Menggunakan embedding untuk peringkat yang lebih akurat dibanding metode berbasis kata.\n",
    "- Mampu mengidentifikasi topik utama dalam setiap chunk dengan metode LDA.\n",
    "- Dapat diterapkan pada berbagai jenis dokumen (TXT, PDF, DOCX).\n",
    "\n",
    "Kekurangan:\n",
    "- LDA masih berbasis bag-of-words, sehingga tidak mempertimbangkan urutan kata.\n",
    "- Kualitas chunk tergantung pada model NLP yang digunakan.\n",
    "- Hasil topik bisa kurang akurat jika jumlah topik tidak ditentukan dengan baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document: data\\dokumen_docx.docx\n",
      "Reading DOCX file: data\\dokumen_docx.docx\n",
      "Preprocessing text: Cleaning and normalizing...\n",
      "Performing semantic chunking...\n",
      "Generated 7 chunks.\n",
      "Processing chunk 1...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 2...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 3...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 4...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 5...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 6...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 7...\n",
      "Extracting topics using LDA...\n",
      "Finished processing: data\\dokumen_docx.docx -> output-llm-semantic-chunking\\dokumen_docx_chunks.txt\n",
      "Processing document: data\\dokumen_pdf.pdf\n",
      "Reading PDF file: data\\dokumen_pdf.pdf\n",
      "Preprocessing text: Cleaning and normalizing...\n",
      "Performing semantic chunking...\n",
      "Generated 7 chunks.\n",
      "Processing chunk 1...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 2...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 3...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 4...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 5...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 6...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 7...\n",
      "Extracting topics using LDA...\n",
      "Finished processing: data\\dokumen_pdf.pdf -> output-llm-semantic-chunking\\dokumen_pdf_chunks.txt\n",
      "Processing document: data\\dokumen_txt.txt\n",
      "Reading TXT file: data\\dokumen_txt.txt\n",
      "Preprocessing text: Cleaning and normalizing...\n",
      "Performing semantic chunking...\n",
      "Generated 7 chunks.\n",
      "Processing chunk 1...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 2...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 3...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 4...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 5...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 6...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 7...\n",
      "Extracting topics using LDA...\n",
      "Finished processing: data\\dokumen_txt.txt -> output-llm-semantic-chunking\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import docx\n",
    "import fitz\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    print(f\"Reading TXT file: {file_path}\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    print(f\"Reading PDF file: {file_path}\")\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    print(f\"Reading DOCX file: {file_path}\")\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def clean_text(text):\n",
    "    print(\"Preprocessing text: Cleaning and normalizing...\")\n",
    "    text = text.replace(\"\\n\", \" \").strip() \n",
    "    text = \" \".join(text.split())  \n",
    "    return text\n",
    "\n",
    "def semantic_chunking(text, chunk_size=700):\n",
    "    print(\"Performing semantic chunking...\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) < chunk_size:\n",
    "            chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    \n",
    "    print(f\"Generated {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    print(\"Extracting topics using LDA...\")\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4)\n",
    "    topics = lda_model.show_topics(formatted=False)\n",
    "    \n",
    "    return topics[0] if topics else None\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    print(f\"Processing document: {file_path}\")\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {ext}\")\n",
    "        return\n",
    "    \n",
    "    cleaned_text = clean_text(text)\n",
    "    chunks = semantic_chunking(cleaned_text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Processing chunk {i+1}...\")\n",
    "            best_topic = extract_topics(chunk)\n",
    "            if best_topic:\n",
    "                topic_words = \", \".join([word for word, _ in best_topic[1]])\n",
    "                topic_str = f\"Best Topic: {topic_words}\"\n",
    "            else:\n",
    "                topic_str = \"Best Topic: No topics found\"\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n{topic_str}\\n\\n\")\n",
    "    \n",
    "    print(f\"Finished processing: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-llm-semantic-chunking\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Berdasarkan dokumen yang diprint, terdapat beberapa perubahan nama dan restrukturisasi PT Pertamina sebagaimana berikut:\n",
       "\n",
       "Perubahan Nama:\n",
       "\n",
       "1. PT Eksploitasi Tambang Minyak Sumatera Utara menjadi PT Perusahaan Minyak Nasional (PERMINA) pada tahun 1957.\n",
       "2. PERMINA menjadi PN Pertambangan Minyak Nasional (Permina) pada tahun 1961.\n",
       "3. Permina bergabung dengan PN Pertamin sehingga menjadi PN Pertambangan Minyak dan Gas Bumi Negara (Pertamina) pada tahun 1968.\n",
       "4. PN Pertamina diubah menjadi Perusahaan Pertambangan Minyak dan Gas Bumi Negara (PN Pertamina) pada tahun 1971.\n",
       "5. PN Pertamina berubah nama menjadi PT Pertamina (Persero) pada tahun 2003.\n",
       "\n",
       "Restrukturisasi:\n",
       "\n",
       "1. Pada tahun 2018, PT Pertamina Gas (Pertagas) bergabung dengan PT Perusahaan Gas Negara (PGN), sehingga PT Pertamina semakin memantapkan posisinya sebagai garda terdepan yang bertugas menjaga kedaulatan dan ketahanan energi nasional.\n",
       "2. Pada tahun 2020, Subholding Gas dipilih sebagai langkah awal untuk restrukturisasi, diikuti oleh pembentukan Upstream Subholding (PT Pertamina Hulu Energi), Refinery and Petrochemical Subholding (PT Kilang Pertamina Internasional), Power & NRE Subholding (PT Pertamina Power Indonesia), Commercial and Trading Subholding (PT Patra Niaga), dan Integrated Marine Logistics Subholding (PT Pertamina International Shipping).\n",
       "\n",
       "Strategi Ekspansi:\n",
       "\n",
       "1. Pada tahun 2007, PT Pertamina mengubah visi perusahaan menjadi \"Menjadi Perusahaan Minyak Nasional Kelas Dunia\".\n",
       "2. Pada tahun 2011, PT Pertamina menyempurnakan visinya menjadi \"Menjadi Perusahaan Energi Nasional Kelas Dunia\".\n",
       "3. Dengan terbentuknya keenam subholding ini, PT Pertamina dapat lebih fokus untuk mewujudkan kedaulatan energi bagi Indonesia dengan senantiasa menggaungkan semangat 'One Energy, One Pertamina' yang menyinergikan seluruh kegiatan penyediaan energi secara lebih fokus dan terarah."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load LLM model\n",
    "model = OllamaLLM(model=OLLAMA_MODEL)\n",
    "\n",
    "# Template prompt untuk ringkasan\n",
    "template = \"\"\"\n",
    "Anda adalah asisten AI yang ahli dalam menganalisis dokumen.  \n",
    "Berdasarkan dokumen berikut, identifikasi perubahan nama, restrukturisasi, dan strategi ekspansi PT Pertamina.  \n",
    "Gunakan hanya informasi yang terdapat dalam dokumen.  \n",
    "\n",
    "Dokumen:\n",
    "\"{document}\"\n",
    "Ringkasan:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def summarize_text(text):\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke({\"document\": text})\n",
    "    return response\n",
    "\n",
    "output_folder = \"output-llm-semantic-chunking\"\n",
    "file_name = \"dokumen_pdf_chunks.txt\" \n",
    "file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    document_text = f.read()\n",
    "\n",
    "summary = summarize_text(document_text)\n",
    "\n",
    "Markdown(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
