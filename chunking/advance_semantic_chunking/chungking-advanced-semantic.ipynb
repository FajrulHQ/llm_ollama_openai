{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - BASIC WITH GENSIM (LDA) \n",
    "Pada metode ini hanya menggunakan metode chungking berdasarkan topik menggunakan Gensim (LDA - Latent Dirichlet Allocation). Gensim menggunakan model berbasis bag-of-words seperti LDA (Latent Dirichlet Allocation) untuk mengekstrak topik dari teks.\n",
    "\n",
    "Cara Kerja:\n",
    "- Memetakan dokumen ke dalam ruang vektor berdasarkan frekuensi kata.\n",
    "- Menggunakan probabilitas untuk menemukan distribusi kata dalam berbagai topik.\n",
    "\n",
    "Kelebihan:\n",
    "- Cocok untuk analisis topik berbasis statistik.\n",
    "- Tidak memerlukan model berbasis pembelajaran mendalam.\n",
    "\n",
    "Kekurangan:\n",
    "- Tidak mempertimbangkan konteks urutan kata dalam kalimat.\n",
    "- Tidak menghasilkan representasi teks yang dapat digunakan untuk perbandingan semantik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-basic-gensim\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-basic-gensim\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-basic-gensim\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import PyPDF2\n",
    "import docx\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    topics = lda_model.print_topics()\n",
    "    return topics\n",
    "\n",
    "def advanced_chunk_text(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_entities = set()\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        entities = {ent.text for ent in sent.ents}\n",
    "        if len(current_chunk) + len(sent.text) < chunk_size and (not current_entities or entities & current_entities):\n",
    "            current_chunk += \" \" + sent.text\n",
    "            current_entities.update(entities)\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent.text\n",
    "            current_entities = entities\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext in \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = advanced_chunk_text(text)\n",
    "    topics = extract_topics(text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n\\n\")\n",
    "        \n",
    "        for topic in topics:\n",
    "            f.write(f\"--- Extracted Topics {i+1} ---\\n{topic}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-basic-gensim\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - BASIC WITH SENTENCETRANSFORMER (BERT)\n",
    "Pada metode ini hanya menggunakan metode chungking berdasarkan topik menggunakan SentenceTransformer (BERT-based Embeddings). SentenceTransformer menggunakan model berbasis Transformer (BERT, RoBERTa, dll.) untuk menghasilkan embedding kalimat yang lebih kontekstual.\n",
    "\n",
    "Cara Kerja:\n",
    "- Mengubah kalimat menjadi vektor berdimensi tinggi menggunakan model deep learning.\n",
    "- Mempertimbangkan makna keseluruhan teks dalam konteksnya.\n",
    "\n",
    "Kelebihan:\n",
    "- Menghasilkan embedding yang lebih kaya dan kontekstual.\n",
    "- Bisa digunakan untuk perbandingan kemiripan antar-kalimat.\n",
    "\n",
    "Kekurangan:\n",
    "- Memerlukan lebih banyak daya komputasi dibandingkan dengan Gensim LDA.\n",
    "- Model lebih besar dan memerlukan dependensi tambahan seperti PyTorch atau TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-basic-sentence-transformers\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-basic-sentence-transformers\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-basic-sentence-transformers\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import PyPDF2\n",
    "import docx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    sentences = [sent.text for sent in nlp(text).sents]\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings\n",
    "\n",
    "def advanced_chunk_text(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_entities = set()\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        entities = {ent.text for ent in sent.ents}\n",
    "        if len(current_chunk) + len(sent.text) < chunk_size and (not current_entities or entities & current_entities):\n",
    "            current_chunk += \" \" + sent.text\n",
    "            current_entities.update(entities)\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent.text\n",
    "            current_entities = entities\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = advanced_chunk_text(text)\n",
    "    topics = extract_topics(text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n\\n\")\n",
    "        \n",
    "        for topic in topics:\n",
    "            f.write(f\"--- Extracted Topics {i+1} ---\\n{topic}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-basic-sentence-transformers\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - NODE ONLY\n",
    "\n",
    "Metode ini hanya menggunakan teknik chunking berbasis graf tanpa perangkingan. Model ini membentuk node berdasarkan kemiripan semantik antar kalimat menggunakan SentenceTransformer dan graph-based clustering.\n",
    "\n",
    "Cara Kerja:\n",
    "- Teks dipecah menjadi kalimat.\n",
    "- Kalimat direpresentasikan sebagai vektor embedding menggunakan SentenceTransformer.\n",
    "- Graf dibentuk berdasarkan cosine similarity antar kalimat.\n",
    "- Komunitas dalam graf ditentukan menggunakan greedy modularity optimization, yang menghasilkan chunk berbasis hubungan semantik.\n",
    "\n",
    "Kelebihan:\n",
    "- Memanfaatkan hubungan semantik antar kalimat.\n",
    "- Cocok untuk dokumen dengan struktur naratif yang kuat.\n",
    "\n",
    "Kekurangan:\n",
    "- Tidak ada prioritas dalam hasil chunking.\n",
    "- Semua chunk dianggap memiliki bobot yang sama dalam informasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-graph-node-only\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-graph-node-only\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-graph-node-only\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import docx\n",
    "import PyPDF2\n",
    "import networkx as nx\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def graph_based_chunking(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i, j])\n",
    "    \n",
    "    partitions = nx.community.greedy_modularity_communities(G)\n",
    "    \n",
    "    chunks = []\n",
    "    for community in partitions:\n",
    "        chunk = \" \".join([sentences[i] for i in sorted(community)])\n",
    "        if len(chunk) > chunk_size:\n",
    "            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    topics = lda_model.print_topics()\n",
    "    return topics\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = graph_based_chunking(text)\n",
    "    topics = extract_topics(text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n\\n\")\n",
    "        \n",
    "        for topic in topics:\n",
    "            f.write(f\"--- Extracted Topics {i+1} ---\\n{topic}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-graph-node-only\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - NODE WITH CHUNK RANK\n",
    "\n",
    "Metode ini menggunakan chunking berbasis graf, namun dengan tambahan perangkingan chunk berdasarkan jumlah kata atau kepadatan informasi.\n",
    "\n",
    "Cara Kerja:\n",
    "- Proses chunking sama seperti metode \"Node Only\".\n",
    "- Setelah chunk terbentuk, setiap chunk diberi skor berdasarkan jumlah kata.\n",
    "- Chunk dengan informasi lebih padat diberikan peringkat lebih tinggi.\n",
    "\n",
    "Kelebihan:\n",
    "- Memungkinkan ekstraksi chunk yang lebih informatif.\n",
    "- Cocok untuk proses summarization berbasis chunk.\n",
    "\n",
    "Kekurangan:\n",
    "- Ranking berdasarkan jumlah kata belum tentu mencerminkan makna semantik yang lebih penting.\n",
    "- Bisa menyebabkan perubahan urutan asli dokumen jika digunakan untuk reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-graph-chunk-rank\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-graph-chunk-rank\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-graph-chunk-rank\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import docx\n",
    "import PyPDF2\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def graph_based_chunking(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i, j])\n",
    "    \n",
    "    partitions = nx.community.greedy_modularity_communities(G)\n",
    "    \n",
    "    chunks = []\n",
    "    for community in partitions:\n",
    "        chunk = \" \".join([sentences[i] for i in sorted(community)])\n",
    "        if len(chunk) > chunk_size:\n",
    "            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    topics = lda_model.print_topics()\n",
    "    return topics\n",
    "\n",
    "def rank_chunks(chunks):\n",
    "    # Rank chunk by word count\n",
    "    chunk_scores = [len(chunk.split()) for chunk in chunks]\n",
    "    ranked_chunks = sorted(zip(chunk_scores, chunks), reverse=True, key=lambda x: x[0])\n",
    "    return ranked_chunks\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = graph_based_chunking(text)\n",
    "    topics = extract_topics(text)\n",
    "    ranked_chunks = rank_chunks(chunks)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (score, chunk) in enumerate(ranked_chunks):\n",
    "            topics = extract_topics(chunk)\n",
    "            f.write(f\"--- Chunk {i+1} (Score: {score}) ---\\n{chunk}\\n\\n\")\n",
    "        \n",
    "        for topic in topics:\n",
    "            f.write(f\"--- Extracted Topics {i+1} ---\\n{topic}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-graph-chunk-rank\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - NODE WITH TOPIC RANK\n",
    "\n",
    "Metode ini menggunakan chunking berbasis graf dan menambahkan perangkingan berbasis topik menggunakan LDA (Latent Dirichlet Allocation).\n",
    "\n",
    "Cara Kerja:\n",
    "- Setelah chunk terbentuk, topik utama diekstrak dari setiap chunk menggunakan Gensim LDA.\n",
    "- Topik terbaik dipilih berdasarkan probabilitas tertinggi dalam distribusi topik chunk tersebut.\n",
    "\n",
    "Kelebihan:\n",
    "- Memungkinkan identifikasi topik utama dalam setiap chunk.\n",
    "- Berguna untuk analisis topik otomatis dari dokumen panjang.\n",
    "\n",
    "Kekurangan:\n",
    "- LDA berbasis bag-of-words, sehingga tidak mempertimbangkan urutan kata.\n",
    "- Hasil topik bisa kurang akurat jika jumlah topik tidak ditentukan dengan baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-graph-topic-rank\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-graph-topic-rank\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-graph-topic-rank\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import docx\n",
    "import PyPDF2\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def graph_based_chunking(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i, j])\n",
    "    \n",
    "    partitions = nx.community.greedy_modularity_communities(G)\n",
    "    \n",
    "    chunks = []\n",
    "    for community in partitions:\n",
    "        chunk = \" \".join([sentences[i] for i in sorted(community)])\n",
    "        if len(chunk) > chunk_size:\n",
    "            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "    topics = lda_model.show_topics(formatted=False)\n",
    "    \n",
    "    # Rank topics by probability\n",
    "    ranked_topics = sorted(topics, key=lambda x: -sum(prob for _, prob in x[1]))\n",
    "    best_topic = ranked_topics[0] if ranked_topics else None\n",
    "    \n",
    "    return best_topic\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = graph_based_chunking(text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            best_topic = extract_topics(chunk)\n",
    "            topic_str = f\"Topic {best_topic[0]}: {[word for word, _ in best_topic[1]]}\" if best_topic else \"No topics found\"\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\nBest Topic: {topic_str}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-graph-topic-rank\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - LLAMA WITH EMBEDDING RANK\n",
    "\n",
    "Metode ini menggunakan pemrosesan bahasa alami (NLP) berbasis spaCy untuk chunking teks, serta peringkat berbasis embedding menggunakan model `HuggingFaceEmbeddings`. Model LDA (Latent Dirichlet Allocation) digunakan untuk mengekstrak topik utama dari setiap chunk.\n",
    "\n",
    "Cara Kerja:\n",
    "- Dokumen dipecah menjadi chunk menggunakan model NLP spaCy.\n",
    "- Embedding setiap chunk dihitung menggunakan `HuggingFaceEmbeddings`.\n",
    "- Chunk diberi peringkat berdasarkan norma embedding menggunakan `numpy.linalg.norm`.\n",
    "- Topik utama dari setiap chunk diekstrak menggunakan Gensim LDA.\n",
    "- Topik terbaik dipilih berdasarkan probabilitas tertinggi dalam distribusi topik chunk tersebut.\n",
    "\n",
    "Kelebihan:\n",
    "- Menggunakan embedding untuk peringkat yang lebih akurat dibanding metode berbasis kata.\n",
    "- Mampu mengidentifikasi topik utama dalam setiap chunk dengan metode LDA.\n",
    "- Dapat diterapkan pada berbagai jenis dokumen (TXT, PDF, DOCX).\n",
    "\n",
    "Kekurangan:\n",
    "- LDA masih berbasis bag-of-words, sehingga tidak mempertimbangkan urutan kata.\n",
    "- Kualitas chunk tergantung pada model NLP yang digunakan.\n",
    "- Hasil topik bisa kurang akurat jika jumlah topik tidak ditentukan dengan baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_docx.docx -> output-llm-semantic-chunking\\dokumen_docx_chunks.txt\n",
      "Processed: data\\dokumen_pdf.pdf -> output-llm-semantic-chunking\\dokumen_pdf_chunks.txt\n",
      "Processed: data\\dokumen_txt.txt -> output-llm-semantic-chunking\\dokumen_txt_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import docx\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    text = \"\"\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def semantic_chunking(text, chunk_size=700):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) < chunk_size:\n",
    "            chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4)\n",
    "    topics = lda_model.show_topics(formatted=False)\n",
    "    \n",
    "    ranked_topics = sorted(topics, key=lambda x: -sum(prob for _, prob in x[1]))\n",
    "    best_topic = ranked_topics[0] if ranked_topics else None\n",
    "    \n",
    "    return best_topic\n",
    "\n",
    "def rank_chunks(chunks):\n",
    "    chunk_embeddings = [embedding_model.embed_query(chunk) for chunk in chunks]\n",
    "    scores = [np.linalg.norm(embedding) for embedding in chunk_embeddings]\n",
    "    ranked_chunks = sorted(zip(chunks, scores), key=lambda x: -x[1])\n",
    "    return ranked_chunks\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    chunks = semantic_chunking(text)\n",
    "    ranked_chunks = rank_chunks(chunks)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, (chunk, score) in enumerate(ranked_chunks):\n",
    "            best_topic = extract_topics(chunk)\n",
    "            topic_str = f\"Topic {best_topic[0]}: {[word for word, _ in best_topic[1]]}\" if best_topic else \"No topics found\"\n",
    "            f.write(f\"--- Chunk {i+1} (Score: {score:.4f}) ---\\n{chunk}\\nBest Topic: {topic_str}\\n\\n\")\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"data\"\n",
    "    output_folder = \"output-llm-semantic-chunking\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
