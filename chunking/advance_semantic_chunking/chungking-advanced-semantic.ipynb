{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - BASIC WITH GENSIM (LDA) \n",
    "Pada metode ini hanya menggunakan metode chungking berdasarkan topik menggunakan Gensim (LDA - Latent Dirichlet Allocation). Gensim menggunakan model berbasis bag-of-words seperti LDA (Latent Dirichlet Allocation) untuk mengekstrak topik dari teks.\n",
    "\n",
    "Cara Kerja:\n",
    "- Memetakan dokumen ke dalam ruang vektor berdasarkan frekuensi kata.\n",
    "- Menggunakan probabilitas untuk menemukan distribusi kata dalam berbagai topik.\n",
    "\n",
    "Kelebihan:\n",
    "- Cocok untuk analisis topik berbasis statistik.\n",
    "- Tidak memerlukan model berbasis pembelajaran mendalam.\n",
    "\n",
    "Kekurangan:\n",
    "- Tidak mempertimbangkan konteks urutan kata dalam kalimat.\n",
    "- Tidak menghasilkan representasi teks yang dapat digunakan untuk perbandingan semantik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_pdf.pdf -> output/basic-gensim\\dokumen_pdf\n",
      "Processed: data\\tko.pdf -> output/basic-gensim\\tko\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import spacy  # Mengimpor spaCy untuk pemrosesan bahasa alami\n",
    "import PyPDF2  # Mengimpor PyPDF2 untuk membaca file PDF\n",
    "import docx  # Mengimpor modul docx untuk membaca file DOCX\n",
    "import gensim  # Mengimpor gensim untuk model topik\n",
    "from gensim import corpora  # Mengimpor corpora dari gensim untuk membuat kamus\n",
    "\n",
    "# Memuat model bahasa Inggris kecil dari spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    # Membaca file teks dan mengembalikan isinya\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Membuka file dalam mode baca\n",
    "        return f.read()  # Mengembalikan seluruh isi file\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    # Membaca file PDF dan mengembalikan isinya\n",
    "    text = \"\"  # Inisialisasi string kosong untuk menyimpan teks\n",
    "    with open(file_path, \"rb\") as f:  # Membuka file PDF dalam mode biner\n",
    "        reader = PyPDF2.PdfReader(f)  # Membaca file PDF\n",
    "        for page in reader.pages:  # Iterasi melalui setiap halaman\n",
    "            text += page.extract_text() + \"\\n\"  # Ekstrak teks dari halaman dan tambahkan ke string\n",
    "    return text  # Mengembalikan teks yang diekstrak\n",
    "\n",
    "def read_docx(file_path):\n",
    "    # Membaca file DOCX dan mengembalikan isinya\n",
    "    doc = docx.Document(file_path)  # Membuka file DOCX\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])  # Menggabungkan teks dari setiap paragraf\n",
    "\n",
    "def extract_topics(text, num_topics=3):\n",
    "    # Mengekstrak topik dari teks menggunakan model LDA\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]  # Tokenisasi dan lemmatization\n",
    "\n",
    "    if not words:  # Jika tidak ada kata yang valid\n",
    "        return [\"[No valid words for topic extraction]\"]  # Mengembalikan pesan bahwa tidak ada kata valid\n",
    "\n",
    "    dictionary = corpora.Dictionary([words])  # Membuat kamus dari kata-kata\n",
    "    corpus = [dictionary.doc2bow(words)]  # Membuat korpus dari kata-kata\n",
    "\n",
    "    if not corpus or all(len(doc) == 0 for doc in corpus):  # Jika korpus kosong\n",
    "        return [\"[No significant topics]\"]  # Mengembalikan pesan bahwa tidak ada topik signifikan\n",
    "\n",
    "    # Membuat model LDA untuk mengekstrak topik\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=min(num_topics, len(dictionary)), \n",
    "    id2word=dictionary, passes=10)\n",
    "    topics = lda_model.print_topics()  # Mencetak topik yang diekstrak\n",
    "\n",
    "    return topics  # Mengembalikan daftar topik\n",
    "\n",
    "def advanced_chunk_text(text, chunk_size=700):\n",
    "    # Membagi teks menjadi bagian-bagian kecil berdasarkan ukuran dan entitas\n",
    "    doc = nlp(text)  # Memproses teks dengan spaCy\n",
    "    chunks = []  # Inisialisasi daftar untuk menyimpan bagian\n",
    "    current_chunk = \"\"  # Inisialisasi string untuk bagian saat ini\n",
    "    current_entities = set()  # Inisialisasi set untuk menyimpan entitas saat ini\n",
    "    \n",
    "    for sent in doc.sents:  # Iterasi melalui setiap kalimat\n",
    "        entities = {ent.text for ent in sent.ents}  # Mengambil entitas dari kalimat\n",
    "        # Jika ukuran bagian saat ini ditambah kalimat masih dalam batas chunk_size\n",
    "        if len(current_chunk) + len(sent.text) < chunk_size and (not current_entities or entities & current_entities):\n",
    "            current_chunk += \" \" + sent.text  # Tambahkan kalimat ke bagian saat ini\n",
    "            current_entities.update(entities)  # Perbarui entitas saat ini\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())  # Tambahkan bagian saat ini ke daftar\n",
    "            current_chunk = sent.text  # Mulai bagian baru dengan kalimat saat ini\n",
    "            current_entities = entities  # Perbarui entitas saat ini\n",
    "    \n",
    "    if current_chunk:  # Jika ada bagian yang tersisa\n",
    "        chunks.append(current_chunk.strip())  # Tambahkan bagian terakhir ke daftar\n",
    "    \n",
    "    return chunks  # Mengembalikan daftar bagian\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    # Memproses dokumen berdasarkan jenis file dan menyimpan hasilnya\n",
    "    ext = file_path.split(\".\")[-1].lower()  # Mendapatkan ekstensi file\n",
    "    \n",
    "    # Membaca teks berdasarkan jenis file\n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)  # Membaca file teks\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf (file_path)  # Membaca file PDF\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)  # Membaca file DOCX\n",
    "    else:\n",
    "        return  # Jika jenis file tidak dikenali, keluar dari fungsi\n",
    "    \n",
    "    base_name = os.path.basename(file_path).split(\".\")[0]  # Mendapatkan nama dasar file tanpa ekstensi\n",
    "    file_output_folder = os.path.join(output_folder, base_name)  # Menentukan folder output untuk file ini\n",
    "    os.makedirs(file_output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    chunks = advanced_chunk_text(text)  # Membagi teks menjadi bagian-bagian kecil\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):  # Iterasi melalui setiap bagian\n",
    "        chunk_file = os.path.join(file_output_folder, f\"{base_name}_chunk_{i+1}.txt\")  # Menentukan nama file untuk bagian\n",
    "        topics = extract_topics(chunk)  # Mengekstrak topik dari bagian\n",
    "        \n",
    "        with open(chunk_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "            f.write(chunk + \"\\n\\n\")  # Menulis bagian ke file\n",
    "            f.write(\"Topik: \\n\")  # Menulis header untuk topik\n",
    "            for topic in topics:  # Iterasi melalui setiap topik\n",
    "                f.write(f\"{topic}\\n\")  # Menulis topik ke file\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {file_output_folder}\")  # Menampilkan pesan bahwa file telah diproses\n",
    "\n",
    "def main():\n",
    "    # Fungsi utama untuk menjalankan proses\n",
    "    input_folder = \"data\"  # Menentukan folder input\n",
    "    output_folder = \"output/basic-gensim\"  # Menentukan folder output\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):  # Iterasi melalui setiap file dalam folder input\n",
    "        file_path = os.path.join(input_folder, file_name)  # Mendapatkan jalur lengkap file\n",
    "        if os.path.isfile(file_path):  # Memastikan bahwa jalur adalah file\n",
    "            process_document(file_path, output_folder)  # Memproses dokumen\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi utama saat skrip dieksekusi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - BASIC WITH SENTENCETRANSFORMER (BERT)\n",
    "Pada metode ini hanya menggunakan metode chungking berdasarkan topik menggunakan SentenceTransformer (BERT-based Embeddings). SentenceTransformer menggunakan model berbasis Transformer (BERT, RoBERTa, dll.) untuk menghasilkan embedding kalimat yang lebih kontekstual.\n",
    "\n",
    "Cara Kerja:\n",
    "- Mengubah kalimat menjadi vektor berdimensi tinggi menggunakan model deep learning.\n",
    "- Mempertimbangkan makna keseluruhan teks dalam konteksnya.\n",
    "\n",
    "Kelebihan:\n",
    "- Menghasilkan embedding yang lebih kaya dan kontekstual.\n",
    "- Bisa digunakan untuk perbandingan kemiripan antar-kalimat.\n",
    "\n",
    "Kekurangan:\n",
    "- Memerlukan lebih banyak daya komputasi dibandingkan dengan Gensim LDA.\n",
    "- Model lebih besar dan memerlukan dependensi tambahan seperti PyTorch atau TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_pdf.pdf -> output/basic-sentence-transformers\\dokumen_pdf\n",
      "Processed: data\\tko.pdf -> output/basic-sentence-transformers\\tko\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import spacy  # Mengimpor spaCy untuk pemrosesan bahasa alami\n",
    "import PyPDF2  # Mengimpor PyPDF2 untuk membaca file PDF\n",
    "import docx  # Mengimpor modul docx untuk membaca file DOCX\n",
    "from sentence_transformers import SentenceTransformer  # Mengimpor SentenceTransformer untuk model embedding kalimat\n",
    "\n",
    "def read_txt(file_path):\n",
    "    # Membaca file teks dan mengembalikan isinya\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Membuka file dalam mode baca\n",
    "        return f.read()  # Mengembalikan seluruh isi file\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    # Membaca file PDF dan mengembalikan isinya\n",
    "    text = \"\"  # Inisialisasi string kosong untuk menyimpan teks\n",
    "    with open(file_path, \"rb\") as f:  # Membuka file PDF dalam mode biner\n",
    "        reader = PyPDF2.PdfReader(f)  # Membaca file PDF\n",
    "        for page in reader.pages:  # Iterasi melalui setiap halaman\n",
    "            text += page.extract_text() + \"\\n\"  # Ekstrak teks dari halaman dan tambahkan ke string\n",
    "    return text  # Mengembalikan teks yang diekstrak\n",
    "\n",
    "def read_docx(file_path):\n",
    "    # Membaca file DOCX dan mengembalikan isinya\n",
    "    doc = docx.Document(file_path)  # Membuka file DOCX\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])  # Menggabungkan teks dari setiap paragraf\n",
    "\n",
    "# Memuat model bahasa Inggris kecil dari spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Memuat model SentenceTransformer untuk embedding kalimat\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    # Mengekstrak topik dari teks menggunakan model SentenceTransformer\n",
    "    sentences = [sent.text for sent in nlp(text).sents]  # Memecah teks menjadi kalimat\n",
    "\n",
    "    if not sentences:  # Jika tidak ada kalimat yang valid\n",
    "        return [\"[No valid sentences for topic extraction]\"]  # Mengembalikan pesan bahwa tidak ada kalimat valid\n",
    "\n",
    "    embeddings = model.encode(sentences)  # Menghitung embedding untuk setiap kalimat\n",
    "    return embeddings[:num_topics]  # Mengembalikan embedding untuk jumlah topik yang diminta\n",
    "\n",
    "def advanced_chunk_text(text, chunk_size=700):\n",
    "    # Membagi teks menjadi bagian-bagian kecil berdasarkan ukuran dan entitas\n",
    "    doc = nlp(text)  # Memproses teks dengan spaCy\n",
    "    chunks = []  # Inisialisasi daftar untuk menyimpan bagian\n",
    "    current_chunk = \"\"  # Inisialisasi string untuk bagian saat ini\n",
    "    current_entities = set()  # Inisialisasi set untuk menyimpan entitas saat ini\n",
    "    \n",
    "    for sent in doc.sents:  # Iterasi melalui setiap kalimat\n",
    "        entities = {ent.text for ent in sent.ents}  # Mengambil entitas dari kalimat\n",
    "        # Jika ukuran bagian saat ini ditambah kalimat masih dalam batas chunk_size\n",
    "        if len(current_chunk) + len(sent.text) < chunk_size and (not current_entities or entities & current_entities):\n",
    "            current_chunk += \" \" + sent.text  # Tambahkan kalimat ke bagian saat ini\n",
    "            current_entities.update(entities)  # Perbarui entitas saat ini\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())  # Tambahkan bagian saat ini ke daftar\n",
    "            current_chunk = sent.text  # Mulai bagian baru dengan kalimat saat ini\n",
    "            current_entities = entities  # Perbarui entitas saat ini\n",
    "    \n",
    "    if current_chunk:  # Jika ada bagian yang tersisa\n",
    "        chunks.append(current_chunk.strip())  # Tambahkan bagian terakhir ke daftar\n",
    "    \n",
    "    return chunks  # Mengembalikan daftar bagian\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    # Memproses dokumen berdasarkan jenis file dan menyimpan hasilnya\n",
    "    ext = file_path.split(\".\")[-1].lower()  # Mendapatkan ekstensi file\n",
    "    \n",
    "    # Membaca teks berdasarkan jenis file\n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)  # Membaca file teks\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)  # Membaca file PDF\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)  # Membaca file DOCX\n",
    "    else:\n",
    "        return  # Jika jenis file tidak dikenali, keluar dari fungsi\n",
    "    \n",
    "    base_name = os.path.basename(file_path).split(\".\")[0]  # Mendapatkan nama dasar file tanpa ekstensi\n",
    "    file_output_folder = os.path.join(output_folder, base_name)  # Menentukan folder output untuk file ini\n",
    "    os.makedirs(file_output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    chunks = advanced_chunk_text(text)  # Membagi teks menjadi bagian-bagian kecil\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):  # Iterasi melalui setiap bagian\n",
    "        chunk_file = os.path.join(file_output_folder, f\"{base_name}_chunk_{i+1}.txt\")  # Menentukan nama file untuk bagian\n",
    "        topics = extract_topics(chunk)  # Mengekstrak topik dari bagian\n",
    "\n",
    "        with open(chunk_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "            f.write(chunk + \"\\n\\n\")  # Menulis bagian ke file\n",
    "            f.write(\"Topik:\\n\")  # Menulis header untuk topik\n",
    "            for topic in topics:  # Iterasi melalui setiap topik\n",
    "                f.write(f\"{topic}\\n\")  # Menulis topik ke file\n",
    "    \n",
    "    print(f\"Processed: {file_path} -> {file_output_folder}\")  # Menampilkan pesan bahwa file telah diproses\n",
    "\n",
    "def main():\n",
    "    # Fungsi utama untuk menjalankan proses\n",
    "    input_folder = \"data\"  # Menentukan folder input\n",
    "    output_folder = \"output/basic-sentence-transformers\"  # Menentukan folder output\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):  # Iterasi melalui setiap file dalam folder input\n",
    "        file_path = os.path.join(input_folder, file_name)  # Mendapatkan jalur lengkap file\n",
    "        if os.path.isfile(file_path):  # Memastikan bahwa jalur adalah file\n",
    "            process_document(file_path, output_folder)  # Memproses dokumen\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi utama saat skrip dieksekusi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - NODE ONLY\n",
    "\n",
    "Metode ini hanya menggunakan teknik chunking berbasis graf tanpa perangkingan. Model ini membentuk node berdasarkan kemiripan semantik antar kalimat menggunakan SentenceTransformer dan graph-based clustering.\n",
    "\n",
    "Cara Kerja:\n",
    "- Teks dipecah menjadi kalimat.\n",
    "- Kalimat direpresentasikan sebagai vektor embedding menggunakan SentenceTransformer.\n",
    "- Graf dibentuk berdasarkan cosine similarity antar kalimat.\n",
    "- Komunitas dalam graf ditentukan menggunakan greedy modularity optimization, yang menghasilkan chunk berbasis hubungan semantik.\n",
    "\n",
    "Kelebihan:\n",
    "- Memanfaatkan hubungan semantik antar kalimat.\n",
    "- Cocok untuk dokumen dengan struktur naratif yang kuat.\n",
    "\n",
    "Kekurangan:\n",
    "- Tidak ada prioritas dalam hasil chunking.\n",
    "- Semua chunk dianggap memiliki bobot yang sama dalam informasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_pdf.pdf -> output/graph-node-only\\dokumen_pdf\n",
      "Processed: data\\tko.pdf -> output/graph-node-only\\tko\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import spacy  # Mengimpor spaCy untuk pemrosesan bahasa alami\n",
    "import docx  # Mengimpor modul docx untuk membaca file DOCX\n",
    "import PyPDF2  # Mengimpor PyPDF2 untuk membaca file PDF\n",
    "import networkx as nx  # Mengimpor NetworkX untuk analisis graf\n",
    "import gensim  # Mengimpor gensim untuk model topik\n",
    "from gensim import corpora  # Mengimpor corpora dari gensim untuk membuat kamus\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Mengimpor fungsi untuk menghitung kesamaan kosinus\n",
    "from sentence_transformers import SentenceTransformer  # Mengimpor SentenceTransformer untuk model embedding kalimat\n",
    "\n",
    "# Memuat model bahasa Inggris kecil dari spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Memuat model SentenceTransformer untuk embedding kalimat\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    # Membaca file teks dan mengembalikan isinya\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Membuka file dalam mode baca\n",
    "        return f.read()  # Mengembalikan seluruh isi file\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    # Membaca file PDF dan mengembalikan isinya\n",
    "    text = \"\"  # Inisialisasi string kosong untuk menyimpan teks\n",
    "    with open(file_path, \"rb\") as f:  # Membuka file PDF dalam mode biner\n",
    "        reader = PyPDF2.PdfReader(f)  # Membaca file PDF\n",
    "        for page in reader.pages:  # Iterasi melalui setiap halaman\n",
    "            text += page.extract_text() + \"\\n\"  # Ekstrak teks dari halaman dan tambahkan ke string\n",
    "    return text  # Mengembalikan teks yang diekstrak\n",
    "\n",
    "def read_docx(file_path):\n",
    "    # Membaca file DOCX dan mengembalikan isinya\n",
    "    doc = docx.Document(file_path)  # Membuka file DOCX\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])  # Menggabungkan teks dari setiap paragraf\n",
    "\n",
    "def graph_based_chunking(text, chunk_size=700):\n",
    "    # Membagi teks menjadi bagian-bagian kecil menggunakan pendekatan berbasis graf\n",
    "    doc = nlp(text)  # Memproses teks dengan spaCy\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]  # Mengambil kalimat yang tidak kosong\n",
    "    \n",
    "    if not sentences:  # Jika tidak ada kalimat yang valid\n",
    "        return []  # Mengembalikan daftar kosong\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)  # Menghitung embedding untuk setiap kalimat\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)  # Menghitung matriks kesamaan kosinus\n",
    "    G = nx.Graph()  # Membuat graf kosong\n",
    "    \n",
    "    # Menambahkan tepi ke graf berdasarkan kesamaan kalimat\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i, j])  # Menambahkan tepi dengan bobot kesamaan\n",
    "    \n",
    "    # Menggunakan algoritma komunitas untuk menemukan partisi dalam graf\n",
    "    partitions = nx.community.greedy_modularity_communities(G)\n",
    "    \n",
    "    chunks = []  # Inisialisasi daftar untuk menyimpan bagian\n",
    "    for community in partitions:  # Iterasi melalui setiap komunitas\n",
    "        chunk = \" \".join([sentences[i] for i in sorted(community)])  # Menggabungkan kalimat dalam komunitas menjadi satu bagian\n",
    "        if len(chunk) > chunk_size:  # Jika ukuran bagian melebihi chunk_size\n",
    "            # Membagi bagian menjadi sub-bagian yang lebih kecil\n",
    "            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            chunks.extend(sub_chunks)  # Menambahkan sub-bagian ke daftar\n",
    "        else:\n",
    "            chunks.append(chunk)  # Menambahkan bagian ke daftar\n",
    "    \n",
    "    return chunks  # Mengembalikan daftar bagian\n",
    "\n",
    "def extract_topics(text, num_topics=3):\n",
    "    # Mengekstrak topik dari teks menggunakan model LDA\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]  # Tokenisasi dan lemmatization\n",
    "\n",
    "    if not words:  # Jika tidak ada kata yang valid\n",
    "        return [\"[No valid words for topic extraction]\"]  # Mengembalikan pesan bahwa tidak ada kata valid\n",
    "\n",
    "    dictionary = corpora.Dictionary([words])  # Membuat kamus dari kata-kata\n",
    "    corpus = [dictionary.doc2bow(words)]  # Membuat korpus dari kata-kata\n",
    "\n",
    "    if not corpus or all(len(doc) == 0 for doc in corpus):  # Jika korpus kosong\n",
    "        return [\"[No significant topics ]\"]  # Mengembalikan pesan bahwa tidak ada topik yang signifikan\n",
    "\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=min(num_topics, len(dictionary)), \n",
    "    id2word=dictionary, passes=10)  # Membuat model LDA\n",
    "    topics = lda_model.print_topics()  # Mengambil topik yang dihasilkan oleh model\n",
    "\n",
    "    return topics  # Mengembalikan daftar topik\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    # Memproses dokumen berdasarkan jenis file dan menyimpan hasilnya\n",
    "    ext = file_path.split(\".\")[-1].lower()  # Mendapatkan ekstensi file\n",
    "    \n",
    "    # Membaca teks berdasarkan jenis file\n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)  # Membaca file teks\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)  # Membaca file PDF\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)  # Membaca file DOCX\n",
    "    else:\n",
    "        return  # Jika jenis file tidak dikenali, keluar dari fungsi\n",
    "    \n",
    "    base_name = os.path.basename(file_path).split(\".\")[0]  # Mendapatkan nama dasar file tanpa ekstensi\n",
    "    file_output_folder = os.path.join(output_folder, base_name)  # Menentukan folder output untuk file ini\n",
    "    os.makedirs(file_output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    chunks = graph_based_chunking(text)  # Membagi teks menjadi bagian-bagian kecil\n",
    "\n",
    "    for i, chunk in enumerate(chunks):  # Iterasi melalui setiap bagian\n",
    "        chunk_file = os.path.join(file_output_folder, f\"{base_name}_chunk_{i+1}.txt\")  # Menentukan nama file untuk bagian\n",
    "        topics = extract_topics(chunk)  # Mengekstrak topik dari bagian\n",
    "        \n",
    "        with open(chunk_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "            f.write(chunk + \"\\n\\n\")  # Menulis bagian ke file\n",
    "            f.write(\"=== Topics ===\\n\")  # Menulis header untuk topik\n",
    "            for topic in topics:  # Iterasi melalui setiap topik\n",
    "                f.write(f\"{topic}\\n\")  # Menulis topik ke file\n",
    "\n",
    "    print(f\"Processed: {file_path} -> {file_output_folder}\")  # Menampilkan pesan bahwa file telah diproses\n",
    "\n",
    "def main():\n",
    "    # Fungsi utama untuk menjalankan proses\n",
    "    input_folder = \"data\"  # Menentukan folder input\n",
    "    output_folder = \"output/graph-node-only\"  # Menentukan folder output\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):  # Iterasi melalui setiap file dalam folder input\n",
    "        file_path = os.path.join(input_folder, file_name)  # Mendapatkan jalur lengkap file\n",
    "        if os.path.isfile(file_path):  # Memastikan bahwa jalur adalah file\n",
    "            process_document(file_path, output_folder)  # Memproses dokumen\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi utama saat skrip dieksekusi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - NODE WITH CHUNK RANK\n",
    "\n",
    "Metode ini menggunakan chunking berbasis graf, namun dengan tambahan perangkingan chunk berdasarkan jumlah kata atau kepadatan informasi.\n",
    "\n",
    "Cara Kerja:\n",
    "- Proses chunking sama seperti metode \"Node Only\".\n",
    "- Setelah chunk terbentuk, setiap chunk diberi skor berdasarkan jumlah kata.\n",
    "- Chunk dengan informasi lebih padat diberikan peringkat lebih tinggi.\n",
    "\n",
    "Kelebihan:\n",
    "- Memungkinkan ekstraksi chunk yang lebih informatif.\n",
    "- Cocok untuk proses summarization berbasis chunk.\n",
    "\n",
    "Kekurangan:\n",
    "- Ranking berdasarkan jumlah kata belum tentu mencerminkan makna semantik yang lebih penting.\n",
    "- Bisa menyebabkan perubahan urutan asli dokumen jika digunakan untuk reordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_pdf.pdf -> output/graph-chunk-rank\\dokumen_pdf\n",
      "Processed: data\\tko.pdf -> output/graph-chunk-rank\\tko\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import spacy  # Mengimpor spaCy untuk pemrosesan bahasa alami\n",
    "import docx  # Mengimpor modul docx untuk membaca file DOCX\n",
    "import PyPDF2  # Mengimpor PyPDF2 untuk membaca file PDF\n",
    "import networkx as nx  # Mengimpor NetworkX untuk analisis graf\n",
    "import gensim  # Mengimpor gensim untuk model topik\n",
    "import numpy as np  # Mengimpor numpy untuk operasi numerik\n",
    "from gensim import corpora  # Mengimpor corpora dari gensim untuk membuat kamus\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Mengimpor fungsi untuk menghitung kesamaan kosinus\n",
    "from sentence_transformers import SentenceTransformer  # Mengimpor SentenceTransformer untuk model embedding kalimat\n",
    "\n",
    "# Memuat model bahasa Inggris kecil dari spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Memuat model SentenceTransformer untuk embedding kalimat\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    # Membaca file teks dan mengembalikan isinya\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Membuka file dalam mode baca\n",
    "        return f.read()  # Mengembalikan seluruh isi file\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    # Membaca file PDF dan mengembalikan isinya\n",
    "    text = \"\"  # Inisialisasi string kosong untuk menyimpan teks\n",
    "    with open(file_path, \"rb\") as f:  # Membuka file PDF dalam mode biner\n",
    "        reader = PyPDF2.PdfReader(f)  # Membaca file PDF\n",
    "        for page in reader.pages:  # Iterasi melalui setiap halaman\n",
    "            text += page.extract_text() + \"\\n\"  # Ekstrak teks dari halaman dan tambahkan ke string\n",
    "    return text  # Mengembalikan teks yang diekstrak\n",
    "\n",
    "def read_docx(file_path):\n",
    "    # Membaca file DOCX dan mengembalikan isinya\n",
    "    doc = docx.Document(file_path)  # Membuka file DOCX\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])  # Menggabungkan teks dari setiap paragraf\n",
    "\n",
    "def graph_based_chunking(text, chunk_size=700):\n",
    "    # Membagi teks menjadi bagian-bagian kecil menggunakan pendekatan berbasis graf\n",
    "    doc = nlp(text)  # Memproses teks dengan spaCy\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]  # Mengambil kalimat yang tidak kosong\n",
    "    \n",
    "    if not sentences:  # Jika tidak ada kalimat yang valid\n",
    "        return []  # Mengembalikan daftar kosong\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)  # Menghitung embedding untuk setiap kalimat\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)  # Menghitung matriks kesamaan kosinus\n",
    "    G = nx.Graph()  # Membuat graf kosong\n",
    "    \n",
    "    # Menambahkan tepi ke graf berdasarkan kesamaan kalimat\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i, j])  # Menambahkan tepi dengan bobot kesamaan\n",
    "    \n",
    "    # Menggunakan algoritma komunitas untuk menemukan partisi dalam graf\n",
    "    partitions = nx.community.greedy_modularity_communities(G)\n",
    "    \n",
    "    chunks = []  # Inisialisasi daftar untuk menyimpan bagian\n",
    "    for community in partitions:  # Iterasi melalui setiap komunitas\n",
    "        chunk = \" \".join([sentences[i] for i in sorted(community)])  # Menggabungkan kalimat dalam komunitas menjadi satu bagian\n",
    "        if len(chunk) > chunk_size:  # Jika ukuran bagian melebihi chunk_size\n",
    "            # Membagi bagian menjadi sub-bagian yang lebih kecil\n",
    "            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            chunks.extend(sub_chunks)  # Menambahkan sub-bagian ke daftar\n",
    "        else:\n",
    "            chunks.append(chunk)  # Menambahkan bagian ke daftar\n",
    "    \n",
    "    return chunks  # Mengembalikan daftar bagian\n",
    "\n",
    "def extract_topics(text, num_topics=3):\n",
    "    # Mengekstrak topik dari teks menggunakan model LDA\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]  # Tokenisasi dan lemmatization\n",
    "\n",
    "    if not words:  # Jika tidak ada kata yang valid\n",
    "        return [\"[No valid words for topic extraction]\"]  # Mengembalikan pesan bahwa tidak ada kata valid\n",
    "\n",
    "    dictionary = corpora.Dictionary([words])  # Membuat kamus dari kata-kata\n",
    "    corpus = [dictionary.doc2bow(words)]  # Membuat korpus dari kata-kata\n",
    "\n",
    "    if not corpus or all(len(doc) == 0 for doc in corpus ):  # Jika korpus kosong\n",
    "        return [\"[No significant topics]\"]  # Mengembalikan pesan bahwa tidak ada topik yang signifikan\n",
    "\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=min(num_topics, len(dictionary)), \n",
    "    id2word=dictionary, passes=10)  # Membuat model LDA\n",
    "    topics = lda_model.print_topics()  # Mengambil topik yang dihasilkan oleh model\n",
    "\n",
    "    return topics  # Mengembalikan daftar topik\n",
    "\n",
    "def rank_chunks(chunks):\n",
    "    # Mengurutkan bagian berdasarkan jumlah kata dalam urutan menurun\n",
    "    ranked_chunks = sorted(chunks, key=lambda chunk: len(chunk.split()), reverse=True)  # Mengurutkan bagian\n",
    "    return ranked_chunks  # Mengembalikan daftar bagian yang terurut\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    # Memproses dokumen berdasarkan jenis file dan menyimpan hasilnya\n",
    "    ext = file_path.split(\".\")[-1].lower()  # Mendapatkan ekstensi file\n",
    "    \n",
    "    # Membaca teks berdasarkan jenis file\n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)  # Membaca file teks\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)  # Membaca file PDF\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)  # Membaca file DOCX\n",
    "    else:\n",
    "        return  # Jika jenis file tidak dikenali, keluar dari fungsi\n",
    "    \n",
    "    base_name = os.path.basename(file_path).split(\".\")[0]  # Mendapatkan nama dasar file tanpa ekstensi\n",
    "    file_output_folder = os.path.join(output_folder, base_name)  # Menentukan folder output untuk file ini\n",
    "    os.makedirs(file_output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    chunks = graph_based_chunking(text)  # Membagi teks menjadi bagian-bagian kecil\n",
    "    ranked_chunks = rank_chunks(chunks)  # Mengurutkan bagian berdasarkan panjangnya\n",
    "    \n",
    "    for i, chunk in enumerate(ranked_chunks):  # Iterasi melalui setiap bagian yang terurut\n",
    "        chunk_file = os.path.join(file_output_folder, f\"{base_name}_chunk_{i+1}.txt\")  # Menentukan nama file untuk bagian\n",
    "        topics = extract_topics(chunk)  # Mengekstrak topik dari bagian\n",
    "        \n",
    "        with open(chunk_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "            f.write(f\"--- Chunk {i+1} (Words: {len(chunk.split())}) ---\\n{chunk}\\n\\n\")  # Menulis bagian ke file\n",
    "            f.write(\"--- Topics ---\\n\")  # Menulis header untuk topik\n",
    "            for topic in topics:  # Iterasi melalui setiap topik\n",
    "                f.write(f\"{topic}\\n\")  # Menulis topik ke file\n",
    "\n",
    "    print(f\"Processed: {file_path} -> {file_output_folder}\")  # Menampilkan pesan bahwa file telah diproses\n",
    "\n",
    "def main():\n",
    "    # Fungsi utama untuk menjalankan proses\n",
    "    input_folder = \"data\"  # Menentukan folder input\n",
    "    output_folder = \"output/graph-chunk-rank\"  # Menentukan folder output\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):  # Iterasi melalui setiap file dalam folder input\n",
    "        file_path = os.path.join(input_folder, file_name)  # Mendapatkan jalur lengkap file\n",
    "        if os.path.isfile(file_path):  # Memastikan bahwa jalur adalah file\n",
    "            process_document(file_path, output_folder)  # Memproses dokumen\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi utama saat skrip dieksekusi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - NODE WITH TOPIC RANK\n",
    "\n",
    "Metode ini menggunakan chunking berbasis graf dan menambahkan perangkingan berbasis topik menggunakan LDA (Latent Dirichlet Allocation).\n",
    "\n",
    "Cara Kerja:\n",
    "- Setelah chunk terbentuk, topik utama diekstrak dari setiap chunk menggunakan Gensim LDA.\n",
    "- Topik terbaik dipilih berdasarkan probabilitas tertinggi dalam distribusi topik chunk tersebut.\n",
    "\n",
    "Kelebihan:\n",
    "- Memungkinkan identifikasi topik utama dalam setiap chunk.\n",
    "- Berguna untuk analisis topik otomatis dari dokumen panjang.\n",
    "\n",
    "Kekurangan:\n",
    "- LDA berbasis bag-of-words, sehingga tidak mempertimbangkan urutan kata.\n",
    "- Hasil topik bisa kurang akurat jika jumlah topik tidak ditentukan dengan baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_pdf.pdf -> output/graph-topic-rank\\dokumen_pdf\n",
      "Processed: data\\tko.pdf -> output/graph-topic-rank\\tko\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import spacy  # Mengimpor spaCy untuk pemrosesan bahasa alami\n",
    "import docx  # Mengimpor modul docx untuk membaca file DOCX\n",
    "import PyPDF2  # Mengimpor PyPDF2 untuk membaca file PDF\n",
    "import networkx as nx  # Mengimpor NetworkX untuk analisis graf\n",
    "import gensim  # Mengimpor gensim untuk model topik\n",
    "from gensim import corpora  # Mengimpor corpora dari gensim untuk membuat kamus\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Mengimpor fungsi untuk menghitung kesamaan kosinus\n",
    "from sentence_transformers import SentenceTransformer  # Mengimpor SentenceTransformer untuk model embedding kalimat\n",
    "\n",
    "# Memuat model bahasa Inggris kecil dari spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Memuat model SentenceTransformer untuk embedding kalimat\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    # Membaca file teks dan mengembalikan isinya\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Membuka file dalam mode baca\n",
    "        return f.read()  # Mengembalikan seluruh isi file\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    # Membaca file PDF dan mengembalikan isinya\n",
    "    text = \"\"  # Inisialisasi string kosong untuk menyimpan teks\n",
    "    with open(file_path, \"rb\") as f:  # Membuka file PDF dalam mode biner\n",
    "        reader = PyPDF2.PdfReader(f)  # Membaca file PDF\n",
    "        for page in reader.pages:  # Iterasi melalui setiap halaman\n",
    "            text += page.extract_text() + \"\\n\"  # Ekstrak teks dari halaman dan tambahkan ke string\n",
    "    return text  # Mengembalikan teks yang diekstrak\n",
    "\n",
    "def read_docx(file_path):\n",
    "    # Membaca file DOCX dan mengembalikan isinya\n",
    "    doc = docx.Document(file_path)  # Membuka file DOCX\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])  # Menggabungkan teks dari setiap paragraf\n",
    "\n",
    "def graph_based_chunking(text, chunk_size=700):\n",
    "    # Membagi teks menjadi bagian-bagian kecil menggunakan pendekatan berbasis graf\n",
    "    doc = nlp(text)  # Memproses teks dengan spaCy\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]  # Mengambil kalimat yang tidak kosong\n",
    "    \n",
    "    if not sentences:  # Jika tidak ada kalimat yang valid\n",
    "        return []  # Mengembalikan daftar kosong\n",
    "    \n",
    "    sentence_embeddings = model.encode(sentences)  # Menghitung embedding untuk setiap kalimat\n",
    "    similarity_matrix = cosine_similarity(sentence_embeddings)  # Menghitung matriks kesamaan kosinus\n",
    "    G = nx.Graph()  # Membuat graf kosong\n",
    "    \n",
    "    # Menambahkan tepi ke graf berdasarkan kesamaan kalimat\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(i + 1, len(sentences)):\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i, j])  # Menambahkan tepi dengan bobot kesamaan\n",
    "    \n",
    "    # Menggunakan algoritma komunitas untuk menemukan partisi dalam graf\n",
    "    partitions = nx.community.greedy_modularity_communities(G)\n",
    "    \n",
    "    chunks = []  # Inisialisasi daftar untuk menyimpan bagian\n",
    "    for community in partitions:  # Iterasi melalui setiap komunitas\n",
    "        chunk = \" \".join([sentences[i] for i in sorted(community)])  # Menggabungkan kalimat dalam komunitas menjadi satu bagian\n",
    "        if len(chunk) > chunk_size:  # Jika ukuran bagian melebihi chunk_size\n",
    "            # Membagi bagian menjadi sub-bagian yang lebih kecil\n",
    "            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]\n",
    "            chunks.extend(sub_chunks)  # Menambahkan sub-bagian ke daftar\n",
    "        else:\n",
    "            chunks.append(chunk)  # Menambahkan bagian ke daftar\n",
    "    \n",
    "    return chunks  # Mengembalikan daftar bagian\n",
    "\n",
    "def extract_topics(text, num_topics=3):\n",
    "    # Mengekstrak topik dari teks menggunakan model LDA\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]  # Tokenisasi dan lemmatization\n",
    "\n",
    "    if not words:  # Jika tidak ada kata yang valid\n",
    "        return [\"No valid words for topic extraction\"]  # Mengembalikan pesan bahwa tidak ada kata valid\n",
    "\n",
    "    dictionary = corpora.Dictionary([words])  # Membuat kamus dari kata-kata\n",
    "    corpus = [dictionary.doc2bow(words)]  # Membuat korpus dari kata-kata\n",
    "\n",
    "    if not corpus or all(len(doc) == 0 for doc in corpus):  # Jika korpus kosong\n",
    "        return [\"No significant topics\"]  # Mengembalikan pesan bahwa tidak ada topik yang signifikan\n",
    "\n",
    "    num_topics = min(num_topics, len(dictionary))  # Menentukan jumlah topik yang akan diekstrak\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)  # Membuat model LDA\n",
    "    topics = lda_model.show_topics(formatted=False)  # Mengambil topik yang dihasilkan oleh model\n",
    "\n",
    "    ranked_topics = sorted(topics, key=lambda x: -sum(prob for _, prob in x[1]))  # Mengurutkan topik berdasarkan probabilitas\n",
    "\n",
    "    return [f\"Topic {t[0]}: {[word for word, _ in t[1]]}\" for t in ranked_topics]  # Mengembalikan daftar topik yang terurut\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    # Memproses dokumen berdasarkan jenis file dan menyimpan hasilnya\n",
    "    ext = file_path.split(\".\")[-1].lower()  # Mendapatkan ekstensi file\n",
    "    \n",
    "    # Membaca teks berdasarkan jenis file\n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)  # Membaca file teks\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)  # Membaca file PDF\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)  # Membaca file DOCX\n",
    "    else:\n",
    "        return  # Jika jenis file tidak dikenali, keluar dari fungsi\n",
    "    \n",
    "    base_name = os.path.basename(file_path).split(\".\")[0]  # Mendapatkan nama dasar file tanpa ekstensi\n",
    "    file_output_folder = os.path.join(output_folder, base_name)  # Menentukan folder output untuk file ini\n",
    "    os.makedirs(file_output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    chunks = graph_based_chunking(text)  # Membagi teks menjadi bagian-bagian kecil\n",
    "\n",
    "    for i, chunk in enumerate(chunks):  # Iterasi melalui setiap bagian\n",
    "        chunk_file = os.path.join(file_output_folder, f\"{base_name}_chunk_{i+1}.txt\")  # Menentukan nama file untuk bagian\n",
    "        best_topics = extract_topics(chunk)  # Mengekstrak topik dari bagian\n",
    "\n",
    "        with open(chunk_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "            f.write(chunk + \"\\n\\nTopics:\\n\")  # Menulis bagian ke file\n",
    "            for topic in best_topics:  # Iterasi melalui setiap topik\n",
    "                f.write(f\"- {topic}\\n\")  # Menulis topik ke file\n",
    "\n",
    "    print(f\"Processed: {file_path} -> {file_output_folder}\")  # Menampilkan pesan bahwa file telah diproses\n",
    "\n",
    "def main():\n",
    "    # Fungsi utama untuk menjalankan proses\n",
    "    input_folder = \"data\"  # Menentukan folder input\n",
    "    output_folder = \"output/graph-topic-rank\"  # Menentukan folder output\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):  # Iterasi melalui setiap file dalam folder input\n",
    "        file_path = os.path.join(input_folder, file_name)  # Mendapatkan jalur lengkap file\n",
    "        if os.path.isfile(file_path):  # Memastikan bahwa jalur adalah file\n",
    "            process_document(file_path, output_folder)  # Memproses dokumen\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi utama saat skrip dieksekusi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADVANCED SEMANTIC CHUNKING - LLAMA WITH EMBEDDING RANK\n",
    "\n",
    "Metode ini menggunakan pemrosesan bahasa alami (NLP) berbasis spaCy untuk chunking teks, serta peringkat berbasis embedding menggunakan model `HuggingFaceEmbeddings`. Model LDA (Latent Dirichlet Allocation) digunakan untuk mengekstrak topik utama dari setiap chunk.\n",
    "\n",
    "Cara Kerja:\n",
    "- Dokumen dipecah menjadi chunk menggunakan model NLP spaCy.\n",
    "- Embedding setiap chunk dihitung menggunakan `HuggingFaceEmbeddings`.\n",
    "- Chunk diberi peringkat berdasarkan norma embedding menggunakan `numpy.linalg.norm`.\n",
    "- Topik utama dari setiap chunk diekstrak menggunakan Gensim LDA.\n",
    "- Topik terbaik dipilih berdasarkan probabilitas tertinggi dalam distribusi topik chunk tersebut.\n",
    "\n",
    "Kelebihan:\n",
    "- Menggunakan embedding untuk peringkat yang lebih akurat dibanding metode berbasis kata.\n",
    "- Mampu mengidentifikasi topik utama dalam setiap chunk dengan metode LDA.\n",
    "- Dapat diterapkan pada berbagai jenis dokumen (TXT, PDF, DOCX).\n",
    "\n",
    "Kekurangan:\n",
    "- LDA masih berbasis bag-of-words, sehingga tidak mempertimbangkan urutan kata.\n",
    "- Kualitas chunk tergantung pada model NLP yang digunakan.\n",
    "- Hasil topik bisa kurang akurat jika jumlah topik tidak ditentukan dengan baik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\dokumen_pdf.pdf -> output/llm-semantic-chunking\\dokumen_pdf\n",
      "Processed: data\\tko.pdf -> output/llm-semantic-chunking\\tko\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import spacy  # Mengimpor spaCy untuk pemrosesan bahasa alami\n",
    "import docx  # Mengimpor modul docx untuk membaca file DOCX\n",
    "import fitz  # Mengimpor fitz (PyMuPDF) untuk membaca file PDF\n",
    "import gensim  # Mengimpor gensim untuk model topik\n",
    "from gensim import corpora  # Mengimpor corpora dari gensim untuk membuat kamus\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Mengimpor model embedding dari Hugging Face\n",
    "from langchain_ollama.llms import OllamaLLM  # Mengimpor model LLM dari Ollama\n",
    "\n",
    "# Memuat model bahasa Inggris kecil dari spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "OLLAMA_MODEL = \"llama3.2\"  # Menentukan model Ollama yang akan digunakan\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL)  # Memuat model LLM\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # Memuat model embedding\n",
    "\n",
    "def read_txt(file_path):\n",
    "    # Membaca file teks dan mengembalikan isinya\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Membuka file dalam mode baca\n",
    "        return f.read()  # Mengembalikan seluruh isi file\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    # Membaca file PDF dan mengembalikan isinya\n",
    "    text = \"\"  # Inisialisasi string kosong untuk menyimpan teks\n",
    "    with fitz.open(file_path) as doc:  # Membuka file PDF menggunakan fitz\n",
    "        for page in doc:  # Iterasi melalui setiap halaman\n",
    "            text += page.get_text(\"text\") + \"\\n\"  # Ekstrak teks dari halaman dan tambahkan ke string\n",
    "    return text  # Mengembalikan teks yang diekstrak\n",
    "\n",
    "def read_docx(file_path):\n",
    "    # Membaca file DOCX dan mengembalikan isinya\n",
    "    doc = docx.Document(file_path)  # Membuka file DOCX\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])  # Menggabungkan teks dari setiap paragraf\n",
    "\n",
    "def clean_text(text):\n",
    "    # Membersihkan teks dengan menghapus newline dan menghapus spasi berlebih\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Mengganti newline dengan spasi dan menghapus spasi di awal/akhir\n",
    "    text = \" \".join(text.split())  # Menghapus spasi berlebih di antara kata\n",
    "    return text  # Mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "def semantic_chunking(text, chunk_size=700):\n",
    "    # Membagi teks menjadi bagian-bagian kecil berdasarkan ukuran yang ditentukan\n",
    "    doc = nlp(text)  # Memproses teks dengan spaCy\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]  # Mengambil kalimat yang tidak kosong\n",
    "    \n",
    "    if not sentences:  # Jika tidak ada kalimat yang valid\n",
    "        return []  # Mengembalikan daftar kosong\n",
    "    \n",
    "    chunks = []  # Inisialisasi daftar untuk menyimpan bagian\n",
    "    chunk = \"\"  # Inisialisasi string untuk bagian saat ini\n",
    "    for sentence in sentences:  # Iterasi melalui setiap kalimat\n",
    "        if len(chunk) + len(sentence) < chunk_size:  # Jika ukuran bagian saat ini ditambah kalimat masih dalam batas chunk_size\n",
    "            chunk += \" \" + sentence  # Tambahkan kalimat ke bagian saat ini\n",
    "        else:\n",
    "            chunks.append(chunk.strip())  # Tambahkan bagian saat ini ke daftar\n",
    "            chunk = sentence  # Mulai bagian baru dengan kalimat saat ini\n",
    "    if chunk:  # Jika ada bagian yang tersisa\n",
    "        chunks.append(chunk.strip())  # Tambahkan bagian terakhir ke daftar\n",
    "\n",
    "    return chunks  # Mengembalikan daftar bagian\n",
    "\n",
    "def extract_topics(text, num_topics=3):\n",
    "    # Mengekstrak topik dari teks menggunakan model LDA\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]  # Tokenisasi dan lemmatization\n",
    "\n",
    "    if not words:  # Jika tidak ada kata yang valid\n",
    "        return [\"[No valid words for topic extraction]\"]  # Mengembalikan pesan bahwa tidak ada kata valid\n",
    "\n",
    "    dictionary = corpora.Dictionary([words])  # Membuat kamus dari kata-kata\n",
    "    corpus = [dictionary.doc2bow(words)]  # Membuat korpus dari kata-kata\n",
    "\n",
    "    if not corpus or all(len(doc) == 0 for doc in corpus):  # Jika korpus kosong\n",
    "        return [\"[No significant topics]\"]  # Mengembalikan pesan bahwa tidak ada topik yang signifikan\n",
    "\n",
    "    lda_model = gensim.models.LdaMulticore(  # Membuat model LDA dengan dukungan multi-core\n",
    "        corpus, num_topics=min(num_topics, len(dictionary)),  # Menentukan jumlah topik yang akan diekstrak\n",
    "        id2word=dictionary, passes=10, workers=4  # Mengatur parameter model LDA\n",
    "    )\n",
    "    topics = lda_model.show_topics(formatted=False)  # Mengambil topik yang dihasilkan oleh model\n",
    "\n",
    "    return [\", \".join([word for word, _ in topic[1]]) for topic in topics]  # Mengembalikan daftar topik yang diekstrak\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    # Memproses dokumen berdasarkan jenis file dan menyimpan hasilnya\n",
    "    ext = file_path.split(\".\")[-1].lower()  # Mendapatkan ekstensi file\n",
    "    \n",
    "    # Membaca teks berdasarkan jenis file\n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)  # Membaca file teks\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)  # Membaca file PDF\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)  # Membaca file DOCX\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {ext}\")  # Menampilkan pesan jika format file tidak didukung\n",
    "        return  # Keluar dari fungsi\n",
    "    \n",
    "    base_name = os.path.basename(file_path).split(\".\")[0]  # Mendapatkan nama dasar file tanpa ekstensi\n",
    "    file_output_folder = os.path.join(output_folder, base_name)  # Menentukan folder output untuk file ini\n",
    "    os.makedirs(file_output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "\n",
    "    cleaned_text = clean_text(text)  # Membersihkan teks dari file\n",
    "    chunks = semantic_chunking(cleaned_text)  # Membagi teks menjadi bagian-bagian kecil\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):  # Iterasi melalui setiap bagian\n",
    "        chunk_file = os.path.join(file_output_folder, f\"{base_name}_chunk_{i+1}.txt\")  # Menentukan nama file untuk bagian\n",
    "        topics = extract_topics(chunk)  # Mengekstrak topik dari bagian\n",
    "        \n",
    "        with open(chunk_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "            f.write(chunk + \"\\n\\n\")  # Menulis bagian ke file\n",
    "            f.write(\"Topik:\\n\")  # Menulis header untuk topik\n",
    "            for topic in topics:  # Iterasi melalui setiap topik\n",
    "                f.write(f\"- {topic}\\n\")  # Menulis topik ke file\n",
    "\n",
    "    print(f\"Processed: {file_path} -> {file_output_folder}\")  # Menampilkan pesan bahwa file telah diproses\n",
    "\n",
    "def main():\n",
    "    # Fungsi utama untuk menjalankan proses\n",
    "    input_folder = \"data\"  # Menentukan folder input\n",
    "    output_folder = \"output/llm-semantic-chunking\"  # Menentukan folder output\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):  # Iterasi melalui setiap file dalam folder input\n",
    "        file_path = os.path.join(input_folder, file_name)  # Mendapatkan jalur lengkap file\n",
    "        if os.path.isfile(file_path):  # Memastikan bahwa jalur adalah file\n",
    "            process_document(file_path, output_folder)  # Memproses dokumen\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi utama saat skrip dieksekusi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Berikut adalah jawaban mengenai pertanyaan Anda berdasarkan informasi yang tersedia dalam dokumen:\n",
       "\n",
       "1. Tujuan dari tata kerja organisasi adalah untuk menentukan proses dan struktur kerja yang efektif dan efisien dalam mencapai tujuan perusahaan.\n",
       "2. Onsite Support adalah layanan teknis yang menyediakan dukungan langsung untuk aplikasi, sedangkan Aplikasi Upstream adalah aplikasi yang dikembangkan dan diterbitkan oleh tim pengembang di luar organisasi yang menerimanya.\n",
       "3. Dokumen terkait Tata Kerja Organisasi dapat ditemukan dalam bagian \"Referensi\" pada dokumen ini, yaitu:\n",
       " * Tata Kerja Organisasi\n",
       " * Prosedur Penggunaan Aplikasi\n",
       " * Standar Komunikasi Tim\n",
       "4. Procedur permintaan baru akun aplikasi adalah sebagai berikut:\n",
       " * Langkah 1: Isi formulir permintaan baru akun aplikasi yang tersedia pada dokumen ini.\n",
       " * Langkah 2: Klik tombol \"Submit\" untuk mengirimkan permintaan.\n",
       " * Langkah 3: Tim pengembang akan memproses permintaan dan melakukan verifikasi bahwa akun tersebut dapat digunakan dengan aman.\n",
       " * Langkah 4: Jika permintaan diverifikasi, akun baru akan diaktifkan dan pengguna akan mendapatkan notifikasi.\n",
       "\n",
       "Perlu diingat bahwa prosedur ini mungkin berbeda-beda tergantung pada kebutuhan dan kebijakan organisasi."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "from IPython.display import Markdown  # Mengimpor Markdown untuk menampilkan teks dalam format Markdown di Jupyter Notebook\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Mengimpor ChatPromptTemplate untuk membuat template prompt\n",
    "from langchain_ollama.llms import OllamaLLM  # Mengimpor OllamaLLM untuk menggunakan model LLM\n",
    "\n",
    "OLLAMA_MODEL = \"llama3.2\"  # Menentukan model LLM yang akan digunakan\n",
    "\n",
    "# Memuat model LLM\n",
    "model = OllamaLLM(model=OLLAMA_MODEL)  # Menginisialisasi model LLM dengan nama model yang ditentukan\n",
    "\n",
    "# Template prompt untuk ringkasan\n",
    "template = \"\"\"\n",
    "1. Apa tujuan dari tata kerja organisasi?\n",
    "2. Apa pengertian Onsite Support dan Aplikasi Upstream?\n",
    "3. Apa dokumen dan referensi terkait Tata Kerja Organisasi?\n",
    "4. Bagaimana prosedur permintaan baru akun aplikasi?\n",
    "Gunakan hanya informasi yang terdapat dalam dokumen.  \n",
    "\n",
    "Dokumen:\n",
    "\"{document}\"\n",
    "\"\"\"  # Template yang akan digunakan untuk meminta ringkasan dari model LLM\n",
    "\n",
    "# Membuat objek ChatPromptTemplate dari template yang telah ditentukan\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def summarize_text(text):\n",
    "    chain = prompt | model  # Menggabungkan prompt dengan model LLM untuk membuat rantai pemrosesan\n",
    "    response = chain.invoke({\"document\": text})  # Mengirimkan teks ke model LLM untuk diringkas\n",
    "    return response  # Mengembalikan hasil ringkasan\n",
    "\n",
    "def read_all_chunks(folder):\n",
    "    all_text = []  # Inisialisasi daftar untuk menyimpan isi semua file\n",
    "    \n",
    "    # Ambil semua file dalam folder yang dimulai dengan \"chunk_\" dan diakhiri dengan \".txt\", lalu urutkan berdasarkan nama file\n",
    "    files = sorted([f for f in os.listdir(folder) if f.startswith(\"chunk_\") and f.endswith(\".txt\")])\n",
    "    \n",
    "    for file in files:  # Iterasi melalui setiap file yang ditemukan\n",
    "        file_path = os.path.join(folder, file)  # Mendapatkan jalur lengkap file\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Membuka file dalam mode baca\n",
    "            all_text.append(f.read())  # Tambahkan isi file ke daftar\n",
    "    \n",
    "    return \"\\n\\n\".join(all_text)  # Gabungkan semua isi file dengan pemisah newline dan kembalikan sebagai satu string\n",
    "\n",
    "# Baca semua chunk dari folder\n",
    "output_folder = \"output/llm-semantic-chunking/tko\"  # Menentukan folder output yang berisi file chunk\n",
    "document_text = read_all_chunks(output_folder)  # Membaca semua chunk dan menggabungkan isinya\n",
    "\n",
    "# Buat ringkasan\n",
    "summary = summarize_text(document_text)  # Menggunakan fungsi untuk meringkas teks yang telah dibaca\n",
    "\n",
    "# Tampilkan hasil sebagai Markdown\n",
    "Markdown(summary)  # Menampilkan ringkasan dalam format Markdown di Jupyter Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
