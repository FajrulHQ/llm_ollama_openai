{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESS DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dokumen (1).pdf - Panjang teks sebelum pemrosesan: 3843\n",
      "[INFO] Total chunks generated for Dokumen (1).pdf: 4\n",
      "[SUCCESS] Proses chunking selesai. Hasilnya disimpan dalam 'chunked_data' dan metadata di 'metadata'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from IPython.display import Markdown\n",
    "from phi.agent import Agent\n",
    "from phi.model.ollama import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from utils.document_processor import DocumentProcessor  \n",
    "\n",
    "# Inisialisasi path dan model\n",
    "DATA_PATH = \"./data\"\n",
    "INDEX_PATH = \"faiss_index\"\n",
    "CHUNKED_DATA_PATH = \"./chunked_data\"  \n",
    "METADATA_PATH = \"./metadata\"  \n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "\n",
    "# Pastikan folder tersedia\n",
    "os.makedirs(CHUNKED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(METADATA_PATH, exist_ok=True)\n",
    "\n",
    "# Inisialisasi model Ollama\n",
    "llm = Ollama(id=OLLAMA_MODEL)\n",
    "\n",
    "# Inisialisasi embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Inisialisasi document processor\n",
    "docs = DocumentProcessor()\n",
    "\n",
    "# Inisialisasi Agent dengan model Ollama\n",
    "agent = Agent(model=llm, show_tool_calls=True, markdown=True)\n",
    "\n",
    "# Parameter chunking\n",
    "CHUNK_SIZE = 1200  # Ditingkatkan untuk efisiensi\n",
    "MIN_CHUNK_SIZE = 500  # Gabungkan chunk kecil\n",
    "MAX_CHUNKS = 30  # Batasi jumlah chunk\n",
    "\n",
    "extracted_docs = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" Membersihkan teks dari karakter kosong, whitespace berlebih, dan simbol aneh \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  \n",
    "    return text\n",
    "\n",
    "def clean_agent_output(text):\n",
    "    \"\"\" Membersihkan output dari agent agar tidak mengandung pemisah yang tidak perlu \"\"\"\n",
    "    text = re.sub(r'\\n?###.*?\\n', '\\n', text)  # Hapus judul markdown seperti ### Section\n",
    "    text = re.sub(r'\\n?\\*\\*\\*.*?\\n', '\\n', text)  # Hapus pemisah ***\n",
    "    text = re.sub(r'\\n?-{3,}\\n?', '\\n', text)  # Hapus garis pemisah ---\n",
    "    text = re.sub(r'(\\s*-{2,}\\s*)', ' ', text)  # Hapus pemisah --\n",
    "    text = re.sub(r'(\\s*\\*{2,}\\s*)', ' ', text)  # Hapus ** pemisah tebal\n",
    "    text = re.sub(r'(\\s*\\*\\s*)', ' ', text)  # Hapus * pemisah tunggal\n",
    "    text = re.sub(r'(\\s*-\\s*)', ' ', text)  # Hapus pemisah -\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text).strip() \n",
    "    return text\n",
    "\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    valid_extensions = ('.pdf', '.docx', '.txt')\n",
    "    if not filename.lower().endswith(valid_extensions):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(DATA_PATH, filename)\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            document = f.read()\n",
    "            result = docs.process_document(document, filename)\n",
    "\n",
    "        if not result or len(result) < 4:\n",
    "            print(f\"[WARNING] Gagal memproses {filename}, melewati file ini.\")\n",
    "            continue\n",
    "\n",
    "        plain_text = clean_text(result[3])  \n",
    "        print(f\"[INFO] {filename} - Panjang teks sebelum pemrosesan: {len(plain_text)}\")\n",
    "\n",
    "        # Gunakan agent untuk chunking secara bertahap\n",
    "        structured_text = \"\"\n",
    "        start_idx = 0\n",
    "        chunk_count = 0\n",
    "\n",
    "        while start_idx < len(plain_text) and chunk_count < MAX_CHUNKS:\n",
    "            chunk_text = plain_text[start_idx:start_idx + CHUNK_SIZE]\n",
    "            response = agent.run(\n",
    "                f\"Split the following text into meaningful segments ensuring logical separation:\\n{chunk_text}\",\n",
    "                max_tokens=8000\n",
    "            )\n",
    "            \n",
    "            if isinstance(response, str):\n",
    "                structured_text += clean_agent_output(response) + \"\\n\\n\"\n",
    "            elif isinstance(response, dict):\n",
    "                structured_text += clean_agent_output(response.get(\"text\", \"\")) + \"\\n\\n\"\n",
    "            else:\n",
    "                structured_text += clean_agent_output(getattr(response, \"content\", str(response))) + \"\\n\\n\"\n",
    "            \n",
    "            start_idx += CHUNK_SIZE\n",
    "            chunk_count += 1\n",
    "\n",
    "        structured_text = structured_text.strip()\n",
    "        chunked_texts = structured_text.split(\"\\n\\n\")\n",
    "\n",
    "        # Gabungkan chunk yang terlalu pendek\n",
    "        optimized_chunks = []\n",
    "        temp_chunk = \"\"\n",
    "\n",
    "        for chunk in chunked_texts:\n",
    "            chunk = chunk.strip()\n",
    "            if len(chunk) < MIN_CHUNK_SIZE:\n",
    "                temp_chunk += \" \" + chunk\n",
    "            else:\n",
    "                if temp_chunk:\n",
    "                    optimized_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = \"\"\n",
    "                optimized_chunks.append(chunk)\n",
    "\n",
    "        if temp_chunk:\n",
    "            optimized_chunks.append(temp_chunk.strip())\n",
    "\n",
    "        chunk_data = [{\"chunk_id\": i+1, \"text\": chunk.strip()} \n",
    "                      for i, chunk in enumerate(optimized_chunks[:MAX_CHUNKS]) if chunk.strip()]\n",
    "\n",
    "        metadata = {\n",
    "            \"filename\": filename,\n",
    "            \"total_chunks\": len(chunk_data),\n",
    "            \"total_length\": len(plain_text)\n",
    "        }\n",
    "\n",
    "        extracted_docs.extend([\n",
    "            Document(page_content=chunk[\"text\"], metadata={\"chunk_id\": chunk[\"chunk_id\"], **metadata}) \n",
    "            for chunk in chunk_data\n",
    "        ])\n",
    "\n",
    "        # Simpan hasil chunking ke file\n",
    "        chunked_filepath = os.path.join(CHUNKED_DATA_PATH, f\"chunked_{filename}.txt\")\n",
    "        with open(chunked_filepath, \"w\", encoding=\"utf-8\") as chunked_file:\n",
    "            for chunk in chunk_data:\n",
    "                chunked_file.write(f\"Chunk {chunk['chunk_id']}:\\n\")\n",
    "                chunked_file.write(f\"{chunk['text']}\\n\")\n",
    "                chunked_file.write(\"\\n---\\n\\n\")  \n",
    "\n",
    "        metadata_filepath = os.path.join(METADATA_PATH, f\"metadata_{filename}.json\")\n",
    "        with open(metadata_filepath, \"w\", encoding=\"utf-8\") as metadata_file:\n",
    "            json.dump(metadata, metadata_file, indent=4)\n",
    "\n",
    "        print(f\"[INFO] Total chunks generated for {filename}: {len(chunk_data)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error processing {filename}: {e}\")\n",
    "\n",
    "# Simpan ke FAISS hanya jika ada dokumen yang diproses\n",
    "if extracted_docs:\n",
    "    vector_store = FAISS.from_documents(extracted_docs, embedding_model)\n",
    "    vector_store.save_local(INDEX_PATH)\n",
    "    print(\"[SUCCESS] Proses chunking selesai. Hasilnya disimpan dalam 'chunked_data' dan metadata di 'metadata'.\")\n",
    "else:\n",
    "    print(\"[INFO] Tidak ada dokumen yang berhasil diproses.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query dengan RAG\n",
    "query = \"Apa isi dokumen tentang topik X?\"\n",
    "query_embedding = embedding_model.embed_query(query)\n",
    "retrieved_docs = vector_store.similarity_search_by_vector(query_embedding, k=3)\n",
    "\n",
    "retrieved_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST OLLAMA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "COLLECTION_NAME = \"ollama_vectore_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Dari dokumen yang disediakan, saya dapat melihat bahwa terdapat beberapa perubahan nama, restrukturisasi, dan strategi ekspansi PT Pertamina sebagai berikut:\n",
       "\n",
       "Perubahan Nama:\n",
       "\n",
       "* PT Eksploitasi Tambang Minyak Sumatera Utara menjadi PT Perusahaan Minyak Nasional (PERMINA) pada 10 Desember 1957\n",
       "* PERMINA menjadi PN Pertambangan Minyak Nasional (Permina) pada 1 Juli 1961\n",
       "* PN Permina bergabung dengan PN Pertamin menjadi PN Pertambangan Minyak dan Gas Bumi Negara (Pertamina) pada 20 Agustus 1968\n",
       "\n",
       "Restrukturisasi:\n",
       "\n",
       "* Pada tahun 2003, PT Pertamina (Persero) didirikan melalui Peraturan Pemerintah No. 31 Tahun 2003\n",
       "* Pada tahun 2011, PT Pertamina menyempurnakan visinya menjadi Perusahaan Energi Nasional Kelas Dunia\n",
       "\n",
       "Strategi Ekspansi:\n",
       "\n",
       "* PT Pertamina mengacuikan visinya untuk menjadi Perusahaan Minyak Nasional Kelas Dunia pada tahun 2007\n",
       "* Pada tahun 2017, PT Pertamina berhasil menuntaskan akuisisi 72,65% saham perusahaan migas Prancis Maurel et Prom (M&P) yang memungkinkan PT Pertamina memiliki akses operasi di 12 negara di 4 benua\n",
       "* Pada tahun 2018, PT Pertamina berhasil menyelesaikan proses pengambilalihan kepemilikan 51% saham PT Pertamina Gas (Pertagas) oleh PT Perusahaan Gas Negara (PGN)\n",
       "\n",
       "Dalam keseluruhan, perubahan nama, restrukturisasi, dan strategi ekspansi PT Pertamina menunjukkan komitmen perusahaan untuk menjadi salah satu penyedia energi terbesar di Indonesia."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pastikan hanya data chunk yang digunakan dalam prompt\n",
    "template = \"\"\"\n",
    "Anda adalah asisten AI yang ahli dalam menganalisis dokumen.  \n",
    "Berdasarkan dokumen berikut, identifikasi perubahan nama, restrukturisasi, dan strategi ekspansi PT Pertamina.  \n",
    "Gunakan hanya informasi yang terdapat dalam dokumen.  \n",
    "\n",
    "Dokumen:\n",
    "\"{document}\"\n",
    "Ringkasan:\n",
    "\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=OLLAMA_MODEL)\n",
    "chain = template | model\n",
    "\n",
    "response = chain.invoke({\"document\": retrieved_text})\n",
    "Markdown(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST OPENAI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "PT. Pertamina (Persero) journey began in the 1950s with the establishment of PT. Eksploitasi Tambang Minyak Sumatera Utara. In 1957, it changed its name to become PT. Perusahaan Minyak Nasional (PERMINA), which was later renamed to PN Pertambangan Minyak Nasional (PERMINA) and eventually PN Pertambangan Minyak dan Gas Bumi Negara (Pertamina) in 1968. In 1971, the government established the role of Pertamina as a company to produce and process oil and gas and changed its name to Perusahaan Pertambangan Minyak dan Gas Bumi Negara. In 2003, it changed its name to PT Pertamina (Persero). In 2007, Pertamina changed its vision to become a world-class national oil company. In 2011, it further improved its vision to become a world-class national energy company. Pertamina acquired Maurel et Prom (M&P) in 2017, which allowed it to operate in 12 countries across four continents. In 2018, Pertamina's position as the first line of defense for national energy sovereignty was strengthened after PT Pertamina Gas was acquired by PT Perusahaan Gas Negara (PGN). Pertamina formed six subholdings to achieve energy sovereignty, namely Upstream Subholding, Gas Subholding, Refinery and Petrochemical Subholding, Power & NRE Subholding, Commercial and Trading Subholding, and Integrated Marine Logistics Subholding. This restructuring was completed in September 2021, and Pertamina aims to achieve its \"One Energy, One Pertamina\" vision through more focused and directed energy procurement activities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = \"gpt-35-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "  engine=model,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\", \n",
    "      \"content\": \"\"\"\n",
    "        You are a helpful assistant for text summarization.\n",
    "        Only include information that is part of the document. \n",
    "        Do not include your own opinion or analysis.\n",
    "      \"\"\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": plain_text\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
