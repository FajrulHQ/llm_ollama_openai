{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESS DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dokumen (1).pdf - Panjang teks sebelum pemrosesan: 3986\n",
      "[INFO] Mengirim chunk 1 ke agent (panjang: 1000)\n",
      "[INFO] Mengirim chunk 2 ke agent (panjang: 999)\n",
      "[INFO] Mengirim chunk 3 ke agent (panjang: 998)\n",
      "[INFO] Mengirim chunk 4 ke agent (panjang: 945)\n",
      "[INFO] Mengirim chunk 5 ke agent (panjang: 631)\n",
      "[INFO] Dokumen (1).pdf - Panjang teks setelah agent: 5410\n",
      "Total chunks generated for Dokumen (1).pdf: 7\n",
      "Proses chunking selesai. Hasilnya disimpan dalam folder 'chunked_data' dan metadata di 'metadata'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from IPython.display import Markdown\n",
    "from phi.agent import Agent\n",
    "from phi.model.ollama import Ollama\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from utils.document_processor import DocumentProcessor  \n",
    "\n",
    "# Inisialisasi path dan model\n",
    "DATA_PATH = \"./data\"\n",
    "INDEX_PATH = \"faiss_index\"\n",
    "CHUNKED_DATA_PATH = \"./chunked_data\"  \n",
    "METADATA_PATH = \"./metadata\"  \n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "\n",
    "# Pastikan folder tersedia\n",
    "os.makedirs(CHUNKED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(METADATA_PATH, exist_ok=True)\n",
    "\n",
    "# Inisialisasi model Ollama\n",
    "llm = Ollama(id=OLLAMA_MODEL)\n",
    "\n",
    "# Inisialisasi embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Inisialisasi document processor\n",
    "docs = DocumentProcessor()\n",
    "\n",
    "# Fungsi untuk chunking teks\n",
    "def chunk_text(text: str):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=200, length_function=len, separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# Inisialisasi Agent dengan model Ollama\n",
    "agent = Agent(model=llm, show_tool_calls=True, markdown=True)\n",
    "\n",
    "# Langkah 1: Proses dokumen dengan agentic chunking\n",
    "extracted_docs = []\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    valid_extensions = ('.pdf', '.docx', '.txt')\n",
    "    if not filename.lower().endswith(valid_extensions):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(DATA_PATH, filename)\n",
    "\n",
    "    try:\n",
    "        # Gunakan DocumentProcessor untuk membaca file\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            document = f.read()\n",
    "            result = docs.process_document(document, filename)\n",
    "\n",
    "        if result is None:\n",
    "            print(f\"Gagal memproses {filename}, melewati file ini.\")\n",
    "            continue\n",
    "\n",
    "        plain_text = result[3]  # Pastikan indeks benar\n",
    "\n",
    "        print(f\"[INFO] {filename} - Panjang teks sebelum pemrosesan: {len(plain_text)}\")\n",
    "\n",
    "        # **Pecah dokumen menjadi bagian lebih kecil sebelum dikirim ke Agent**\n",
    "        text_chunks = chunk_text(plain_text)\n",
    "        structured_chunks = []\n",
    "\n",
    "        for i, chunk in enumerate(text_chunks):\n",
    "            print(f\"[INFO] Mengirim chunk {i+1} ke agent (panjang: {len(chunk)})\")\n",
    "            response = agent.run(\n",
    "                f\"Organize the following text into well-structured segments:\\n{chunk}\",\n",
    "                max_tokens=4000\n",
    "            )\n",
    "\n",
    "            # Ambil teks dari response\n",
    "            if isinstance(response, str):\n",
    "                response_text = response\n",
    "            elif isinstance(response, dict):\n",
    "                response_text = response.get(\"text\", \"\")\n",
    "            else:\n",
    "                response_text = response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "            response_text = response_text.strip()\n",
    "            structured_chunks.append(response_text)\n",
    "\n",
    "        # **Gabungkan kembali hasil agent**\n",
    "        final_text = \"\\n\\n\".join(structured_chunks)\n",
    "\n",
    "        print(f\"[INFO] {filename} - Panjang teks setelah agent: {len(final_text)}\")\n",
    "\n",
    "        # Lakukan chunking ulang setelah agent\n",
    "        final_chunks = chunk_text(final_text)\n",
    "\n",
    "        # Simpan hasil chunking dalam list\n",
    "        chunk_data = [{\"chunk_id\": i+1, \"text\": chunk} for i, chunk in enumerate(final_chunks)]\n",
    "        metadata = {\"filename\": filename, \"total_chunks\": len(final_chunks)}\n",
    "\n",
    "        extracted_docs.extend([Document(page_content=chunk[\"text\"], metadata={\"chunk_id\": chunk[\"chunk_id\"], **metadata}) for chunk in chunk_data])\n",
    "\n",
    "        # Simpan hasil chunking dalam file txt\n",
    "        chunked_filepath = os.path.join(CHUNKED_DATA_PATH, f\"chunked_{filename}.txt\")\n",
    "        with open(chunked_filepath, \"w\", encoding=\"utf-8\") as chunked_file:\n",
    "            for chunk in chunk_data:\n",
    "                chunked_file.write(f\"Chunk {chunk['chunk_id']}:\\n\")\n",
    "                chunked_file.write(f\"{chunk['text']}\\n\")\n",
    "                chunked_file.write(\"\\n---\\n\\n\")  \n",
    "\n",
    "        # Simpan metadata dalam file JSON\n",
    "        metadata_filepath = os.path.join(METADATA_PATH, f\"metadata_{filename}.json\")\n",
    "        with open(metadata_filepath, \"w\", encoding=\"utf-8\") as metadata_file:\n",
    "            json.dump(metadata, metadata_file, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Simpan ke FAISS secara lokal\n",
    "vector_store = FAISS.from_documents(extracted_docs, embedding_model)\n",
    "vector_store.save_local(INDEX_PATH)\n",
    "\n",
    "print(f\"Total chunks generated for {filename}: {len(final_chunks)}\")\n",
    "print(\"Proses chunking selesai. Hasilnya disimpan dalam folder 'chunked_data' dan metadata di 'metadata'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query dengan RAG\n",
    "query = \"Apa isi dokumen tentang topik X?\"\n",
    "query_embedding = embedding_model.embed_query(query)\n",
    "retrieved_docs = vector_store.similarity_search_by_vector(query_embedding, k=3)\n",
    "\n",
    "# Gabungkan dokumen hasil pencarian untuk input ke model\n",
    "retrieved_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST OLLAMA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "COLLECTION_NAME = \"ollama_vectore_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Berikut ringkasan dokumen:\n",
       "\n",
       "PT Pertamina (Persero) adalah perusahaan energi nasional yang didirikan pada tahun 1950-an sebagai PT Eksploitasi Tambang Minyak Sumatera Utara. Pada tahun 1961, nama perusahaan diubah menjadi Perusahaan Negara (PN) dengan nama PN Pertambangan Minyak Nasional (Permina). Pada tahun 1971, PN Permina bergabung dengan PN Pertamin untuk membentuk PN Pertambangan Minyak dan Gas Bumi Negara (Pertamina). Pada tahun 2003, nama perusahaan diubah menjadi PT Pertamina (Persero).\n",
       "\n",
       "Pada tahun 2007, PERTAMINA mengubah visi perusahaan menjadi \"Menjadi Perusahaan Minyak Nasional Kelas Dunia\". Pada tahun 2011, visi tersebut disempurnakan menjadi \"Menjadi Perusahaan Energi Nasional Kelas Dunia\".\n",
       "\n",
       "Pada tahun 2017, PERTAMINA menuntaskan akuisisi 72,65% saham perusahaan migas Prancis Maurel et Prom (M&P). Pada tahun 2018, PT Pertamina Gas (Pertagas) bergabung dengan PT Perusahaan Gas Negara (PGN), sehingga PERTAMINA memantapkan posisinya sebagai garda terdepan yang menjaga ketahanan energi nasional.\n",
       "\n",
       "Pada tahun 2020 dan 2021, PERTAMINA melaksanakan restrukturisasi pembentukan subholding baru, termasuk Upstream Subholding, Gas Subholding, Refinery and Petrochemical Subholding, Power & NRE Subholding, Commercial and Trading Subholding, dan Integrated Marine Logistics Subholding."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "  Anda adalah asisten yang membantu dalam meringkas teks.  \n",
    "  Hanya sertakan informasi yang ada dalam dokumen.  \n",
    "  Jangan tambahkan opini atau analisis Anda sendiri.  \n",
    "\n",
    "  Dokumen:  \n",
    "  \"{document}\"  \n",
    "  Ringkasan:  \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=OLLAMA_MODEL)\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"document\": plain_text})\n",
    "Markdown(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST OPENAI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = \"gpt-35-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "  engine=model,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\", \n",
    "      \"content\": \"\"\"\n",
    "        You are a helpful assistant for text summarization.\n",
    "        Only include information that is part of the document. \n",
    "        Do not include your own opinion or analysis.\n",
    "      \"\"\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": plain_text\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
