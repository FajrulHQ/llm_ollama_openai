{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 10 chunks successfully saved to 'overlap_chunking\\chunks.txt'\n",
      "\n",
      "Chunking Analysis:\n",
      "Total chunks: 10\n",
      "Average chunk size: 1087 characters\n",
      "Min/Max chunk size: 754/1444 characters\n",
      "\n",
      "Sentences per chunk:\n",
      "Min: 7\n",
      "Max: 25\n",
      "Average: 12.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import nltk\n",
    "import re\n",
    "import textwrap\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class PDFChunker:\n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=200, line_width=80):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.line_width = line_width\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from PDF file with better formatting.\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text_blocks = []\n",
    "            \n",
    "            for page in doc:\n",
    "                blocks = page.get_text(\"blocks\")\n",
    "                for block in blocks:\n",
    "                    clean_text = block[4].strip()\n",
    "                    if clean_text:\n",
    "                        text_blocks.append(clean_text)\n",
    "            \n",
    "            text = \" \".join(text_blocks)\n",
    "            return self.clean_text(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if 'doc' in locals():\n",
    "                doc.close()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text while preserving structure.\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'([.!?])\\s*', r'\\1 ', text)\n",
    "        text = re.sub(r'\\s+([.!?])', r'\\1', text)\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def create_improved_chunks(self, text):\n",
    "        \"\"\"Create chunks with improved sentence handling.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            sentence_length = len(sentence)\n",
    "\n",
    "            if not current_chunk:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length = sentence_length\n",
    "                continue\n",
    "\n",
    "            if current_length + len(\" \") + sentence_length <= self.chunk_size:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += len(\" \") + sentence_length\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = sentence_length\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        final_chunks = []\n",
    "        for i in range(len(chunks)):\n",
    "            if i > 0:\n",
    "                prev_chunk_sentences = sent_tokenize(chunks[i-1])\n",
    "                overlap_sentences = prev_chunk_sentences[-2:] if len(prev_chunk_sentences) > 2 else prev_chunk_sentences\n",
    "                current_chunk = \" \".join(overlap_sentences) + \" \" + chunks[i]\n",
    "                final_chunks.append(current_chunk)\n",
    "            else:\n",
    "                final_chunks.append(chunks[i])\n",
    "\n",
    "        return final_chunks\n",
    "\n",
    "    def wrap_text(self, text):\n",
    "        \"\"\"Wrap text to specified line width while preserving paragraphs.\"\"\"\n",
    "        # Split text into paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        \n",
    "        # Wrap each paragraph\n",
    "        wrapped_paragraphs = []\n",
    "        for paragraph in paragraphs:\n",
    "            # Wrap the paragraph text\n",
    "            wrapped = textwrap.fill(paragraph.strip(), width=self.line_width)\n",
    "            wrapped_paragraphs.append(wrapped)\n",
    "        \n",
    "        # Join paragraphs with double newlines\n",
    "        return '\\n\\n'.join(wrapped_paragraphs)\n",
    "\n",
    "    def save_chunks_to_file(self, chunks, output_folder=\"chunks_output\"):\n",
    "        \"\"\"Save chunks to a single file with proper text wrapping.\"\"\"\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        output_file = os.path.join(output_folder, \"chunks.txt\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                for i, chunk in enumerate(chunks, 1):\n",
    "                    # Write chunk header\n",
    "                    header = f\"Chunk {i}\"\n",
    "                    file.write(f\"{header}\\n\")\n",
    "                    file.write(\"=\"* len(header) + \"\\n\\n\")\n",
    "                    \n",
    "                    # Write wrapped chunk content\n",
    "                    wrapped_content = self.wrap_text(chunk)\n",
    "                    file.write(wrapped_content)\n",
    "                    file.write(\"\\n\\n\\n\")  # Add extra spacing between chunks\n",
    "\n",
    "            print(f\"✅ {len(chunks)} chunks successfully saved to '{output_file}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving chunks: {e}\")\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        \"\"\"Process PDF and create better chunks.\"\"\"\n",
    "        text = self.extract_text_from_pdf(pdf_path)\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        return self.create_improved_chunks(text)\n",
    "\n",
    "    def analyze_chunks(self, chunks):\n",
    "        \"\"\"Analyze chunks with improved metrics.\"\"\"\n",
    "        if not chunks:\n",
    "            return {\n",
    "                \"total_chunks\": 0,\n",
    "                \"average_size\": 0,\n",
    "                \"size_stats\": {},\n",
    "                \"sentence_stats\": {}\n",
    "            }\n",
    "\n",
    "        chunk_sizes = [len(chunk) for chunk in chunks]\n",
    "        sentences_per_chunk = [len(sent_tokenize(chunk)) for chunk in chunks]\n",
    "\n",
    "        analysis = {\n",
    "            \"total_chunks\": len(chunks),\n",
    "            \"average_size\": sum(chunk_sizes) / len(chunks),\n",
    "            \"size_stats\": {\n",
    "                \"min\": min(chunk_sizes),\n",
    "                \"max\": max(chunk_sizes),\n",
    "                \"avg\": sum(chunk_sizes) / len(chunks)\n",
    "            },\n",
    "            \"sentence_stats\": {\n",
    "                \"min_sentences\": min(sentences_per_chunk),\n",
    "                \"max_sentences\": max(sentences_per_chunk),\n",
    "                \"avg_sentences\": sum(sentences_per_chunk) / len(sentences_per_chunk)\n",
    "            }\n",
    "        }\n",
    "        return analysis\n",
    "\n",
    "def main():\n",
    "    # Initialize chunker with line width for text wrapping\n",
    "    chunker = PDFChunker(chunk_size=1000, chunk_overlap=200, line_width=80)\n",
    "    \n",
    "    # Configuration\n",
    "    pdf_path = \"tko.pdf\"\n",
    "    output_folder = \"overlap_chunking\"\n",
    "    \n",
    "    # Process PDF\n",
    "    chunks = chunker.process_pdf(pdf_path)\n",
    "    \n",
    "    if chunks:\n",
    "        # Save results\n",
    "        chunker.save_chunks_to_file(chunks, output_folder)\n",
    "        \n",
    "        # Analyze and display results\n",
    "        analysis = chunker.analyze_chunks(chunks)\n",
    "        print(\"\\nChunking Analysis:\")\n",
    "        print(f\"Total chunks: {analysis['total_chunks']}\")\n",
    "        print(f\"Average chunk size: {analysis['size_stats']['avg']:.0f} characters\")\n",
    "        print(f\"Min/Max chunk size: {analysis['size_stats']['min']}/{analysis['size_stats']['max']} characters\")\n",
    "        print(f\"\\nSentences per chunk:\")\n",
    "        print(f\"Min: {analysis['sentence_stats']['min_sentences']:.0f}\")\n",
    "        print(f\"Max: {analysis['sentence_stats']['max_sentences']:.0f}\")\n",
    "        print(f\"Average: {analysis['sentence_stats']['avg_sentences']:.1f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Memproses chunk 1/10...\n",
      "🔹 Memproses chunk 2/10...\n",
      "🔹 Memproses chunk 3/10...\n",
      "🔹 Memproses chunk 4/10...\n",
      "🔹 Memproses chunk 5/10...\n",
      "🔹 Memproses chunk 6/10...\n",
      "🔹 Memproses chunk 7/10...\n",
      "🔹 Memproses chunk 8/10...\n",
      "🔹 Memproses chunk 9/10...\n",
      "🔹 Memproses chunk 10/10...\n",
      "✅ Ringkasan berhasil disimpan di 'overlap_chunking\\summary.txt'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL)\n",
    "\n",
    "# Template prompt untuk ringkasan per chunk\n",
    "template = \"\"\"\n",
    "Anda adalah asisten AI yang ahli dalam menganalisis dokumen.  \n",
    "Berdasarkan dokumen berikut, identifikasi perubahan nama, restrukturisasi, dan strategi ekspansi PT Pertamina.  \n",
    "Gunakan hanya informasi yang terdapat dalam dokumen.  \n",
    "\n",
    "Dokumen:\n",
    "\"{document}\"\n",
    "Ringkasan:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def summarize_text(chunks):\n",
    "    \"\"\"Melakukan ringkasan untuk setiap chunk dan menggabungkannya.\"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"🔹 Memproses chunk {i}/{len(chunks)}...\")\n",
    "        chain = prompt | llm\n",
    "        response = chain.invoke({\"document\": chunk})\n",
    "        summaries.append(response)\n",
    "    \n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "# Load hasil chunking dari PDF\n",
    "chunker = PDFChunker(chunk_size=1000, chunk_overlap=200, line_width=80)\n",
    "pdf_path = \"tko.pdf\"\n",
    "chunks = chunker.process_pdf(pdf_path)\n",
    "\n",
    "if chunks:\n",
    "    summary = summarize_text(chunks)\n",
    "\n",
    "    # Simpan hasil ringkasan\n",
    "    output_folder = \"overlap_chunking\"\n",
    "    summary_file = os.path.join(output_folder, \"summary.txt\")\n",
    "\n",
    "    with open(summary_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary)\n",
    "\n",
    "    print(f\"✅ Ringkasan berhasil disimpan di '{summary_file}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
