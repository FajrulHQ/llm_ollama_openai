{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECURSIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF untuk membaca file PDF\n",
    "import os  # Modul os untuk operasi sistem file\n",
    "import re  # Modul re untuk ekspresi reguler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fungsi clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Membersihkan dan merapikan teks hasil ekstraksi.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Mengganti spasi berlebih dengan satu spasi\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)  # Menggabungkan kata yang terputus di akhir baris\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Mengganti newline dengan spasi\n",
    "    return text.strip()  # Menghapus spasi di awal dan akhir teks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fungsi load_pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"\n",
    "    Load dan ekstrak teks dari PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)  # Membuka file PDF\n",
    "        full_text = \"\"  # Inisialisasi string untuk menyimpan teks\n",
    "        \n",
    "        for page_num in range(len(doc)):  # Iterasi melalui setiap halaman\n",
    "            page = doc.load_page(page_num)  # Memuat halaman\n",
    "            page_text = page.get_text(\"text\")  # Mengambil teks dari halaman\n",
    "            page_text = clean_text(page_text)  # Membersihkan teks\n",
    "            full_text += page_text + \" \"  # Menambahkan teks halaman ke full_text\n",
    "        \n",
    "        return clean_text(full_text)  # Mengembalikan teks yang telah dibersihkan\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan: {e}\")  # Menangani kesalahan saat membuka file\n",
    "        return \"\"  # Mengembalikan string kosong jika terjadi kesalahan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fungsi recursive_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_chunk(text, chunk_size, separators=None):\n",
    "    \"\"\"\n",
    "    Membagi teks secara rekursif menjadi chunk dengan panjang maksimum menggunakan separator yang ditentukan.\n",
    "    \"\"\"\n",
    "    if not text.strip():  # Jika teks kosong, kembalikan daftar kosong\n",
    "        return []\n",
    "\n",
    "    if separators is None:  # Jika tidak ada separator yang ditentukan, gunakan titik sebagai default\n",
    "        separators = [\".\"]\n",
    "    \n",
    "    for separator in separators:  # Iterasi melalui setiap separator\n",
    "        if separator in text[:chunk_size]:  # Jika separator ditemukan dalam batas chunk_size\n",
    "            split_index = text[:chunk_size].rfind(separator) + len(separator)  # Temukan indeks pemisah terakhir\n",
    "            chunk = text[:split_index]  # Ambil chunk\n",
    "            remaining_text = text[split_index:].strip()  # Ambil sisa teks\n",
    "            return [chunk] + recursive_chunk(remaining_text, chunk_size, separators)  # Kembalikan chunk dan sisa teks yang dipecah\n",
    "\n",
    "    chunk = text[:chunk_size]  # Jika tidak ada separator ditemukan, ambil chunk berdasarkan ukuran\n",
    "    remaining_text = text[chunk_size:].strip()  # Ambil sisa teks\n",
    "    return [chunk] + recursive_chunk(remaining_text, chunk_size, separators)  # Kembalikan chunk dan sisa teks yang dipecah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fungsi save_chunks_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_to_file(chunks, output_file):\n",
    "    \"\"\"\n",
    "    Menyimpan semua chunk ke dalam satu file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "        for idx, chunk in enumerate(chunks, 1):  # Iterasi melalui setiap chunk\n",
    "            f.write(f\"Chunk {idx:03d}:\\n\")  # Menulis header untuk chunk\n",
    "            f.write(chunk)  # Menulis isi chunk\n",
    "            f.write(\"\\n\\n\")  # Menambahkan newline setelah setiap chunk\n",
    "    \n",
    "    print(f\"Semua chunks telah disimpan ke dalam file: {output_file}\")  # Menampilkan pesan bahwa penyimpanan selesai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Fungsi main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Konfigurasi\n",
    "    pdf_path = \"./data/tko.pdf\"  # Jalur ke file PDF yang akan diproses\n",
    "    output_file = \"./output_chunks/output_recursive.txt\"  # File untuk menyimpan hasil chunking\n",
    "    chunk_size = 1000  # Ukuran maksimum setiap chunk\n",
    "\n",
    "    # Load PDF dan ekstrak teks\n",
    "    print(\"Mengekstrak teks dari PDF...\")\n",
    "    full_text = load_pdf_text(pdf_path)  # Memanggil fungsi untuk mengekstrak teks dari PDF\n",
    "\n",
    "    if not full_text:  # Jika tidak ada teks yang diekstrak\n",
    "        print(\"Tidak ada teks yang bisa diekstrak. Program dihentikan.\")  # Menampilkan pesan kesalahan\n",
    "        return\n",
    "\n",
    "    # Chunk teks secara rekursif\n",
    "    print(\"Membagi teks menjadi chunks secara rekursif...\")\n",
    "    chunks = recursive_chunk(full_text, chunk_size) \n",
    "\n",
    "    print(f\"Selesai membagi menjadi {len(chunks)} chunks.\")  # Menampilkan jumlah chunk yang dihasilkan\n",
    "\n",
    "    # Simpan semua chunk ke satu file \n",
    "    save_chunks_to_file(chunks, output_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Eksekusi Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi main jika file ini dieksekusi sebagai skrip utama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Keseluruhan Kode Program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengekstrak teks dari PDF...\n",
      "Membagi teks menjadi chunks secara rekursif...\n",
      "Selesai membagi menjadi 11 chunks.\n",
      "Semua chunks telah disimpan ke dalam file: ./output_chunks/output_recursive.txt\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF untuk membaca file PDF\n",
    "import os  # Modul os untuk operasi sistem file\n",
    "import re  # Modul re untuk ekspresi reguler\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Membersihkan dan merapikan teks hasil ekstraksi.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Mengganti spasi berlebih dengan satu spasi\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)  # Menggabungkan kata yang terputus di akhir baris\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Mengganti newline dengan spasi\n",
    "    return text.strip()  # Menghapus spasi di awal dan akhir teks\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"\n",
    "    Load dan ekstrak teks dari PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)  # Membuka file PDF\n",
    "        full_text = \"\"  # Inisialisasi string untuk menyimpan teks\n",
    "        \n",
    "        for page_num in range(len(doc)):  # Iterasi melalui setiap halaman\n",
    "            page = doc.load_page(page_num)  # Memuat halaman\n",
    "            page_text = page.get_text(\"text\")  # Mengambil teks dari halaman\n",
    "            page_text = clean_text(page_text)  # Membersihkan teks\n",
    "            full_text += page_text + \" \"  # Menambahkan teks halaman ke full_text\n",
    "        \n",
    "        return clean_text(full_text)  # Mengembalikan teks yang telah dibersihkan\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan: {e}\")  # Menangani kesalahan saat membuka file\n",
    "        return \"\"  # Mengembalikan string kosong jika terjadi kesalahan\n",
    "\n",
    "def recursive_chunk(text, chunk_size, separators=None):\n",
    "    \"\"\"\n",
    "    Membagi teks secara rekursif menjadi chunk dengan panjang maksimum menggunakan separator yang ditentukan.\n",
    "    \"\"\"\n",
    "    if not text.strip():  # Jika teks kosong, kembalikan daftar kosong\n",
    "        return []\n",
    "\n",
    "    if separators is None:  # Jika tidak ada separator yang ditentukan, gunakan titik sebagai default\n",
    "        separators = [\".\"]\n",
    "    \n",
    "    for separator in separators:  # Iterasi melalui setiap separator\n",
    "        if separator in text[:chunk_size]:  # Jika separator ditemukan dalam batas chunk_size\n",
    "            split_index = text[:chunk_size].rfind(separator) + len(separator)  # Temukan indeks pemisah terakhir\n",
    "            chunk = text[:split_index]  # Ambil chunk\n",
    "            remaining_text = text[split_index:].strip()  # Ambil sisa teks\n",
    "            return [chunk] + recursive_chunk(remaining_text, chunk_size, separators)  # Kembalikan chunk dan sisa teks yang dipecah\n",
    "\n",
    "    chunk = text[:chunk_size]  # Jika tidak ada separator ditemukan, ambil chunk berdasarkan ukuran\n",
    "    remaining_text = text[chunk_size:].strip()  # Ambil sisa teks\n",
    "    return [chunk] + recursive_chunk(remaining_text, chunk_size, separators)  # Kembalikan chunk dan sisa teks yang dipecah\n",
    "\n",
    "def save_chunks_to_file(chunks, output_file):\n",
    "    \"\"\"\n",
    "    Menyimpan semua chunk ke dalam satu file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "        for idx, chunk in enumerate(chunks, 1):  # Iterasi melalui setiap chunk\n",
    "            f.write(f\"Chunk {idx:03d}:\\n\")  # Menulis header untuk chunk\n",
    "            f.write(chunk)  # Menulis isi chunk\n",
    "            f.write(\"\\n\\n\")  # Menambahkan newline setelah setiap chunk\n",
    "    \n",
    "    print(f\"Semua chunks telah disimpan ke dalam file: {output_file}\")  # Menampilkan pesan bahwa penyimpanan selesai\n",
    "\n",
    "def main():\n",
    "    # Konfigurasi\n",
    "    pdf_path = \"./data/tko.pdf\"  # Jalur ke file PDF yang akan diproses\n",
    "    output_file = \"./output_chunks/output_recursive.txt\"  # File untuk menyimpan hasil chunking\n",
    "    chunk_size = 1000  # Ukuran maksimum setiap chunk\n",
    "\n",
    "    # Load PDF dan ekstrak teks\n",
    "    print(\"Mengekstrak teks dari PDF...\")\n",
    "    full_text = load_pdf_text(pdf_path)  # Memanggil fungsi untuk mengekstrak teks dari PDF\n",
    "\n",
    "    if not full_text:  # Jika tidak ada teks yang diekstrak\n",
    "        print(\"Tidak ada teks yang bisa diekstrak. Program dihentikan.\")  # Menampilkan pesan kesalahan\n",
    "        return\n",
    "\n",
    "    # Chunk teks secara rekursif\n",
    "    print(\"Membagi teks menjadi chunks secara rekursif...\")\n",
    "    chunks = recursive_chunk(full_text, chunk_size) \n",
    "\n",
    "    print(f\"Selesai membagi menjadi {len(chunks)} chunks.\")  # Menampilkan jumlah chunk yang dihasilkan\n",
    "\n",
    "    # Simpan semua chunk ke satu file \n",
    "    save_chunks_to_file(chunks, output_file) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi main jika file ini dieksekusi sebagai skrip utama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERLAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import fitz  # Mengimpor fitz (PyMuPDF) untuk membaca file PDF\n",
    "import nltk  # Mengimpor NLTK untuk pemrosesan bahasa alami\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler\n",
    "import textwrap  # Mengimpor modul textwrap untuk membungkus teks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Mengimpor RecursiveCharacterTextSplitter untuk membagi teks\n",
    "from nltk.tokenize import sent_tokenize  # Mengimpor fungsi untuk tokenisasi kalimat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Kelas PDFChunker\n",
    "\n",
    "    Berisikan fungsi\n",
    "    - init (Konfigurasi paramter dan text_splitter)\n",
    "    - extract_text_from_pdf\n",
    "    - clean_text\n",
    "    - create_improved_chunks\n",
    "    - wrap_text\n",
    "    - save_chunks_to_file\n",
    "    - process_pdf\n",
    "    - analyze_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFChunker:\n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=200, line_width=80):\n",
    "        # Inisialisasi parameter untuk chunking\n",
    "        self.chunk_size = chunk_size  # Ukuran maksimum setiap chunk\n",
    "        self.chunk_overlap = chunk_overlap  # Jumlah karakter yang tumpang tindih antara chunk\n",
    "        self.line_width = line_width  # Lebar baris untuk membungkus teks\n",
    "        # Inisialisasi text splitter dengan parameter yang ditentukan\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from PDF file with better formatting.\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)  # Membuka file PDF\n",
    "            text_blocks = []  # Inisialisasi daftar untuk menyimpan blok teks\n",
    "            \n",
    "            for page in doc:  # Iterasi melalui setiap halaman\n",
    "                blocks = page.get_text(\"blocks\")  # Mengambil blok teks dari halaman\n",
    "                for block in blocks:  # Iterasi melalui setiap blok\n",
    "                    clean_text = block[4].strip()  # Mengambil teks dan menghapus spasi di awal/akhir\n",
    "                    if clean_text:  # Jika teks tidak kosong\n",
    "                        text_blocks.append(clean_text)  # Tambahkan blok teks ke daftar\n",
    "            \n",
    "            text = \" \".join(text_blocks)  # Gabungkan semua blok teks menjadi satu string\n",
    "            return self.clean_text(text)  # Mengembalikan teks yang telah dibersihkan\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")  # Menangani kesalahan saat membaca PDF\n",
    "            return None\n",
    "        finally:\n",
    "            if 'doc' in locals():  # Pastikan dokumen ditutup jika berhasil dibuka\n",
    "                doc.close()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text while preserving structure.\"\"\"\n",
    "        if not text:  # Jika teks kosong\n",
    "            return \"\"\n",
    "        \n",
    "        # Menghapus spasi berlebih dan normalisasi tanda baca\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Mengganti spasi berlebih dengan satu spasi\n",
    "        text = re.sub(r'([.!?])\\s*', r'\\1 ', text)  # Menambahkan spasi setelah tanda baca\n",
    "        text = re.sub(r'\\s+([.!?])', r'\\1', text)  # Menghapus spasi sebelum tanda baca\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Menghapus newline berlebih\n",
    "        \n",
    "        return text.strip()  # Mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "    def create_improved_chunks(self, text):\n",
    "        \"\"\"Create chunks with improved sentence handling.\"\"\"\n",
    "        if not text:  # Jika teks kosong\n",
    "            return []\n",
    "\n",
    "        sentences = sent_tokenize(text)  # Tokenisasi teks menjadi kalimat\n",
    "        chunks = []  # Inisialisasi daftar untuk menyimpan chunk\n",
    "        current_chunk = []  # Inisialisasi daftar untuk menyimpan kalimat dalam chunk\n",
    "        current_length = 0  # Inisialisasi panjang chunk saat ini\n",
    "\n",
    "        for sentence in sentences:  # Iterasi melalui setiap kalimat\n",
    "            sentence = sentence.strip()  # Menghapus spasi di awal/akhir kalimat\n",
    "            sentence_length = len(sentence)  # Menghitung panjang kalimat\n",
    "\n",
    "            if not current_chunk:  # Jika chunk saat ini kosong\n",
    "                current_chunk.append(sentence)  # Tambahkan kalimat ke chunk\n",
    "                current_length = sentence_length  # Perbarui panjang chunk\n",
    "                continue\n",
    "\n",
    "            # Jika panjang chunk ditambah kalimat tidak melebihi batas\n",
    "            if current_length + len(\" \") + sentence_length <= self.chunk_size:\n",
    "                current_chunk.append(sentence)  # Tambahkan kalimat ke chunk\n",
    "                current_length += len(\" \") + sentence_length  # Perbarui panjang chunk\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))  # Tambahkan chunk ke daftar\n",
    "                current_chunk = [sentence]  # Mulai chunk baru dengan kalimat saat ini\n",
    "                current_length = sentence_length  # Perbarui panjang chunk\n",
    "\n",
    "        if current_chunk:  # Jika ada chunk yang tersisa\n",
    "            chunks.append(\" \".join(current_chunk))  # Tambahkan chunk terakhir ke daftar\n",
    "\n",
    "        final_chunks = []  # Inisialisasi daftar untuk menyimpan chunk akhir\n",
    "        for i in range(len(chunks)):  # Iterasi melalui setiap chunk\n",
    "            if i > 0:  # Jika bukan chunk pertama\n",
    "                prev_chunk_sentences = sent_tokenize(chunks[i-1])  # Tokenisasi chunk sebelumnya\n",
    "                overlap_sentences = prev_chunk_sentences[-2:] if len(prev_chunk_sentences) > 2 else prev_chunk_sentences  # Ambil dua kalimat terakhir untuk tumpang tindih\n",
    "                current_chunk = \" \".join(overlap_sentences) + \" \" + chunks[i]  # Gabungkan kalimat tumpang tindih dengan chunk saat ini\n",
    "                final_chunks.append(current_chunk)  # Tambahkan chunk yang telah digabungkan ke daftar\n",
    "            else:\n",
    "                final_chunks.append(chunks[i])  # Tambahkan chunk pertama ke daftar\n",
    "\n",
    "        return final_chunks  # Mengembalikan daftar chunk akhir\n",
    "\n",
    "    def wrap_text(self, text):\n",
    "        \"\"\"Wrap text to specified line width while preserving paragraphs.\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')  # Memisahkan teks menjadi paragraf\n",
    "        \n",
    "        wrapped_paragraphs = []  # Inisialisasi daftar untuk menyimpan paragraf yang dibungkus\n",
    "        for paragraph in paragraphs:  # Iterasi melalui setiap paragraf\n",
    "            wrapped = textwrap.fill(paragraph.strip(), width=self.line_width)  # Membungkus paragraf dengan lebar yang ditentukan\n",
    "            wrapped_paragraphs.append(wrapped)  # Tambahkan paragraf yang dibungkus ke daftar\n",
    "        \n",
    "        return '\\n\\n'.join(wrapped_paragraphs)  # Menggabungkan paragraf yang dibungkus dengan newline ganda\n",
    "\n",
    "    def save_chunks_to_file(self, chunks, output_folder=\"chunks_output\"):\n",
    "        \"\"\"Save chunks to a single file with proper text wrapping.\"\"\"\n",
    "        if not os.path.exists(output_folder):  # Jika folder output tidak ada\n",
    "            os.makedirs(output_folder)  # Buat folder output\n",
    "\n",
    "        output_file = os.path.join(output_folder, \"output_overlap.txt\")  # Tentukan nama file output\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:  # Membuka file untuk menulis\n",
    "                for i, chunk in enumerate(chunks, 1):  # Iterasi melalui setiap chunk\n",
    "                    header = f\"Chunk {i}\"  # Membuat header untuk chunk\n",
    "                    file.write(f\"{header}\\n\")  # Menulis header ke file\n",
    "                    file.write(\"=\"* len(header) + \"\\n\\n\")  # Menulis garis pemisah\n",
    "                    \n",
    "                    wrapped_content = self.wrap_text(chunk)  # Membungkus konten chunk\n",
    "                    file.write(wrapped_content)  # Menulis konten chunk yang dibungkus ke file\n",
    "                    file.write(\"\\n\\n\\n\")  # Menambahkan spasi ekstra antara chunk\n",
    "\n",
    "            print(f\"✅ {len(chunks)} chunks successfully saved to '{output_file}'\")  # Menampilkan pesan sukses\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving chunks: {e}\")  # Menangani kesalahan saat menyimpan chunk\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        \"\"\"Process PDF and create better chunks.\"\"\"\n",
    "        text = self.extract_text_from_pdf(pdf_path)  # Mengambil teks dari PDF\n",
    "        if not text:  # Jika tidak ada teks\n",
    "            return []\n",
    "        \n",
    "        return self.create_improved_chunks(text)  # Mengembalikan chunk yang telah dibuat\n",
    "\n",
    "    def analyze_chunks(self, chunks):\n",
    "        \"\"\"Analyze chunks with improved metrics.\"\"\"\n",
    "        if not chunks:  # Jika tidak ada chunk\n",
    "            return {\n",
    "                \"total_chunks\": 0,\n",
    "                \"average_size\": 0,\n",
    "                \"size_stats\": {},\n",
    "                \"sentence_stats\": {}\n",
    "            }\n",
    "\n",
    "        chunk_sizes = [len(chunk) for chunk in chunks]  # Menghitung ukuran setiap chunk\n",
    "        sentences_per_chunk = [len(sent_tokenize(chunk)) for chunk in chunks]  # Menghitung jumlah kalimat per chunk\n",
    "\n",
    "        analysis = {\n",
    "            \"total_chunks\": len(chunks),  # Total chunk\n",
    "            \"average_size\": sum(chunk_sizes) / len(chunks),  # Ukuran rata-rata chunk\n",
    "            \"size_stats\": {\n",
    "                \"min\": min(chunk_sizes),  # Ukuran chunk terkecil\n",
    "                \"max\": max(chunk_sizes),  # Ukuran chunk terbesar\n",
    "                \"avg\": sum(chunk_sizes) / len(chunks)  # Ukuran rata-rata chunk\n",
    "            },\n",
    "            \"sentence_stats\": {\n",
    "                \"min_sentences\": min(sentences_per_chunk),  # Jumlah kalimat minimum per chunk\n",
    "                \"max_sentences\": max(sentences_per_chunk),  # Jumlah kalimat maksimum per chunk\n",
    "                \"avg_sentences\": sum(sentences_per_chunk) / len(sentences_per_chunk)  # Jumlah kalimat rata-rata per chunk\n",
    "            }\n",
    "        }\n",
    "        return analysis  # Mengembalikan hasil analisis chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fungsi main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Inisialisasi chunker dengan parameter untuk membungkus teks\n",
    "    chunker = PDFChunker(chunk_size=1000, chunk_overlap=200, line_width=80)\n",
    "    \n",
    "    # Konfigurasi\n",
    "    pdf_path = \"./data/tko.pdf\"  # Jalur ke file PDF yang akan diproses\n",
    "    output_folder = \"./output_chunks/\"  # Folder untuk menyimpan hasil chunking\n",
    "    \n",
    "    # Proses PDF\n",
    "    chunks = chunker.process_pdf(pdf_path)  # Memanggil fungsi untuk memproses PDF\n",
    "    \n",
    "    if chunks:  # Jika ada chunk yang dihasilkan\n",
    "        # Simpan hasil\n",
    "        chunker.save_chunks_to_file(chunks, output_folder)  # Memanggil fungsi untuk menyimpan chunk ke file\n",
    "        \n",
    "        # Analisis dan tampilkan hasil\n",
    "        analysis = chunker.analyze_chunks(chunks)  # Memanggil fungsi untuk menganalisis chunk\n",
    "        print(\"\\nChunking Analysis:\")  # Menampilkan analisis chunk\n",
    "        print(f\"Total chunks: {analysis['total_chunks']}\")  # Menampilkan total chunk\n",
    "        print(f\"Average chunk size: {analysis['size_stats']['avg']:.0f} characters\")  # Menampilkan ukuran rata-rata chunk\n",
    "        print(f\"Min/Max chunk size: {analysis['size_stats']['min']}/{analysis['size_stats']['max']} characters\")  # Menampilkan ukuran chunk minimum dan maksimum\n",
    "        print(f\"\\nSentences per chunk:\")  # Menampilkan jumlah kalimat per chunk\n",
    "        print(f\"Min: {analysis['sentence_stats']['min_sentences']:.0f}\")  # Menampilkan jumlah kalimat minimum\n",
    "        print(f\"Max: {analysis['sentence_stats']['max_sentences']:.0f}\")  # Menampilkan jumlah kalimat maksimum\n",
    "        print(f\"Average: {analysis['sentence_stats']['avg_sentences']:.1f}\")  # Menampilkan jumlah kalimat rata-rata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Eksekusi program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi main jika file ini dieksekusi sebagai skrip utama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Keseluruhan kode program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 10 chunks successfully saved to './output_chunks/output_overlap.txt'\n",
      "\n",
      "Chunking Analysis:\n",
      "Total chunks: 10\n",
      "Average chunk size: 1087 characters\n",
      "Min/Max chunk size: 754/1444 characters\n",
      "\n",
      "Sentences per chunk:\n",
      "Min: 7\n",
      "Max: 25\n",
      "Average: 12.2\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import fitz  # Mengimpor fitz (PyMuPDF) untuk membaca file PDF\n",
    "import nltk  # Mengimpor NLTK untuk pemrosesan bahasa alami\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler\n",
    "import textwrap  # Mengimpor modul textwrap untuk membungkus teks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Mengimpor RecursiveCharacterTextSplitter untuk membagi teks\n",
    "from nltk.tokenize import sent_tokenize  # Mengimpor fungsi untuk tokenisasi kalimat\n",
    "\n",
    "class PDFChunker:\n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=200, line_width=80):\n",
    "        # Inisialisasi parameter untuk chunking\n",
    "        self.chunk_size = chunk_size  # Ukuran maksimum setiap chunk\n",
    "        self.chunk_overlap = chunk_overlap  # Jumlah karakter yang tumpang tindih antara chunk\n",
    "        self.line_width = line_width  # Lebar baris untuk membungkus teks\n",
    "        # Inisialisasi text splitter dengan parameter yang ditentukan\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from PDF file with better formatting.\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)  # Membuka file PDF\n",
    "            text_blocks = []  # Inisialisasi daftar untuk menyimpan blok teks\n",
    "            \n",
    "            for page in doc:  # Iterasi melalui setiap halaman\n",
    "                blocks = page.get_text(\"blocks\")  # Mengambil blok teks dari halaman\n",
    "                for block in blocks:  # Iterasi melalui setiap blok\n",
    "                    clean_text = block[4].strip()  # Mengambil teks dan menghapus spasi di awal/akhir\n",
    "                    if clean_text:  # Jika teks tidak kosong\n",
    "                        text_blocks.append(clean_text)  # Tambahkan blok teks ke daftar\n",
    "            \n",
    "            text = \" \".join(text_blocks)  # Gabungkan semua blok teks menjadi satu string\n",
    "            return self.clean_text(text)  # Mengembalikan teks yang telah dibersihkan\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")  # Menangani kesalahan saat membaca PDF\n",
    "            return None\n",
    "        finally:\n",
    "            if 'doc' in locals():  # Pastikan dokumen ditutup jika berhasil dibuka\n",
    "                doc.close()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text while preserving structure.\"\"\"\n",
    "        if not text:  # Jika teks kosong\n",
    "            return \"\"\n",
    "        \n",
    "        # Menghapus spasi berlebih dan normalisasi tanda baca\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Mengganti spasi berlebih dengan satu spasi\n",
    "        text = re.sub(r'([.!?])\\s*', r'\\1 ', text)  # Menambahkan spasi setelah tanda baca\n",
    "        text = re.sub(r'\\s+([.!?])', r'\\1', text)  # Menghapus spasi sebelum tanda baca\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Menghapus newline berlebih\n",
    "        \n",
    "        return text.strip()  # Mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "    def create_improved_chunks(self, text):\n",
    "        \"\"\"Create chunks with improved sentence handling.\"\"\"\n",
    "        if not text:  # Jika teks kosong\n",
    "            return []\n",
    "\n",
    "        sentences = sent_tokenize(text)  # Tokenisasi teks menjadi kalimat\n",
    "        chunks = []  # Inisialisasi daftar untuk menyimpan chunk\n",
    "        current_chunk = []  # Inisialisasi daftar untuk menyimpan kalimat dalam chunk\n",
    "        current_length = 0  # Inisialisasi panjang chunk saat ini\n",
    "\n",
    "        for sentence in sentences:  # Iterasi melalui setiap kalimat\n",
    "            sentence = sentence.strip()  # Menghapus spasi di awal/akhir kalimat\n",
    "            sentence_length = len(sentence)  # Menghitung panjang kalimat\n",
    "\n",
    "            if not current_chunk:  # Jika chunk saat ini kosong\n",
    "                current_chunk.append(sentence)  # Tambahkan kalimat ke chunk\n",
    "                current_length = sentence_length  # Perbarui panjang chunk\n",
    "                continue\n",
    "\n",
    "            # Jika panjang chunk ditambah kalimat tidak melebihi batas\n",
    "            if current_length + len(\" \") + sentence_length <= self.chunk_size:\n",
    "                current_chunk.append(sentence)  # Tambahkan kalimat ke chunk\n",
    "                current_length += len(\" \") + sentence_length  # Perbarui panjang chunk\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))  # Tambahkan chunk ke daftar\n",
    "                current_chunk = [sentence]  # Mulai chunk baru dengan kalimat saat ini\n",
    "                current_length = sentence_length  # Perbarui panjang chunk\n",
    "\n",
    "        if current_chunk:  # Jika ada chunk yang tersisa\n",
    "            chunks.append(\" \".join(current_chunk))  # Tambahkan chunk terakhir ke daftar\n",
    "\n",
    "        final_chunks = []  # Inisialisasi daftar untuk menyimpan chunk akhir\n",
    "        for i in range(len(chunks)):  # Iterasi melalui setiap chunk\n",
    "            if i > 0:  # Jika bukan chunk pertama\n",
    "                prev_chunk_sentences = sent_tokenize(chunks[i-1])  # Tokenisasi chunk sebelumnya\n",
    "                overlap_sentences = prev_chunk_sentences[-2:] if len(prev_chunk_sentences) > 2 else prev_chunk_sentences  # Ambil dua kalimat terakhir untuk tumpang tindih\n",
    "                current_chunk = \" \".join(overlap_sentences) + \" \" + chunks[i]  # Gabungkan kalimat tumpang tindih dengan chunk saat ini\n",
    "                final_chunks.append(current_chunk)  # Tambahkan chunk yang telah digabungkan ke daftar\n",
    "            else:\n",
    "                final_chunks.append(chunks[i])  # Tambahkan chunk pertama ke daftar\n",
    "\n",
    "        return final_chunks  # Mengembalikan daftar chunk akhir\n",
    "\n",
    "    def wrap_text(self, text):\n",
    "        \"\"\"Wrap text to specified line width while preserving paragraphs.\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')  # Memisahkan teks menjadi paragraf\n",
    "        \n",
    "        wrapped_paragraphs = []  # Inisialisasi daftar untuk menyimpan paragraf yang dibungkus\n",
    "        for paragraph in paragraphs:  # Iterasi melalui setiap paragraf\n",
    "            wrapped = textwrap.fill(paragraph.strip(), width=self.line_width)  # Membungkus paragraf dengan lebar yang ditentukan\n",
    "            wrapped_paragraphs.append(wrapped)  # Tambahkan paragraf yang dibungkus ke daftar\n",
    "        \n",
    "        return '\\n\\n'.join(wrapped_paragraphs)  # Menggabungkan paragraf yang dibungkus dengan newline ganda\n",
    "\n",
    "    def save_chunks_to_file(self, chunks, output_folder=\"chunks_output\"):\n",
    "        \"\"\"Save chunks to a single file with proper text wrapping.\"\"\"\n",
    "        if not os.path.exists(output_folder):  # Jika folder output tidak ada\n",
    "            os.makedirs(output_folder)  # Buat folder output\n",
    "\n",
    "        output_file = os.path.join(output_folder, \"output_overlap.txt\")  # Tentukan nama file output\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:  # Membuka file untuk menulis\n",
    "                for i, chunk in enumerate(chunks, 1):  # Iterasi melalui setiap chunk\n",
    "                    header = f\"Chunk {i}\"  # Membuat header untuk chunk\n",
    "                    file.write(f\"{header}\\n\")  # Menulis header ke file\n",
    "                    file.write(\"=\"* len(header) + \"\\n\\n\")  # Menulis garis pemisah\n",
    "                    \n",
    "                    wrapped_content = self.wrap_text(chunk)  # Membungkus konten chunk\n",
    "                    file.write(wrapped_content)  # Menulis konten chunk yang dibungkus ke file\n",
    "                    file.write(\"\\n\\n\\n\")  # Menambahkan spasi ekstra antara chunk\n",
    "\n",
    "            print(f\"✅ {len(chunks)} chunks successfully saved to '{output_file}'\")  # Menampilkan pesan sukses\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving chunks: {e}\")  # Menangani kesalahan saat menyimpan chunk\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        \"\"\"Process PDF and create better chunks.\"\"\"\n",
    "        text = self.extract_text_from_pdf(pdf_path)  # Mengambil teks dari PDF\n",
    "        if not text:  # Jika tidak ada teks\n",
    "            return []\n",
    "        \n",
    "        return self.create_improved_chunks(text)  # Mengembalikan chunk yang telah dibuat\n",
    "\n",
    "    def analyze_chunks(self, chunks):\n",
    "        \"\"\"Analyze chunks with improved metrics.\"\"\"\n",
    "        if not chunks:  # Jika tidak ada chunk\n",
    "            return {\n",
    "                \"total_chunks\": 0,\n",
    "                \"average_size\": 0,\n",
    "                \"size_stats\": {},\n",
    "                \"sentence_stats\": {}\n",
    "            }\n",
    "\n",
    "        chunk_sizes = [len(chunk) for chunk in chunks]  # Menghitung ukuran setiap chunk\n",
    "        sentences_per_chunk = [len(sent_tokenize(chunk)) for chunk in chunks]  # Menghitung jumlah kalimat per chunk\n",
    "\n",
    "        analysis = {\n",
    "            \"total_chunks\": len(chunks),  # Total chunk\n",
    "            \"average_size\": sum(chunk_sizes) / len(chunks),  # Ukuran rata-rata chunk\n",
    "            \"size_stats\": {\n",
    "                \"min\": min(chunk_sizes),  # Ukuran chunk terkecil\n",
    "                \"max\": max(chunk_sizes),  # Ukuran chunk terbesar\n",
    "                \"avg\": sum(chunk_sizes) / len(chunks)  # Ukuran rata-rata chunk\n",
    "            },\n",
    "            \"sentence_stats\": {\n",
    "                \"min_sentences\": min(sentences_per_chunk),  # Jumlah kalimat minimum per chunk\n",
    "                \"max_sentences\": max(sentences_per_chunk),  # Jumlah kalimat maksimum per chunk\n",
    "                \"avg_sentences\": sum(sentences_per_chunk) / len(sentences_per_chunk)  # Jumlah kalimat rata-rata per chunk\n",
    "            }\n",
    "        }\n",
    "        return analysis  # Mengembalikan hasil analisis chunk\n",
    "\n",
    "def main():\n",
    "    # Inisialisasi chunker dengan parameter untuk membungkus teks\n",
    "    chunker = PDFChunker(chunk_size=1000, chunk_overlap=200, line_width=80)\n",
    "    \n",
    "    # Konfigurasi\n",
    "    pdf_path = \"./data/tko.pdf\"  # Jalur ke file PDF yang akan diproses\n",
    "    output_folder = \"./output_chunks/\"  # Folder untuk menyimpan hasil chunking\n",
    "    \n",
    "    # Proses PDF\n",
    "    chunks = chunker.process_pdf(pdf_path)  # Memanggil fungsi untuk memproses PDF\n",
    "    \n",
    "    if chunks:  # Jika ada chunk yang dihasilkan\n",
    "        # Simpan hasil\n",
    "        chunker.save_chunks_to_file(chunks, output_folder)  # Memanggil fungsi untuk menyimpan chunk ke file\n",
    "        \n",
    "        # Analisis dan tampilkan hasil\n",
    "        analysis = chunker.analyze_chunks(chunks)  # Memanggil fungsi untuk menganalisis chunk\n",
    "        print(\"\\nChunking Analysis:\")  # Menampilkan analisis chunk\n",
    "        print(f\"Total chunks: {analysis['total_chunks']}\")  # Menampilkan total chunk\n",
    "        print(f\"Average chunk size: {analysis['size_stats']['avg']:.0f} characters\")  # Menampilkan ukuran rata-rata chunk\n",
    "        print(f\"Min/Max chunk size: {analysis['size_stats']['min']}/{analysis['size_stats']['max']} characters\")  # Menampilkan ukuran chunk minimum dan maksimum\n",
    "        print(f\"\\nSentences per chunk:\")  # Menampilkan jumlah kalimat per chunk\n",
    "        print(f\"Min: {analysis['sentence_stats']['min_sentences']:.0f}\")  # Menampilkan jumlah kalimat minimum\n",
    "        print(f\"Max: {analysis['sentence_stats']['max_sentences']:.0f}\")  # Menampilkan jumlah kalimat maksimum\n",
    "        print(f\"Average: {analysis['sentence_stats']['avg_sentences']:.1f}\")  # Menampilkan jumlah kalimat rata-rata\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi main jika file ini dieksekusi sebagai skrip utama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKEN BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import tiktoken  # Mengimpor tiktoken untuk pengkodean token\n",
    "import fitz  # Mengimpor fitz (PyMuPDF) untuk membaca file PDF\n",
    "from datetime import datetime  # Mengimpor datetime untuk menangani tanggal dan waktu\n",
    "from typing import List, Tuple  # Mengimpor tipe data List dan Tuple untuk anotasi tipe\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler\n",
    "import logging  # Mengimpor modul logging untuk mencatat informasi\n",
    "import gc  # Mengimpor modul gc untuk pengelolaan memori\n",
    "import nltk  # Mengimpor NLTK untuk pemrosesan bahasa alami\n",
    "nltk.download('punkt')  # Mengunduh model tokenisasi kalimat dari NLTK\n",
    "from nltk.tokenize import sent_tokenize  # Mengimpor fungsi untuk tokenisasi kalimat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename='pdf_processing.log',  # Menentukan nama file log\n",
    "    level=logging.INFO,  # Menentukan level logging\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Format pesan log\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Kelas SentenceChunkProcessor\n",
    "\n",
    "    Berisikan fungsi\n",
    "    - init (Konfigurasi dan inisialisasi parameter)\n",
    "    - read_pdf_and_clean\n",
    "    - restore_numbering\n",
    "    - chunk_text_by_sentences\n",
    "    - save_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceChunkProcessor:\n",
    "    def __init__(self, chunk_size=500):\n",
    "        # Inisialisasi parameter untuk chunking\n",
    "        self.chunk_size = chunk_size  # Ukuran maksimum setiap chunk\n",
    "        self.encoder = tiktoken.get_encoding(\"cl100k_base\")  # Menginisialisasi encoder token\n",
    "\n",
    "    def read_pdf_and_clean(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file with better formatting.\"\"\"\n",
    "        logging.info(f\"Started processing PDF: {file_path}\")  # Mencatat bahwa pemrosesan PDF dimulai\n",
    "        doc = fitz.open(file_path)  # Membuka file PDF\n",
    "        text = \" \".join([page.get_text(\"text\") for page in doc])  # Mengambil teks dari setiap halaman\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Menghapus spasi berlebih dan menghapus spasi di awal/akhir\n",
    "\n",
    "        # Menghindari pemisahan angka yang salah (4. 3.)\n",
    "        text = re.sub(r'(\\d+)\\.\\s(?=\\d+)', r'\\1|', text)  # Mengganti pemisahan angka dengan karakter '|'\n",
    "\n",
    "        logging.info(f\"Successfully read and cleaned PDF: {file_path}\")  # Mencatat bahwa PDF berhasil dibaca dan dibersihkan\n",
    "        return text  # Mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "    def restore_numbering(self, text: str) -> str:\n",
    "        \"\"\"Mengembalikan angka yang sebelumnya diganti (4|3 -> 4. 3.)\"\"\"\n",
    "        return text.replace('|', '. ')  # Mengganti karakter '|' kembali menjadi '. '\n",
    "\n",
    "    def chunk_text_by_sentences(self, text: str) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Create chunks with improved sentence handling.\"\"\"\n",
    "        text = self.restore_numbering(text)  # Kembalikan format angka\n",
    "        sentences = sent_tokenize(text)  # Tokenisasi teks menjadi kalimat\n",
    "        chunks = []  # Inisialisasi daftar untuk menyimpan chunk\n",
    "        current_tokens = []  # Inisialisasi daftar untuk menyimpan token saat ini\n",
    "        current_text = \"\"  # Inisialisasi string untuk menyimpan teks saat ini\n",
    "\n",
    "        for sentence in sentences:  # Iterasi melalui setiap kalimat\n",
    "            sentence = sentence.strip()  # Menghapus spasi di awal/akhir kalimat\n",
    "            sentence_tokens = self.encoder.encode(sentence)  # Mengkodekan kalimat menjadi token\n",
    "            \n",
    "            # Jika penambahan kalimat melebihi ukuran chunk\n",
    "            if len(current_tokens) + len(sentence_tokens) > self.chunk_size:\n",
    "                if current_tokens:  # Jika ada token saat ini\n",
    "                    chunks.append((self.encoder.decode(current_tokens), len(current_tokens)))  # Tambahkan chunk ke daftar\n",
    "                    current_tokens = []  # Reset token saat ini tanpa overlap\n",
    "                    current_text = \"\"  # Reset teks saat ini\n",
    "            \n",
    "            current_tokens.extend(sentence_tokens)  # Tambahkan token kalimat ke token saat ini\n",
    "            current_text += sentence  # Tambahkan kalimat ke teks saat ini\n",
    "            \n",
    "            gc.collect()  # Mengumpulkan sampah untuk mengelola memori\n",
    "        \n",
    "        if current_tokens:  # Jika ada token yang tersisa\n",
    "            chunks.append((self.encoder.decode(current_tokens), len(current_tokens)))  # Tambahkan chunk terakhir ke daftar\n",
    "\n",
    "        logging.info(f\"Processed and chunked text into {len(chunks)} chunks\")  # Mencatat jumlah chunk yang dihasilkan\n",
    "        return chunks  # Mengembalikan daftar chunk\n",
    "\n",
    "    def save_chunks(self, content_chunks: List[Tuple[str, int]], output_path: str) -> None:\n",
    "        \"\"\"Save chunks to a single file with proper text wrapping.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Mendapatkan timestamp untuk penamaan file\n",
    "        output_path = f\"{os.path.splitext(output_path)[0]}_{timestamp}.txt\"  # Menambahkan timestamp ke nama file output\n",
    "\n",
    "        try:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "                f.write(f\"Processing Date: {datetime.now()}\\n\")  # Menulis tanggal pemrosesan\n",
    "                f.write(f\"Chunk Size: {self.chunk_size} tokens\\n\")  # Menulis ukuran chunk\n",
    "                f.write(\"=\" * 50 + \"\\n\\n\")  # Menulis garis pemisah\n",
    "\n",
    "                for i, (chunk, token_count) in enumerate(content_chunks, 1):  # Iterasi melalui setiap chunk\n",
    "                    f.write(f\"=== Chunk {i} ===\\n\")  # Menulis header untuk chunk\n",
    "                    f.write(f\"Tokens: {token_count}\\n\")  # Menulis jumlah token dalam chunk\n",
    "                    f.write(chunk + \"\\n\\n\")  # Menulis konten chunk\n",
    "                    f.write(\"-\" * 30 + \"\\n\\n\")  # Menulis garis pemisah antar chunk\n",
    "\n",
    "            logging.info(f\"Successfully saved chunks to: {output_path}\")  # Mencatat bahwa chunk berhasil disimpan\n",
    "            logging.info(f\"Total chunks: {len(content_chunks)}\")  # Mencatat total chunk yang disimpan\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving chunks: {str(e)}\")  # Mencatat kesalahan saat menyimpan chunk\n",
    "            raise  # Mengangkat kembali kesalahan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Eksekusi Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"./data/tko.pdf\"  # Menentukan jalur ke file PDF yang akan diproses\n",
    "    output_path = \"./output_chunks/output_token_base.txt\"  # Menentukan jalur untuk menyimpan hasil chunking\n",
    "\n",
    "    processor = SentenceChunkProcessor(chunk_size=500)  # Membuat instance dari SentenceChunkProcessor dengan ukuran chunk 500\n",
    "\n",
    "    if not os.path.exists(file_path):  # Memeriksa apakah file PDF ada\n",
    "        print(f\"Error: File '{file_path}' tidak ditemukan!\")  # Menampilkan pesan kesalahan jika file tidak ditemukan\n",
    "    else:\n",
    "        try:\n",
    "            print(\"🔍 Memulai proses membaca PDF...\")  # Menampilkan pesan bahwa proses membaca PDF dimulai\n",
    "            text = processor.read_pdf_and_clean(file_path)  # Membaca dan membersihkan teks dari PDF\n",
    "            print(f\"Teks PDF terbaca (50 karakter pertama): {text[:50]}...\")  # Menampilkan 50 karakter pertama dari teks yang terbaca\n",
    "\n",
    "            print(\"🔍 Memulai proses chunking...\")  # Menampilkan pesan bahwa proses chunking dimulai\n",
    "            content_chunks = processor.chunk_text_by_sentences(text)  # Membagi teks menjadi chunk berdasarkan kalimat\n",
    "            print(f\"Total chunk yang dibuat: {len(content_chunks)}\")  # Menampilkan total chunk yang dibuat\n",
    "\n",
    "            print(\"🔍 Menyimpan hasil chunking ke output file...\")  # Menampilkan pesan bahwa hasil chunking akan disimpan\n",
    "            processor.save_chunks(content_chunks, output_path)  # Menyimpan chunk ke file output\n",
    "            print(f\"✅ File output tersimpan di: {output_path}\")  # Menampilkan pesan sukses\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Terjadi error: {str(e)}\")  # Menampilkan pesan kesalahan jika terjadi error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Keseluruhan Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Agus\n",
      "[nltk_data]     Syuhada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Memulai proses membaca PDF...\n",
      "Teks PDF terbaca (50 karakter pertama): TATA KERJA ORGANISASI PENGELOLAAN AKUN APLIKASI No...\n",
      "🔍 Memulai proses chunking...\n",
      "Total chunk yang dibuat: 6\n",
      "🔍 Menyimpan hasil chunking ke output file...\n",
      "✅ File output tersimpan di: ./output_chunks/output_token_base.txt\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import tiktoken  # Mengimpor tiktoken untuk pengkodean token\n",
    "import fitz  # Mengimpor fitz (PyMuPDF) untuk membaca file PDF\n",
    "from datetime import datetime  # Mengimpor datetime untuk menangani tanggal dan waktu\n",
    "from typing import List, Tuple  # Mengimpor tipe data List dan Tuple untuk anotasi tipe\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler\n",
    "import logging  # Mengimpor modul logging untuk mencatat informasi\n",
    "import gc  # Mengimpor modul gc untuk pengelolaan memori\n",
    "import nltk  # Mengimpor NLTK untuk pemrosesan bahasa alami\n",
    "nltk.download('punkt')  # Mengunduh model tokenisasi kalimat dari NLTK\n",
    "from nltk.tokenize import sent_tokenize  # Mengimpor fungsi untuk tokenisasi kalimat\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename='pdf_processing.log',  # Menentukan nama file log\n",
    "    level=logging.INFO,  # Menentukan level logging\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Format pesan log\n",
    ")\n",
    "\n",
    "class SentenceChunkProcessor:\n",
    "    def __init__(self, chunk_size=500):\n",
    "        # Inisialisasi parameter untuk chunking\n",
    "        self.chunk_size = chunk_size  # Ukuran maksimum setiap chunk\n",
    "        self.encoder = tiktoken.get_encoding(\"cl100k_base\")  # Menginisialisasi encoder token\n",
    "\n",
    "    def read_pdf_and_clean(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file with better formatting.\"\"\"\n",
    "        logging.info(f\"Started processing PDF: {file_path}\")  # Mencatat bahwa pemrosesan PDF dimulai\n",
    "        doc = fitz.open(file_path)  # Membuka file PDF\n",
    "        text = \" \".join([page.get_text(\"text\") for page in doc])  # Mengambil teks dari setiap halaman\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Menghapus spasi berlebih dan menghapus spasi di awal/akhir\n",
    "\n",
    "        # Menghindari pemisahan angka yang salah (4. 3.)\n",
    "        text = re.sub(r'(\\d+)\\.\\s(?=\\d+)', r'\\1|', text)  # Mengganti pemisahan angka dengan karakter '|'\n",
    "\n",
    "        logging.info(f\"Successfully read and cleaned PDF: {file_path}\")  # Mencatat bahwa PDF berhasil dibaca dan dibersihkan\n",
    "        return text  # Mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "    def restore_numbering(self, text: str) -> str:\n",
    "        \"\"\"Mengembalikan angka yang sebelumnya diganti (4|3 -> 4. 3.)\"\"\"\n",
    "        return text.replace('|', '. ')  # Mengganti karakter '|' kembali menjadi '. '\n",
    "\n",
    "    def chunk_text_by_sentences(self, text: str) -> List[Tuple[str, int]]:\n",
    "        \"\"\"Create chunks with improved sentence handling.\"\"\"\n",
    "        text = self.restore_numbering(text)  # Kembalikan format angka\n",
    "        sentences = sent_tokenize(text)  # Tokenisasi teks menjadi kalimat\n",
    "        chunks = []  # Inisialisasi daftar untuk menyimpan chunk\n",
    "        current_tokens = []  # Inisialisasi daftar untuk menyimpan token saat ini\n",
    "        current_text = \"\"  # Inisialisasi string untuk menyimpan teks saat ini\n",
    "\n",
    "        for sentence in sentences:  # Iterasi melalui setiap kalimat\n",
    "            sentence = sentence.strip()  # Menghapus spasi di awal/akhir kalimat\n",
    "            sentence_tokens = self.encoder.encode(sentence)  # Mengkodekan kalimat menjadi token\n",
    "            \n",
    "            # Jika penambahan kalimat melebihi ukuran chunk\n",
    "            if len(current_tokens) + len(sentence_tokens) > self.chunk_size:\n",
    "                if current_tokens:  # Jika ada token saat ini\n",
    "                    chunks.append((self.encoder.decode(current_tokens), len(current_tokens)))  # Tambahkan chunk ke daftar\n",
    "                    current_tokens = []  # Reset token saat ini tanpa overlap\n",
    "                    current_text = \"\"  # Reset teks saat ini\n",
    "            \n",
    "            current_tokens.extend(sentence_tokens)  # Tambahkan token kalimat ke token saat ini\n",
    "            current_text += sentence  # Tambahkan kalimat ke teks saat ini\n",
    "            \n",
    "            gc.collect()  # Mengumpulkan sampah untuk mengelola memori\n",
    "        \n",
    "        if current_tokens:  # Jika ada token yang tersisa\n",
    "            chunks.append((self.encoder.decode(current_tokens), len(current_tokens)))  # Tambahkan chunk terakhir ke daftar\n",
    "\n",
    "        logging.info(f\"Processed and chunked text into {len(chunks)} chunks\")  # Mencatat jumlah chunk yang dihasilkan\n",
    "        return chunks  # Mengembalikan daftar chunk\n",
    "\n",
    "    def save_chunks(self, content_chunks: List[Tuple[str, int]], output_path: str) -> None:\n",
    "        \"\"\"Save chunks to a single file with proper text wrapping.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # Mendapatkan timestamp untuk penamaan file\n",
    "        output_path = f\"{os.path.splitext(output_path)[0]}_{timestamp}.txt\"  # Menambahkan timestamp ke nama file output\n",
    "\n",
    "        try:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "                f.write(f\"Processing Date: {datetime.now()}\\n\")  # Menulis tanggal pemrosesan\n",
    "                f.write(f\"Chunk Size: {self.chunk_size} tokens\\n\")  # Menulis ukuran chunk\n",
    "                f.write(\"=\" * 50 + \"\\n\\n\")  # Menulis garis pemisah\n",
    "\n",
    "                for i, (chunk, token_count) in enumerate(content_chunks, 1):  # Iterasi melalui setiap chunk\n",
    "                    f.write(f\"=== Chunk {i} ===\\n\")  # Menulis header untuk chunk\n",
    "                    f.write(f\"Tokens: {token_count}\\n\")  # Menulis jumlah token dalam chunk\n",
    "                    f.write(chunk + \"\\n\\n\")  # Menulis konten chunk\n",
    "                    f.write(\"-\" * 30 + \"\\n\\n\")  # Menulis garis pemisah antar chunk\n",
    "\n",
    "            logging.info(f\"Successfully saved chunks to: {output_path}\")  # Mencatat bahwa chunk berhasil disimpan\n",
    "            logging.info(f\"Total chunks: {len(content_chunks)}\")  # Mencatat total chunk yang disimpan\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving chunks: {str(e)}\")  # Mencatat kesalahan saat menyimpan chunk\n",
    "            raise  # Mengangkat kembali kesalahan\n",
    "\n",
    "# Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"./data/tko.pdf\"  # Menentukan jalur ke file PDF yang akan diproses\n",
    "    output_path = \"./output_chunks/output_token_base.txt\"  # Menentukan jalur untuk menyimpan hasil chunking\n",
    "\n",
    "    processor = SentenceChunkProcessor(chunk_size=500)  # Membuat instance dari SentenceChunkProcessor dengan ukuran chunk 500\n",
    "\n",
    "    if not os.path.exists(file_path):  # Memeriksa apakah file PDF ada\n",
    "        print(f\"Error: File '{file_path}' tidak ditemukan!\")  # Menampilkan pesan kesalahan jika file tidak ditemukan\n",
    "    else:\n",
    "        try:\n",
    "            print(\"🔍 Memulai proses membaca PDF...\")  # Menampilkan pesan bahwa proses membaca PDF dimulai\n",
    "            text = processor.read_pdf_and_clean(file_path)  # Membaca dan membersihkan teks dari PDF\n",
    "            print(f\"Teks PDF terbaca (50 karakter pertama): {text[:50]}...\")  # Menampilkan 50 karakter pertama dari teks yang terbaca\n",
    "\n",
    "            print(\"🔍 Memulai proses chunking...\")  # Menampilkan pesan bahwa proses chunking dimulai\n",
    "            content_chunks = processor.chunk_text_by_sentences(text)  # Membagi teks menjadi chunk berdasarkan kalimat\n",
    "            print(f\"Total chunk yang dibuat: {len(content_chunks)}\")  # Menampilkan total chunk yang dibuat\n",
    "\n",
    "            print(\"🔍 Menyimpan hasil chunking ke output file...\")  # Menampilkan pesan bahwa hasil chunking akan disimpan\n",
    "            processor.save_chunks(content_chunks, output_path)  # Menyimpan chunk ke file output\n",
    "            print(f\"✅ File output tersimpan di: {output_path}\")  # Menampilkan pesan sukses\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Terjadi error: {str(e)}\")  # Menampilkan pesan kesalahan jika terjadi error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTEXT ENRICHED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler\n",
    "from PyPDF2 import PdfReader  # Mengimpor PdfReader dari PyPDF2 untuk membaca file PDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Mengimpor RecursiveCharacterTextSplitter untuk membagi teks\n",
    "from reportlab.lib.pagesizes import A4  # Mengimpor ukuran halaman A4 dari reportlab\n",
    "from reportlab.pdfgen import canvas  # Mengimpor canvas dari reportlab untuk membuat PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Kelas Advanced Chunking\n",
    "    \n",
    "    Berisikan fungsi\n",
    "    - init (Konfigurasi dan inisialisasi parameter)\n",
    "    - baca_pdf\n",
    "    - preprocessing_teks\n",
    "    - chunk_dengan_konteks\n",
    "    - proses_folder_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedChunking:\n",
    "    def __init__(self, ukuran_chunk=1000, overlap=200):\n",
    "        # Inisialisasi parameter untuk chunking\n",
    "        self.ukuran_chunk = ukuran_chunk  # Ukuran maksimum setiap chunk\n",
    "        self.overlap = overlap  # Jumlah karakter yang tumpang tindih antara chunk\n",
    "        self.output_folder = 'output_chunks'  # Folder untuk menyimpan hasil chunking\n",
    "        os.makedirs(self.output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "\n",
    "    def baca_pdf(self, path_file):\n",
    "        \"\"\"Membaca teks dari file PDF.\"\"\"\n",
    "        pdf_reader = PdfReader(path_file)  # Membuka file PDF\n",
    "        teks = \"\".join([halaman.extract_text() or \"\" for halaman in pdf_reader.pages])  # Mengambil teks dari setiap halaman\n",
    "        return teks  # Mengembalikan teks yang diekstrak\n",
    "\n",
    "    def preprocessing_teks(self, teks):\n",
    "        \"\"\"Membersihkan teks dari spasi berlebih dan karakter tidak perlu.\"\"\"\n",
    "        return re.sub(r'\\s+', ' ', teks).strip()  # Menghapus spasi berlebih dan mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "    def chunk_dengan_konteks(self, teks):\n",
    "        \"\"\"Membagi teks menjadi chunk dengan mempertahankan sedikit konteks dari chunk sebelumnya dan berikutnya.\"\"\"\n",
    "        teks_bersih = self.preprocessing_teks(teks)  # Membersihkan teks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.ukuran_chunk,  # Ukuran chunk\n",
    "            chunk_overlap=self.overlap,  # Jumlah overlap\n",
    "            length_function=len,  # Fungsi untuk menghitung panjang\n",
    "            separators=[\".\", \"\\n\\n\", \"\\n\", \" \", \"\"]  # Pemisah untuk chunking\n",
    "        )\n",
    "\n",
    "        chunks = text_splitter.create_documents([teks_bersih])  # Membagi teks menjadi chunk\n",
    "        chunks_dengan_konteks = []  # Inisialisasi daftar untuk menyimpan chunk dengan konteks\n",
    "\n",
    "        for i, chunk in enumerate(chunks):  # Iterasi melalui setiap chunk\n",
    "            konteks_sebelum = chunks[i - 1].page_content if i > 0 else \"\"  # Ambil konteks dari chunk sebelumnya\n",
    "            konteks_sesudah = chunks[i + 1].page_content if i < len(chunks) - 1 else \"\"  # Ambil konteks dari chunk berikutnya\n",
    "\n",
    "            # Gabungkan konteks dengan chunk saat ini\n",
    "            chunk_konteks = f\"{konteks_sebelum} {chunk.page_content} {konteks_sesudah}\".strip()\n",
    "            chunk_konteks = chunk_konteks.rstrip('.') + '.'  # Pastikan chunk diakhiri dengan titik\n",
    "\n",
    "            # Metadata untuk chunk\n",
    "            metadata = {\n",
    "                'chunk_id': i,\n",
    "                'total_chunks': len(chunks),\n",
    "                'panjang_chunk': len(chunk_konteks),\n",
    "                'konteks_sebelum': konteks_sebelum[:50],  # Ambil 50 karakter pertama dari konteks sebelumnya\n",
    "                'konteks_sesudah': konteks_sesudah[:50]   # Ambil 50 karakter pertama dari konteks berikutnya\n",
    "            }\n",
    "\n",
    "            chunks_dengan_konteks.append({'konten': chunk_konteks, 'metadata': metadata})  # Tambahkan chunk dan metadata ke daftar\n",
    "\n",
    "        return chunks_dengan_konteks  # Mengembalikan daftar chunk dengan konteks\n",
    "\n",
    "    def proses_folder_pdf(self, folder_path):\n",
    "        \"\"\"Memproses semua file PDF dalam folder dan membaginya menjadi chunk.\"\"\"\n",
    "        hasil = {}  # Inisialisasi dictionary untuk menyimpan hasil\n",
    "        total_chunk = 0  # Inisialisasi total chunk\n",
    "\n",
    "        for filename in os.listdir(folder_path):  # Iterasi melalui setiap file dalam folder\n",
    "            if filename.endswith('.pdf'):  # Memeriksa apakah file adalah PDF\n",
    "                file_path = os.path.join(folder_path, filename)  # Mendapatkan jalur lengkap file\n",
    "                print(f\"\\n{'='*50}\")  # Menampilkan garis pemisah\n",
    "                print(f\"Memproses file: {filename}\")  # Menampilkan nama file yang sedang diproses\n",
    "                print(f\"{'='*50}\")\n",
    "\n",
    "                teks_pdf = self.baca_pdf(file_path)  # Membaca teks dari file PDF\n",
    "                chunks = self.chunk_dengan_konteks(teks_pdf)  # Membagi teks menjadi chunk total_chunk += len(chunks)  # Menambahkan jumlah chunk yang dihasilkan\n",
    "                hasil[filename] = {'chunks': chunks}  # Menyimpan hasil chunking dalam dictionary\n",
    "\n",
    "                # Menyimpan hasil chunking ke file teks\n",
    "                chunk_file = os.path.join(self.output_folder, f\"{filename}_chunks.txt\")  # Menentukan jalur untuk file output\n",
    "                with open(chunk_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "                    for i, chunk in enumerate(chunks, 1):  # Iterasi melalui setiap chunk\n",
    "                        f.write(f\"Chunk {i}:\\n{chunk['konten']}\\n\\n\")  # Menulis konten chunk ke file\n",
    "\n",
    "                print(f\"Hasil chunking disimpan di: {chunk_file}\")  # Menampilkan pesan bahwa hasil chunking telah disimpan\n",
    "\n",
    "        return hasil  # Mengembalikan hasil chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Eksekusi Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chunker = AdvancedChunking(ukuran_chunk=1000, overlap=200)  # Membuat instance dari AdvancedChunking dengan ukuran chunk dan overlap yang ditentukan\n",
    "    FOLDER_PATH = \"./data\"  # Menentukan jalur folder yang berisi file PDF\n",
    "\n",
    "    try:\n",
    "        hasil_chunking = chunker.proses_folder_pdf(FOLDER_PATH)  # Memproses semua file PDF dalam folder\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi error: {str(e)}\")  # Menampilkan pesan kesalahan jika terjadi error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Keseluruhan Kode Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Memproses file: dokumen_pdf.pdf\n",
      "==================================================\n",
      "Hasil chunking disimpan di: output_chunks\\dokumen_pdf.pdf_chunks.txt\n",
      "\n",
      "==================================================\n",
      "Memproses file: tko.pdf\n",
      "==================================================\n",
      "Hasil chunking disimpan di: output_chunks\\tko.pdf_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler\n",
    "from PyPDF2 import PdfReader  # Mengimpor PdfReader dari PyPDF2 untuk membaca file PDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Mengimpor RecursiveCharacterTextSplitter untuk membagi teks\n",
    "from reportlab.lib.pagesizes import A4  # Mengimpor ukuran halaman A4 dari reportlab\n",
    "from reportlab.pdfgen import canvas  # Mengimpor canvas dari reportlab untuk membuat PDF\n",
    "\n",
    "class AdvancedChunking:\n",
    "    def __init__(self, ukuran_chunk=1000, overlap=200):\n",
    "        # Inisialisasi parameter untuk chunking\n",
    "        self.ukuran_chunk = ukuran_chunk  # Ukuran maksimum setiap chunk\n",
    "        self.overlap = overlap  # Jumlah karakter yang tumpang tindih antara chunk\n",
    "        self.output_folder = 'output_chunks'  # Folder untuk menyimpan hasil chunking\n",
    "        os.makedirs(self.output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "\n",
    "    def baca_pdf(self, path_file):\n",
    "        \"\"\"Membaca teks dari file PDF.\"\"\"\n",
    "        pdf_reader = PdfReader(path_file)  # Membuka file PDF\n",
    "        teks = \"\".join([halaman.extract_text() or \"\" for halaman in pdf_reader.pages])  # Mengambil teks dari setiap halaman\n",
    "        return teks  # Mengembalikan teks yang diekstrak\n",
    "\n",
    "    def preprocessing_teks(self, teks):\n",
    "        \"\"\"Membersihkan teks dari spasi berlebih dan karakter tidak perlu.\"\"\"\n",
    "        return re.sub(r'\\s+', ' ', teks).strip()  # Menghapus spasi berlebih dan mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "    def chunk_dengan_konteks(self, teks):\n",
    "        \"\"\"Membagi teks menjadi chunk dengan mempertahankan sedikit konteks dari chunk sebelumnya dan berikutnya.\"\"\"\n",
    "        teks_bersih = self.preprocessing_teks(teks)  # Membersihkan teks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.ukuran_chunk,  # Ukuran chunk\n",
    "            chunk_overlap=self.overlap,  # Jumlah overlap\n",
    "            length_function=len,  # Fungsi untuk menghitung panjang\n",
    "            separators=[\".\", \"\\n\\n\", \"\\n\", \" \", \"\"]  # Pemisah untuk chunking\n",
    "        )\n",
    "\n",
    "        chunks = text_splitter.create_documents([teks_bersih])  # Membagi teks menjadi chunk\n",
    "        chunks_dengan_konteks = []  # Inisialisasi daftar untuk menyimpan chunk dengan konteks\n",
    "\n",
    "        for i, chunk in enumerate(chunks):  # Iterasi melalui setiap chunk\n",
    "            konteks_sebelum = chunks[i - 1].page_content if i > 0 else \"\"  # Ambil konteks dari chunk sebelumnya\n",
    "            konteks_sesudah = chunks[i + 1].page_content if i < len(chunks) - 1 else \"\"  # Ambil konteks dari chunk berikutnya\n",
    "\n",
    "            # Gabungkan konteks dengan chunk saat ini\n",
    "            chunk_konteks = f\"{konteks_sebelum} {chunk.page_content} {konteks_sesudah}\".strip()\n",
    "            chunk_konteks = chunk_konteks.rstrip('.') + '.'  # Pastikan chunk diakhiri dengan titik\n",
    "\n",
    "            # Metadata untuk chunk\n",
    "            metadata = {\n",
    "                'chunk_id': i,\n",
    "                'total_chunks': len(chunks),\n",
    "                'panjang_chunk': len(chunk_konteks),\n",
    "                'konteks_sebelum': konteks_sebelum[:50],  # Ambil 50 karakter pertama dari konteks sebelumnya\n",
    "                'konteks_sesudah': konteks_sesudah[:50]   # Ambil 50 karakter pertama dari konteks berikutnya\n",
    "            }\n",
    "\n",
    "            chunks_dengan_konteks.append({'konten': chunk_konteks, 'metadata': metadata})  # Tambahkan chunk dan metadata ke daftar\n",
    "\n",
    "        return chunks_dengan_konteks  # Mengembalikan daftar chunk dengan konteks\n",
    "\n",
    "    def proses_folder_pdf(self, folder_path):\n",
    "        \"\"\"Memproses semua file PDF dalam folder dan membaginya menjadi chunk.\"\"\"\n",
    "        hasil = {}  # Inisialisasi dictionary untuk menyimpan hasil\n",
    "        total_chunk = 0  # Inisialisasi total chunk\n",
    "\n",
    "        for filename in os.listdir(folder_path):  # Iterasi melalui setiap file dalam folder\n",
    "            if filename.endswith('.pdf'):  # Memeriksa apakah file adalah PDF\n",
    "                file_path = os.path.join(folder_path, filename)  # Mendapatkan jalur lengkap file\n",
    "                print(f\"\\n{'='*50}\")  # Menampilkan garis pemisah\n",
    "                print(f\"Memproses file: {filename}\")  # Menampilkan nama file yang sedang diproses\n",
    "                print(f\"{'='*50}\")\n",
    "\n",
    "                teks_pdf = self.baca_pdf(file_path)  # Membaca teks dari file PDF\n",
    "                chunks = self.chunk_dengan_konteks(teks_pdf)  # Membagi teks menjadi chunk total_chunk += len(chunks)  # Menambahkan jumlah chunk yang dihasilkan\n",
    "                hasil[filename] = {'chunks': chunks}  # Menyimpan hasil chunking dalam dictionary\n",
    "\n",
    "                # Menyimpan hasil chunking ke file teks\n",
    "                chunk_file = os.path.join(self.output_folder, f\"{filename}_chunks.txt\")  # Menentukan jalur untuk file output\n",
    "                with open(chunk_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "                    for i, chunk in enumerate(chunks, 1):  # Iterasi melalui setiap chunk\n",
    "                        f.write(f\"Chunk {i}:\\n{chunk['konten']}\\n\\n\")  # Menulis konten chunk ke file\n",
    "\n",
    "                print(f\"Hasil chunking disimpan di: {chunk_file}\")  # Menampilkan pesan bahwa hasil chunking telah disimpan\n",
    "\n",
    "        return hasil  # Mengembalikan hasil chunking\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chunker = AdvancedChunking(ukuran_chunk=1000, overlap=200)  # Membuat instance dari AdvancedChunking dengan ukuran chunk dan overlap yang ditentukan\n",
    "    FOLDER_PATH = \"./data\"  # Menentukan jalur folder yang berisi file PDF\n",
    "\n",
    "    try:\n",
    "        hasil_chunking = chunker.proses_folder_pdf(FOLDER_PATH)  # Memproses semua file PDF dalam folder\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi error: {str(e)}\")  # Menampilkan pesan kesalahan jika terjadi error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEMANTIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # Mengimpor PyMuPDF untuk membaca file PDF\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fungsi read_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    \"\"\"Membaca teks dari file PDF.\"\"\"\n",
    "    doc = fitz.open(file_path)  # Membuka file PDF\n",
    "    # Mengambil teks dari setiap halaman dan menggabungkannya dengan newline\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])  \n",
    "    return text  # Mengembalikan teks yang diekstrak dari PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fungsi celan_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Membersihkan teks dari karakter khusus, spasi berlebih, dan format yang tidak diperlukan.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Mengganti newline dengan spasi dan menghapus spasi di awal/akhir\n",
    "    text = \" \".join(text.split())  # Menghapus spasi berlebih di antara kata\n",
    "    return text  # Mengembalikan teks yang telah dibersihkan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fungsi chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000):\n",
    "    \"\"\"Membagi teks menjadi chunk lebih kecil berdasarkan ukuran karakter.\"\"\"\n",
    "    # Membagi teks menjadi daftar chunk dengan ukuran maksimum chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fungsi save_to_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(filename, chunks):\n",
    "    \"\"\"Menyimpan hasil ke file .txt.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "        for i, chunk in enumerate(chunks):  # Iterasi melalui setiap chunk\n",
    "            f.write(f\"Chunk {i+1}:\\n{chunk}\\n\\n\")  # Menulis chunk ke file dengan header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Eksekusi Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Path ke file PDF\n",
    "    pdf_path = \"./data/tko.pdf\"  # Menentukan jalur ke file PDF\n",
    "    output_txt = \"./output_chunks/output_semantic.txt\"  # Menentukan jalur untuk menyimpan file output\n",
    "\n",
    "    # Membaca, membersihkan, dan membagi teks\n",
    "    raw_text = read_pdf(pdf_path)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "\n",
    "    # Menyimpan output ke file .txt\n",
    "    save_to_txt(output_txt, chunks)\n",
    "\n",
    "    print(f\"Hasil telah disimpan dalam {output_txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Keseluruhan Kode Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil telah disimpan dalam ./output_chunks/output_semantic.txt\n"
     ]
    }
   ],
   "source": [
    "import fitz  # Mengimpor PyMuPDF untuk membaca file PDF\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Membaca teks dari file PDF.\"\"\"\n",
    "    doc = fitz.open(file_path)  # Membuka file PDF\n",
    "    # Mengambil teks dari setiap halaman dan menggabungkannya dengan newline\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])  \n",
    "    return text  # Mengembalikan teks yang diekstrak dari PDF\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Membersihkan teks dari karakter khusus, spasi berlebih, dan format yang tidak diperlukan.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Mengganti newline dengan spasi dan menghapus spasi di awal/akhir\n",
    "    text = \" \".join(text.split())  # Menghapus spasi berlebih di antara kata\n",
    "    return text  # Mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    \"\"\"Membagi teks menjadi chunk lebih kecil berdasarkan ukuran karakter.\"\"\"\n",
    "    # Membagi teks menjadi daftar chunk dengan ukuran maksimum chunk_size\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def save_to_txt(filename, chunks):\n",
    "    \"\"\"Menyimpan hasil ke file .txt.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:  # Membuka file untuk menulis\n",
    "        for i, chunk in enumerate(chunks):  # Iterasi melalui setiap chunk\n",
    "            f.write(f\"Chunk {i+1}:\\n{chunk}\\n\\n\")  # Menulis chunk ke file dengan header\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path ke file PDF\n",
    "    pdf_path = \"./data/tko.pdf\"  # Menentukan jalur ke file PDF\n",
    "    output_txt = \"./output_chunks/output_semantic.txt\"  # Menentukan jalur untuk menyimpan file output\n",
    "\n",
    "    # Membaca, membersihkan, dan membagi teks\n",
    "    raw_text = read_pdf(pdf_path)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    chunks = chunk_text(cleaned_text)\n",
    "\n",
    "    # Menyimpan output ke file .txt\n",
    "    save_to_txt(output_txt, chunks)\n",
    "\n",
    "    print(f\"Hasil telah disimpan dalam {output_txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADVANCE SEMANTIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import spacy  # Mengimpor spaCy untuk pemrosesan bahasa alami\n",
    "import docx  # Mengimpor modul docx untuk membaca file DOCX\n",
    "import fitz  # Mengimpor fitz (PyMuPDF) untuk membaca file PDF\n",
    "import numpy as np  # Mengimpor numpy untuk operasi numerik\n",
    "import gensim  # Mengimpor gensim untuk model topik\n",
    "from gensim import corpora  # Mengimpor corpora dari gensim untuk membuat kamus\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Mengimpor model embedding dari Hugging Face\n",
    "from langchain_ollama.llms import OllamaLLM  # Mengimpor model LLM dari Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Kondigurasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")  # Memuat model bahasa Inggris dari spaCy\n",
    "OLLAMA_MODEL = \"llama3.2\"  # Menentukan model LLM yang akan digunakan\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL)  # Menginisialisasi model LLM\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # Memuat model embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fungsi read_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    print(f\"Reading PDF file: {file_path}\")  # Menampilkan pesan bahwa file PDF sedang dibaca\n",
    "    text = \"\"  # Inisialisasi string kosong untuk menyimpan teks\n",
    "    with fitz.open(file_path) as doc:  # Membuka file PDF\n",
    "        for page in doc:  # Iterasi melalui setiap halaman\n",
    "            text += page.get_text(\"text\") + \"\\n\"  # Mengambil teks dari halaman dan menambahkannya ke string\n",
    "    return text  # Mengembalikan teks yang diekstrak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fungsi read_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docx(file_path):\n",
    "    print(f\"Reading DOCX file: {file_path}\")  # Menampilkan pesan bahwa file DOCX sedang dibaca\n",
    "    doc = docx.Document(file_path)  # Membuka file DOCX\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])  # Menggabungkan teks dari setiap paragraf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fungsi clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    print(\"Preprocessing text: Cleaning and normalizing...\")  # Menampilkan pesan bahwa teks sedang diproses\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Mengganti newline dengan spasi dan menghapus spasi di awal/akhir\n",
    "    text = \" \".join(text.split())  # Menghapus spasi berlebih di antara kata\n",
    "    return text  # Mengembalikan teks yang telah dibersihkan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Fungsi semantic_chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_chunking(text, chunk_size=1000):\n",
    "    print(\"Performing semantic chunking...\")  # Menampilkan pesan bahwa chunking sedang dilakukan\n",
    "    doc = nlp(text)  # Memproses teks dengan spaCy\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]  # Mengambil kalimat yang tidak kosong\n",
    "    \n",
    "    if not sentences:  # Jika tidak ada kalimat yang valid\n",
    "        return []  # Mengembalikan daftar kosong\n",
    "    \n",
    "    chunks = []  # Inisialisasi daftar untuk menyimpan chunk\n",
    "    chunk = \"\"  # Inisialisasi string untuk menyimpan teks chunk saat ini\n",
    "    for sentence in sentences:  # Iterasi melalui setiap kalimat\n",
    "        if len(chunk) + len(sentence) < chunk_size:  # Jika penambahan kalimat tidak melebihi ukuran chunk\n",
    "            chunk += \" \" + sentence  # Tambahkan kalimat ke chunk\n",
    "        else:\n",
    "            chunks.append(chunk.strip())  # Tambahkan chunk yang telah dibentuk ke daftar\n",
    "            chunk = sentence  # Mulai chunk baru dengan kalimat saat ini\n",
    "    if chunk:  # Jika ada chunk yang tersisa\n",
    "        chunks.append(chunk.strip())  # Tambahkan chunk terakhir ke daftar\n",
    "    \n",
    "    print(f\"Generated {len(chunks)} chunks.\")  # Menampilkan jumlah chunk yang dihasilkan\n",
    "    return chunks  # Mengembalikan daftar chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Fungsi extract_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topics(text, num_topics=5):\n",
    "    print(\"Extracting topics using LDA...\")  # Menampilkan pesan bahwa topik sedang diekstrak\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]  # Tokenisasi dan lemmatization\n",
    "    dictionary = corpora.Dictionary([words])  # Membuat kamus dari kata-kata\n",
    "    corpus = [dictionary.doc2bow(words)]  # Membuat korpus dari kata-kata\n",
    "    lda_model = gensim .models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4)  # Melatih model LDA dengan korpus\n",
    "    topics = lda_model.show_topics(formatted=False)  # Mengambil topik yang dihasilkan dari model\n",
    "    \n",
    "    return topics[0] if topics else None  # Mengembalikan topik pertama jika ada, jika tidak mengembalikan None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Fungsi process_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, output_folder):\n",
    "    print(f\"Processing document: {file_path}\")  # Menampilkan pesan bahwa dokumen sedang diproses\n",
    "    ext = file_path.split(\".\")[-1].lower()  # Mengambil ekstensi file\n",
    "    \n",
    "    if ext == \"txt\":  # Jika ekstensi adalah TXT\n",
    "        text = read_txt(file_path)  # Membaca file TXT\n",
    "    elif ext == \"pdf\":  # Jika ekstensi adalah PDF\n",
    "        text = read_pdf(file_path)  # Membaca file PDF\n",
    "    elif ext == \"docx\":  # Jika ekstensi adalah DOCX\n",
    "        text = read_docx(file_path)  # Membaca file DOCX\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {ext}\")  # Menampilkan pesan jika format file tidak didukung\n",
    "        return  # Menghentikan eksekusi fungsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Fungsi main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    input_folder = \"./data\"  # Menentukan folder input\n",
    "    output_folder = \"./output_chunks/\"  # Menentukan folder output\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):  # Iterasi melalui setiap file dalam folder input\n",
    "        file_path = os.path.join(input_folder, file_name)  # Menentukan jalur file\n",
    "        if os.path.isfile(file_path):  # Jika jalur adalah file\n",
    "            process_document(file_path, output_folder)  # Memproses dokumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Eksekusi Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi main jika file dijalankan sebagai skrip utama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Keseluruhan Kode Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document: ./data\\.gitkeep\n",
      "Unsupported file format: gitkeep\n",
      "Processing document: ./data\\dokumen_pdf.pdf\n",
      "Reading PDF file: ./data\\dokumen_pdf.pdf\n",
      "Preprocessing text: Cleaning and normalizing...\n",
      "Performing semantic chunking...\n",
      "Generated 5 chunks.\n",
      "Processing chunk 1...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 2...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 3...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 4...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 5...\n",
      "Extracting topics using LDA...\n",
      "Finished processing: ./data\\dokumen_pdf.pdf -> ./output_chunks/dokumen_pdf_chunks.txt\n",
      "Processing document: ./data\\tko.pdf\n",
      "Reading PDF file: ./data\\tko.pdf\n",
      "Preprocessing text: Cleaning and normalizing...\n",
      "Performing semantic chunking...\n",
      "Generated 10 chunks.\n",
      "Processing chunk 1...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 2...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 3...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 4...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 5...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 6...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 7...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 8...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 9...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 10...\n",
      "Extracting topics using LDA...\n",
      "Finished processing: ./data\\tko.pdf -> ./output_chunks/tko_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import spacy  # Mengimpor spaCy untuk pemrosesan bahasa alami\n",
    "import docx  # Mengimpor modul docx untuk membaca file DOCX\n",
    "import fitz  # Mengimpor fitz (PyMuPDF) untuk membaca file PDF\n",
    "import numpy as np  # Mengimpor numpy untuk operasi numerik\n",
    "import gensim  # Mengimpor gensim untuk model topik\n",
    "from gensim import corpora  # Mengimpor corpora dari gensim untuk membuat kamus\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Mengimpor model embedding dari Hugging Face\n",
    "from langchain_ollama.llms import OllamaLLM  # Mengimpor model LLM dari Ollama\n",
    "\n",
    "# Memuat model NLP\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Memuat model bahasa Inggris dari spaCy\n",
    "OLLAMA_MODEL = \"llama3.2\"  # Menentukan model LLM yang akan digunakan\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL)  # Menginisialisasi model LLM\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")  # Memuat model embedding\n",
    "\n",
    "def read_txt(file_path):\n",
    "    print(f\"Reading TXT file: {file_path}\")  # Menampilkan pesan bahwa file TXT sedang dibaca\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:  # Membuka file dalam mode baca\n",
    "        return f.read()  # Mengembalikan seluruh isi file\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    print(f\"Reading PDF file: {file_path}\")  # Menampilkan pesan bahwa file PDF sedang dibaca\n",
    "    text = \"\"  # Inisialisasi string kosong untuk menyimpan teks\n",
    "    with fitz.open(file_path) as doc:  # Membuka file PDF\n",
    "        for page in doc:  # Iterasi melalui setiap halaman\n",
    "            text += page.get_text(\"text\") + \"\\n\"  # Mengambil teks dari halaman dan menambahkannya ke string\n",
    "    return text  # Mengembalikan teks yang diekstrak\n",
    "\n",
    "def read_docx(file_path):\n",
    "    print(f\"Reading DOCX file: {file_path}\")  # Menampilkan pesan bahwa file DOCX sedang dibaca\n",
    "    doc = docx.Document(file_path)  # Membuka file DOCX\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])  # Menggabungkan teks dari setiap paragraf\n",
    "\n",
    "def clean_text(text):\n",
    "    print(\"Preprocessing text: Cleaning and normalizing...\")  # Menampilkan pesan bahwa teks sedang diproses\n",
    "    text = text.replace(\"\\n\", \" \").strip()  # Mengganti newline dengan spasi dan menghapus spasi di awal/akhir\n",
    "    text = \" \".join(text.split())  # Menghapus spasi berlebih di antara kata\n",
    "    return text  # Mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "def semantic_chunking(text, chunk_size=1000):\n",
    "    print(\"Performing semantic chunking...\")  # Menampilkan pesan bahwa chunking sedang dilakukan\n",
    "    doc = nlp(text)  # Memproses teks dengan spaCy\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]  # Mengambil kalimat yang tidak kosong\n",
    "    \n",
    "    if not sentences:  # Jika tidak ada kalimat yang valid\n",
    "        return []  # Mengembalikan daftar kosong\n",
    "    \n",
    "    chunks = []  # Inisialisasi daftar untuk menyimpan chunk\n",
    "    chunk = \"\"  # Inisialisasi string untuk menyimpan teks chunk saat ini\n",
    "    for sentence in sentences:  # Iterasi melalui setiap kalimat\n",
    "        if len(chunk) + len(sentence) < chunk_size:  # Jika penambahan kalimat tidak melebihi ukuran chunk\n",
    "            chunk += \" \" + sentence  # Tambahkan kalimat ke chunk\n",
    "        else:\n",
    "            chunks.append(chunk.strip())  # Tambahkan chunk yang telah dibentuk ke daftar\n",
    "            chunk = sentence  # Mulai chunk baru dengan kalimat saat ini\n",
    "    if chunk:  # Jika ada chunk yang tersisa\n",
    "        chunks.append(chunk.strip())  # Tambahkan chunk terakhir ke daftar\n",
    "    \n",
    "    print(f\"Generated {len(chunks)} chunks.\")  # Menampilkan jumlah chunk yang dihasilkan\n",
    "    return chunks  # Mengembalikan daftar chunk\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    print(\"Extracting topics using LDA...\")  # Menampilkan pesan bahwa topik sedang diekstrak\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]  # Tokenisasi dan lemmatization\n",
    "    dictionary = corpora.Dictionary([words])  # Membuat kamus dari kata-kata\n",
    "    corpus = [dictionary.doc2bow(words)]  # Membuat korpus dari kata-kata\n",
    "    lda_model = gensim .models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4)  # Melatih model LDA dengan korpus\n",
    "    topics = lda_model.show_topics(formatted=False)  # Mengambil topik yang dihasilkan dari model\n",
    "    \n",
    "    return topics[0] if topics else None  # Mengembalikan topik pertama jika ada, jika tidak mengembalikan None\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    print(f\"Processing document: {file_path}\")  # Menampilkan pesan bahwa dokumen sedang diproses\n",
    "    ext = file_path.split(\".\")[-1].lower()  # Mengambil ekstensi file\n",
    "    \n",
    "    if ext == \"txt\":  # Jika ekstensi adalah TXT\n",
    "        text = read_txt(file_path)  # Membaca file TXT\n",
    "    elif ext == \"pdf\":  # Jika ekstensi adalah PDF\n",
    "        text = read_pdf(file_path)  # Membaca file PDF\n",
    "    elif ext == \"docx\":  # Jika ekstensi adalah DOCX\n",
    "        text = read_docx(file_path)  # Membaca file DOCX\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {ext}\")  # Menampilkan pesan jika format file tidak didukung\n",
    "        return  # Menghentikan eksekusi fungsi\n",
    "    \n",
    "    cleaned_text = clean_text(text)  # Membersihkan teks yang dibaca\n",
    "    chunks = semantic_chunking(cleaned_text)  # Melakukan chunking pada teks yang telah dibersihkan\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")  # Menentukan jalur untuk file output\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:  # Membuka file output untuk menulis\n",
    "        for i, chunk in enumerate(chunks):  # Iterasi melalui setiap chunk\n",
    "            print(f\"Processing chunk {i+1}...\")  # Menampilkan pesan bahwa chunk sedang diproses\n",
    "            best_topic = extract_topics(chunk)  # Mengekstrak topik dari chunk\n",
    "            if best_topic:  # Jika ada topik yang ditemukan\n",
    "                topic_words = \", \".join([word for word, _ in best_topic[1]])  # Menggabungkan kata-kata topik\n",
    "                topic_str = f\"Best Topic: {topic_words}\"  # Menyusun string topik terbaik\n",
    "            else:\n",
    "                topic_str = \"Best Topic: No topics found\"  # Menyusun string jika tidak ada topik ditemukan\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n{topic_str}\\n\\n\")  # Menulis chunk dan topik ke file output\n",
    "    \n",
    "    print(f\"Finished processing: {file_path} -> {output_file}\")  # Menampilkan pesan bahwa pemrosesan selesai\n",
    "\n",
    "def main():\n",
    "    input_folder = \"./data\"  # Menentukan folder input\n",
    "    output_folder = \"./output_chunks/\"  # Menentukan folder output\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Membuat folder output jika belum ada\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):  # Iterasi melalui setiap file dalam folder input\n",
    "        file_path = os.path.join(input_folder, file_name)  # Menentukan jalur file\n",
    "        if os.path.isfile(file_path):  # Jika jalur adalah file\n",
    "            process_document(file_path, output_folder)  # Memproses dokumen\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # Menjalankan fungsi main jika file dijalankan sebagai skrip utama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AGENTIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import json  # Mengimpor modul json untuk pengelolaan data JSON\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler\n",
    "from IPython.display import Markdown  # Mengimpor Markdown untuk menampilkan teks dalam format Markdown di Jupyter Notebook\n",
    "from phi.agent import Agent  # Mengimpor Agent untuk penggunaan model LLM\n",
    "from phi.model.ollama import Ollama  # Mengimpor Ollama untuk penggunaan model LLM\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Mengimpor model embedding dari Hugging Face\n",
    "from langchain.vectorstores import FAISS  # Mengimpor FAISS untuk pengelolaan vektor\n",
    "from langchain_core.documents import Document  # Mengimpor Document untuk pengelolaan dokumen\n",
    "from utils.document_processor import DocumentProcessor  # Mengimpor DocumentProcessor untuk pengelolaan dokumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Konfigurasi Model dan Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi path dan model\n",
    "DATA_PATH = \"./data\"  # Menentukan folder data\n",
    "INDEX_PATH = \"faiss_index\"  # Menentukan jalur untuk indeks FAISS\n",
    "CHUNKED_DATA_PATH = \"./output_chunks\"  # Menentukan folder untuk hasil chunking\n",
    "METADATA_PATH = \"./metadata\"  # Menentukan folder untuk metadata\n",
    "OLLAMA_MODEL = \"llama3.2\"  # Menentukan model LLM yang akan digunakan\n",
    "\n",
    "# Pastikan folder tersedia\n",
    "os.makedirs(CHUNKED_DATA_PATH, exist_ok=True)  # Membuat folder untuk hasil chunking jika belum ada\n",
    "os.makedirs(METADATA_PATH, exist_ok=True)  # Membuat folder untuk metadata jika belum ada\n",
    "\n",
    "# Inisialisasi model Ollama\n",
    "llm = Ollama(id=OLLAMA_MODEL)  # Menginisialisasi model LLM\n",
    "\n",
    "# Inisialisasi embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # Menginisialisasi model embedding\n",
    "\n",
    "# Inisialisasi document processor\n",
    "docs = DocumentProcessor()  # Menginisialisasi DocumentProcessor\n",
    "\n",
    "# Inisialisasi Agent dengan model Ollama\n",
    "agent = Agent(model=llm, show_tool_calls=True, markdown=True)  # Menginisialisasi Agent dengan model LLM\n",
    "\n",
    "# Parameter chunking\n",
    "CHUNK_SIZE = 1200  # Menentukan ukuran chunk\n",
    "MIN_CHUNK_SIZE = 500  # Menentukan ukuran chunk minimum\n",
    "MAX_CHUNKS = 30  # Menentukan jumlah chunk maksimum\n",
    "\n",
    "extracted_docs = []  # Inisialisasi daftar untuk menyimpan dokumen yang telah diproses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fungsi clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" Membersihkan teks dari karakter kosong, whitespace berlebih, dan simbol aneh \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Mengganti spasi berlebih dengan satu spasi dan menghapus spasi di awal/akhir\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Mengganti karakter yang tidak dapat diwakili dengan spasi\n",
    "    return text  # Mengembalikan teks yang telah dibersihkan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Fungsi clean_agent_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_agent_output(text):\n",
    "    \"\"\" Membersihkan output dari agent agar tidak mengandung pemisah yang tidak perlu \"\"\"\n",
    "    text = re.sub(r'\\n?###.*?\\n', '\\n', text)  # Menghapus judul markdown seperti ### Section\n",
    "    text = re.sub(r'\\n?\\*\\*\\*.*?\\n', '\\n', text)  # Menghapus pemisah ***\n",
    "    text = re.sub(r'\\n?-{3,}\\n?', '\\n', text)  # Menghapus garis pemisah ---\n",
    "    text = re.sub(r'(\\s*-{2,}\\s*)', ' ', text)  # Menghapus pemisah --\n",
    "    text = re.sub(r'(\\s*\\*{2,}\\s*)', ' ', text)  # Menghapus ** pemisah tebal\n",
    "    text = re.sub(r'(\\s*\\*\\s*)', ' ', text)  # Menghapus * pemisah tunggal\n",
    "    text = re.sub(r'(\\s*-\\s*)', ' ', text)  # Menghapus pemisah -\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text).strip()  # Mengganti newline berlebih dengan dua newline\n",
    "    return text  # Mengembalikan teks yang telah dibersihkan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fungsi membaca, memproses, dan membagi dokumen teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(DATA_PATH):  # Iterasi melalui setiap file dalam folder data\n",
    "    valid_extensions = ('.pdf', '.docx', '.txt')  # Menentukan ekstensi file yang valid\n",
    "    if not filename.lower().endswith(valid_extensions):  # Jika file tidak memiliki ek stensi yang valid, lanjutkan ke file berikutnya\n",
    "        continue  # Melewati file yang tidak valid\n",
    "\n",
    "    filepath = os.path.join(DATA_PATH, filename)  # Menentukan jalur lengkap untuk file\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:  # Membuka file dalam mode baca biner\n",
    "            document = f.read()  # Membaca isi file\n",
    "            result = docs.process_document(document, filename)  # Memproses dokumen menggunakan DocumentProcessor\n",
    "\n",
    "        if not result or len(result) < 4:  # Memeriksa apakah hasil pemrosesan valid\n",
    "            print(f\"[WARNING] Gagal memproses {filename}, melewati file ini.\")  # Menampilkan peringatan jika pemrosesan gagal\n",
    "            continue  # Melewati file yang gagal diproses\n",
    "\n",
    "        plain_text = clean_text(result[3])  # Mengambil dan membersihkan teks dari hasil pemrosesan\n",
    "        print(f\"[INFO] {filename} - Panjang teks sebelum pemrosesan: {len(plain_text)}\")  # Menampilkan panjang teks yang dibaca\n",
    "\n",
    "        # Gunakan agent untuk chunking secara bertahap\n",
    "        structured_text = \"\"  # Inisialisasi string untuk menyimpan teks terstruktur\n",
    "        start_idx = 0  # Inisialisasi indeks awal untuk chunking\n",
    "        chunk_count = 0  # Inisialisasi penghitung chunk\n",
    "\n",
    "        while start_idx < len(plain_text) and chunk_count < MAX_CHUNKS:  # Selama ada teks yang tersisa dan jumlah chunk belum mencapai maksimum\n",
    "            chunk_text = plain_text[start_idx:start_idx + CHUNK_SIZE]  # Mengambil chunk teks\n",
    "            response = agent.run(  # Menjalankan agent untuk memproses chunk\n",
    "                f\"Split the following text into meaningful segments ensuring logical separation:\\n{chunk_text}\",\n",
    "                max_tokens=8000  # Menentukan batas token untuk respons\n",
    "            )\n",
    "            \n",
    "            if isinstance(response, str):  # Jika respons adalah string\n",
    "                structured_text += clean_agent_output(response) + \"\\n\\n\"  # Membersihkan dan menambahkan respons ke teks terstruktur\n",
    "            elif isinstance(response, dict):  # Jika respons adalah dictionary\n",
    "                structured_text += clean_agent_output(response.get(\"text\", \"\")) + \"\\n\\n\"  # Mengambil teks dari dictionary\n",
    "            else:  # Jika respons tidak sesuai dengan tipe yang diharapkan\n",
    "                structured_text += clean_agent_output(getattr(response, \"content\", str(response))) + \"\\n\\n\"  # Mengambil konten respons\n",
    "\n",
    "            start_idx += CHUNK_SIZE  # Memperbarui indeks awal untuk chunk berikutnya\n",
    "            chunk_count += 1  # Meningkatkan penghitung chunk\n",
    "\n",
    "        structured_text = structured_text.strip()  # Menghapus spasi di awal dan akhir teks terstruktur\n",
    "        chunked_texts = structured_text.split(\"\\n\\n\")  # Memisahkan teks terstruktur menjadi chunk berdasarkan dua newline\n",
    "\n",
    "        # Gabungkan chunk yang terlalu pendek\n",
    "        optimized_chunks = []  # Inisialisasi daftar untuk menyimpan chunk yang dioptimalkan\n",
    "        temp_chunk = \"\"  # Inisialisasi string untuk menyimpan chunk sementara\n",
    "\n",
    "        for chunk in chunked_texts:  # Iterasi melalui setiap chunk\n",
    "            chunk = chunk.strip()  # Menghapus spasi di awal dan akhir chunk\n",
    "            if len(chunk) < MIN_CHUNK_SIZE:  # Jika chunk lebih kecil dari ukuran minimum\n",
    "                temp_chunk += \" \" + chunk  # Tambahkan chunk ke chunk sementara\n",
    "            else:  # Jika chunk memenuhi ukuran minimum\n",
    "                if temp_chunk:  # Jika ada chunk sementara yang tersimpan\n",
    "                    optimized_chunks.append(temp_chunk.strip())  # Tambahkan chunk sementara ke daftar chunk yang dioptimalkan\n",
    "                    temp_chunk = \"\"  # Reset chunk sementara\n",
    "                optimized_chunks.append(chunk)  # Tambahkan chunk yang valid ke daftar\n",
    "\n",
    "        if temp_chunk:  # Jika ada chunk sementara yang tersisa\n",
    "            optimized_chunks.append(temp_chunk.strip())  # Tambahkan chunk terakhir ke daftar\n",
    "\n",
    "        chunk_data = [{\"chunk_id\": i+1, \"text\": chunk.strip()}  # Membuat daftar chunk dengan ID dan teks\n",
    "                      for i, chunk in enumerate(optimized_chunks[:MAX_CHUNKS]) if chunk.strip()]  # Hanya menyertakan chunk yang tidak kosong\n",
    "\n",
    "        metadata = {  # Menyusun metadata untuk dokumen\n",
    "            \"filename\": filename,  # Menyimpan nama file\n",
    "            \"total_chunks\": len(chunk_data),  # Menyimpan jumlah total chunk\n",
    "            \"total_length\": len(plain_text)  # Menyimpan panjang total teks\n",
    "        }\n",
    "\n",
    "        extracted_docs.extend([  # Menambahkan dokumen yang diekstrak ke daftar\n",
    "            Document(page_content=chunk[\"text\"], metadata={\"chunk_id\": chunk[\"chunk_id\"], **metadata}) \n",
    "            for chunk in chunk_data\n",
    "        ])\n",
    "\n",
    "        # Simpan hasil chunking ke file\n",
    "        chunked_filepath = os.path.join(CHUNKED_DATA_PATH, f\"chunked_{filename}.txt\")  # Menentukan jalur untuk file chunked\n",
    "        with open(chunked_filepath, \"w\", encoding=\"utf-8\") as chunked_file:  # Membuka file untuk menulis hasil chunking\n",
    "            for chunk in chunk_data:  # Iterasi melalui setiap chunk\n",
    "                chunked_file.write(f\"Chunk {chunk['chunk_id']}:\\n\")  # Menulis ID chunk\n",
    "                chunked_file.write(f\"{chunk['text']}\\n\")  # Menulis teks chunk\n",
    "                chunked_file.write(\"\\n---\\n\\n\")  # Menambahkan pemisah antar chunk\n",
    "\n",
    "        metadata_filepath = os.path.join(METADATA_PATH, f\"metadata_{filename}.json\")  # Menentukan jalur untuk file metadata\n",
    "        with open(metadata_filepath, \"w\", encoding=\"utf-8\") as metadata_file:  # Membuka file untuk menulis metadata\n",
    "            json.dump(metadata, metadata_file, indent=4)  # Menyimpan metadata dalam format JSON\n",
    "\n",
    "        print(f\"[INFO] Total chunks generated for {filename}: {len(chunk_data)}\")  # Menampilkan informasi jumlah chunk yang dihasilkan\n",
    "\n",
    "    except Exception as e:  # Menangkap kesalahan yang mungkin terjadi selama pemrosesan\n",
    "        print(f\"[ERROR] Error processing {filename}: {e}\")  # Menampilkan pesan kesalahan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Fungsi menyimpan hasil pemrosesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan ke FAISS hanya jika ada dokumen yang diproses\n",
    "if extracted_docs:  # Jika ada dokumen yang berhasil diproses\n",
    "    vector_store = FAISS.from_documents(extracted_docs, embedding_model)  # Membuat penyimpanan vektor dari dokumen yang diekstrak\n",
    "    vector_store.save_local(INDEX_PATH)  # Menyimpan penyimpanan vektor secara lokal\n",
    "    print(\"[SUCCESS] Proses chunking selesai. Hasilnya disimpan dalam 'chunked_data' dan metadata di 'metadata'.\")  # Menampilkan pesan sukses\n",
    "else:  # Jika tidak ada dokumen yang berhasil diproses\n",
    "    print(\"[INFO] Tidak ada dokumen yang berhasil diproses.\")  # Menampilkan informasi bahwa tidak ada dokumen yang diproses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Keseluruhan Kode Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dokumen_pdf.pdf - Panjang teks sebelum pemrosesan: 3843\n",
      "[INFO] Total chunks generated for dokumen_pdf.pdf: 4\n",
      "[INFO] tko.pdf - Panjang teks sebelum pemrosesan: 8838\n",
      "[INFO] Total chunks generated for tko.pdf: 7\n",
      "[SUCCESS] Proses chunking selesai. Hasilnya disimpan dalam 'chunked_data' dan metadata di 'metadata'.\n"
     ]
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "import json  # Mengimpor modul json untuk pengelolaan data JSON\n",
    "import re  # Mengimpor modul re untuk ekspresi reguler\n",
    "from IPython.display import Markdown  # Mengimpor Markdown untuk menampilkan teks dalam format Markdown di Jupyter Notebook\n",
    "from phi.agent import Agent  # Mengimpor Agent untuk penggunaan model LLM\n",
    "from phi.model.ollama import Ollama  # Mengimpor Ollama untuk penggunaan model LLM\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Mengimpor model embedding dari Hugging Face\n",
    "from langchain.vectorstores import FAISS  # Mengimpor FAISS untuk pengelolaan vektor\n",
    "from langchain_core.documents import Document  # Mengimpor Document untuk pengelolaan dokumen\n",
    "from utils.document_processor import DocumentProcessor  # Mengimpor DocumentProcessor untuk pengelolaan dokumen\n",
    "\n",
    "# Inisialisasi path dan model\n",
    "DATA_PATH = \"./data\"  # Menentukan folder data\n",
    "INDEX_PATH = \"faiss_index\"  # Menentukan jalur untuk indeks FAISS\n",
    "CHUNKED_DATA_PATH = \"./output_chunks\"  # Menentukan folder untuk hasil chunking\n",
    "METADATA_PATH = \"./metadata\"  # Menentukan folder untuk metadata\n",
    "OLLAMA_MODEL = \"llama3.2\"  # Menentukan model LLM yang akan digunakan\n",
    "\n",
    "# Pastikan folder tersedia\n",
    "os.makedirs(CHUNKED_DATA_PATH, exist_ok=True)  # Membuat folder untuk hasil chunking jika belum ada\n",
    "os.makedirs(METADATA_PATH, exist_ok=True)  # Membuat folder untuk metadata jika belum ada\n",
    "\n",
    "# Inisialisasi model Ollama\n",
    "llm = Ollama(id=OLLAMA_MODEL)  # Menginisialisasi model LLM\n",
    "\n",
    "# Inisialisasi embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # Menginisialisasi model embedding\n",
    "\n",
    "# Inisialisasi document processor\n",
    "docs = DocumentProcessor()  # Menginisialisasi DocumentProcessor\n",
    "\n",
    "# Inisialisasi Agent dengan model Ollama\n",
    "agent = Agent(model=llm, show_tool_calls=True, markdown=True)  # Menginisialisasi Agent dengan model LLM\n",
    "\n",
    "# Parameter chunking\n",
    "CHUNK_SIZE = 1200  # Menentukan ukuran chunk\n",
    "MIN_CHUNK_SIZE = 500  # Menentukan ukuran chunk minimum\n",
    "MAX_CHUNKS = 30  # Menentukan jumlah chunk maksimum\n",
    "\n",
    "extracted_docs = []  # Inisialisasi daftar untuk menyimpan dokumen yang telah diproses\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" Membersihkan teks dari karakter kosong, whitespace berlebih, dan simbol aneh \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Mengganti spasi berlebih dengan satu spasi dan menghapus spasi di awal/akhir\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Mengganti karakter yang tidak dapat diwakili dengan spasi\n",
    "    return text  # Mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "def clean_agent_output(text):\n",
    "    \"\"\" Membersihkan output dari agent agar tidak mengandung pemisah yang tidak perlu \"\"\"\n",
    "    text = re.sub(r'\\n?###.*?\\n', '\\n', text)  # Menghapus judul markdown seperti ### Section\n",
    "    text = re.sub(r'\\n?\\*\\*\\*.*?\\n', '\\n', text)  # Menghapus pemisah ***\n",
    "    text = re.sub(r'\\n?-{3,}\\n?', '\\n', text)  # Menghapus garis pemisah ---\n",
    "    text = re.sub(r'(\\s*-{2,}\\s*)', ' ', text)  # Menghapus pemisah --\n",
    "    text = re.sub(r'(\\s*\\*{2,}\\s*)', ' ', text)  # Menghapus ** pemisah tebal\n",
    "    text = re.sub(r'(\\s*\\*\\s*)', ' ', text)  # Menghapus * pemisah tunggal\n",
    "    text = re.sub(r'(\\s*-\\s*)', ' ', text)  # Menghapus pemisah -\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text).strip()  # Mengganti newline berlebih dengan dua newline\n",
    "    return text  # Mengembalikan teks yang telah dibersihkan\n",
    "\n",
    "for filename in os.listdir(DATA_PATH):  # Iterasi melalui setiap file dalam folder data\n",
    "    valid_extensions = ('.pdf', '.docx', '.txt')  # Menentukan ekstensi file yang valid\n",
    "    if not filename.lower().endswith(valid_extensions):  # Jika file tidak memiliki ek stensi yang valid, lanjutkan ke file berikutnya\n",
    "        continue  # Melewati file yang tidak valid\n",
    "\n",
    "    filepath = os.path.join(DATA_PATH, filename)  # Menentukan jalur lengkap untuk file\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:  # Membuka file dalam mode baca biner\n",
    "            document = f.read()  # Membaca isi file\n",
    "            result = docs.process_document(document, filename)  # Memproses dokumen menggunakan DocumentProcessor\n",
    "\n",
    "        if not result or len(result) < 4:  # Memeriksa apakah hasil pemrosesan valid\n",
    "            print(f\"[WARNING] Gagal memproses {filename}, melewati file ini.\")  # Menampilkan peringatan jika pemrosesan gagal\n",
    "            continue  # Melewati file yang gagal diproses\n",
    "\n",
    "        plain_text = clean_text(result[3])  # Mengambil dan membersihkan teks dari hasil pemrosesan\n",
    "        print(f\"[INFO] {filename} - Panjang teks sebelum pemrosesan: {len(plain_text)}\")  # Menampilkan panjang teks yang dibaca\n",
    "\n",
    "        # Gunakan agent untuk chunking secara bertahap\n",
    "        structured_text = \"\"  # Inisialisasi string untuk menyimpan teks terstruktur\n",
    "        start_idx = 0  # Inisialisasi indeks awal untuk chunking\n",
    "        chunk_count = 0  # Inisialisasi penghitung chunk\n",
    "\n",
    "        while start_idx < len(plain_text) and chunk_count < MAX_CHUNKS:  # Selama ada teks yang tersisa dan jumlah chunk belum mencapai maksimum\n",
    "            chunk_text = plain_text[start_idx:start_idx + CHUNK_SIZE]  # Mengambil chunk teks\n",
    "            response = agent.run(  # Menjalankan agent untuk memproses chunk\n",
    "                f\"Split the following text into meaningful segments ensuring logical separation:\\n{chunk_text}\",\n",
    "                max_tokens=8000  # Menentukan batas token untuk respons\n",
    "            )\n",
    "            \n",
    "            if isinstance(response, str):  # Jika respons adalah string\n",
    "                structured_text += clean_agent_output(response) + \"\\n\\n\"  # Membersihkan dan menambahkan respons ke teks terstruktur\n",
    "            elif isinstance(response, dict):  # Jika respons adalah dictionary\n",
    "                structured_text += clean_agent_output(response.get(\"text\", \"\")) + \"\\n\\n\"  # Mengambil teks dari dictionary\n",
    "            else:  # Jika respons tidak sesuai dengan tipe yang diharapkan\n",
    "                structured_text += clean_agent_output(getattr(response, \"content\", str(response))) + \"\\n\\n\"  # Mengambil konten respons\n",
    "\n",
    "            start_idx += CHUNK_SIZE  # Memperbarui indeks awal untuk chunk berikutnya\n",
    "            chunk_count += 1  # Meningkatkan penghitung chunk\n",
    "\n",
    "        structured_text = structured_text.strip()  # Menghapus spasi di awal dan akhir teks terstruktur\n",
    "        chunked_texts = structured_text.split(\"\\n\\n\")  # Memisahkan teks terstruktur menjadi chunk berdasarkan dua newline\n",
    "\n",
    "        # Gabungkan chunk yang terlalu pendek\n",
    "        optimized_chunks = []  # Inisialisasi daftar untuk menyimpan chunk yang dioptimalkan\n",
    "        temp_chunk = \"\"  # Inisialisasi string untuk menyimpan chunk sementara\n",
    "\n",
    "        for chunk in chunked_texts:  # Iterasi melalui setiap chunk\n",
    "            chunk = chunk.strip()  # Menghapus spasi di awal dan akhir chunk\n",
    "            if len(chunk) < MIN_CHUNK_SIZE:  # Jika chunk lebih kecil dari ukuran minimum\n",
    "                temp_chunk += \" \" + chunk  # Tambahkan chunk ke chunk sementara\n",
    "            else:  # Jika chunk memenuhi ukuran minimum\n",
    "                if temp_chunk:  # Jika ada chunk sementara yang tersimpan\n",
    "                    optimized_chunks.append(temp_chunk.strip())  # Tambahkan chunk sementara ke daftar chunk yang dioptimalkan\n",
    "                    temp_chunk = \"\"  # Reset chunk sementara\n",
    "                optimized_chunks.append(chunk)  # Tambahkan chunk yang valid ke daftar\n",
    "\n",
    "        if temp_chunk:  # Jika ada chunk sementara yang tersisa\n",
    "            optimized_chunks.append(temp_chunk.strip())  # Tambahkan chunk terakhir ke daftar\n",
    "\n",
    "        chunk_data = [{\"chunk_id\": i+1, \"text\": chunk.strip()}  # Membuat daftar chunk dengan ID dan teks\n",
    "                      for i, chunk in enumerate(optimized_chunks[:MAX_CHUNKS]) if chunk.strip()]  # Hanya menyertakan chunk yang tidak kosong\n",
    "\n",
    "        metadata = {  # Menyusun metadata untuk dokumen\n",
    "            \"filename\": filename,  # Menyimpan nama file\n",
    "            \"total_chunks\": len(chunk_data),  # Menyimpan jumlah total chunk\n",
    "            \"total_length\": len(plain_text)  # Menyimpan panjang total teks\n",
    "        }\n",
    "\n",
    "        extracted_docs.extend([  # Menambahkan dokumen yang diekstrak ke daftar\n",
    "            Document(page_content=chunk[\"text\"], metadata={\"chunk_id\": chunk[\"chunk_id\"], **metadata}) \n",
    "            for chunk in chunk_data\n",
    "        ])\n",
    "\n",
    "        # Simpan hasil chunking ke file\n",
    "        chunked_filepath = os.path.join(CHUNKED_DATA_PATH, f\"chunked_{filename}.txt\")  # Menentukan jalur untuk file chunked\n",
    "        with open(chunked_filepath, \"w\", encoding=\"utf-8\") as chunked_file:  # Membuka file untuk menulis hasil chunking\n",
    "            for chunk in chunk_data:  # Iterasi melalui setiap chunk\n",
    "                chunked_file.write(f\"Chunk {chunk['chunk_id']}:\\n\")  # Menulis ID chunk\n",
    "                chunked_file.write(f\"{chunk['text']}\\n\")  # Menulis teks chunk\n",
    "                chunked_file.write(\"\\n---\\n\\n\")  # Menambahkan pemisah antar chunk\n",
    "\n",
    "        metadata_filepath = os.path.join(METADATA_PATH, f\"metadata_{filename}.json\")  # Menentukan jalur untuk file metadata\n",
    "        with open(metadata_filepath, \"w\", encoding=\"utf-8\") as metadata_file:  # Membuka file untuk menulis metadata\n",
    "            json.dump(metadata, metadata_file, indent=4)  # Menyimpan metadata dalam format JSON\n",
    "\n",
    "        print(f\"[INFO] Total chunks generated for {filename}: {len(chunk_data)}\")  # Menampilkan informasi jumlah chunk yang dihasilkan\n",
    "\n",
    "    except Exception as e:  # Menangkap kesalahan yang mungkin terjadi selama pemrosesan\n",
    "        print(f\"[ERROR] Error processing {filename}: {e}\")  # Menampilkan pesan kesalahan\n",
    "\n",
    "# Simpan ke FAISS hanya jika ada dokumen yang diproses\n",
    "if extracted_docs:  # Jika ada dokumen yang berhasil diproses\n",
    "    vector_store = FAISS.from_documents(extracted_docs, embedding_model)  # Membuat penyimpanan vektor dari dokumen yang diekstrak\n",
    "    vector_store.save_local(INDEX_PATH)  # Menyimpan penyimpanan vektor secara lokal\n",
    "    print(\"[SUCCESS] Proses chunking selesai. Hasilnya disimpan dalam 'chunked_data' dan metadata di 'metadata'.\")  # Menampilkan pesan sukses\n",
    "else:  # Jika tidak ada dokumen yang berhasil diproses\n",
    "    print(\"[INFO] Tidak ada dokumen yang berhasil diproses.\")  # Menampilkan informasi bahwa tidak ada dokumen yang diproses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PENGETESAN DENGAN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "from IPython.display import display, Markdown, HTML  # Mengimpor fungsi untuk menampilkan output di Jupyter Notebook\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Mengimpor ChatPromptTemplate untuk membuat template prompt\n",
    "from langchain_ollama.llms import OllamaLLM  # Mengimpor OllamaLLM untuk menggunakan model LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fungsi read_chunks_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks_ordered(path):\n",
    "    \"\"\"Membaca file chunks dengan urutan yang benar\"\"\"\n",
    "    files = []  # Inisialisasi daftar untuk menyimpan nama file\n",
    "    for file in os.listdir(path):  # Iterasi melalui setiap file dalam folder\n",
    "        if file.endswith('.txt') and file.startswith('chunk_'):  # Memeriksa apakah file adalah chunk\n",
    "            files.append(file)  # Menambahkan file ke daftar jika memenuhi syarat\n",
    "    \n",
    "    # Sort files naturally (chunk_1.txt, chunk_2.txt, etc.)\n",
    "    files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))  # Mengurutkan file berdasarkan nomor chunk\n",
    "\n",
    "    all_text = []  # Inisialisasi daftar untuk menyimpan konten dari semua chunk\n",
    "    for file in files:  # Iterasi melalui setiap file yang telah diurutkan\n",
    "        file_path = os.path.join(path, file)  # Menentukan jalur lengkap untuk file\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:  # Membuka file dalam mode baca\n",
    "                content = f.read()  # Membaca isi file\n",
    "                all_text.append(content)  # Menambahkan konten ke daftar\n",
    "                display(HTML(f\"✅ Read {file}: {len(content)} characters\"))  # Menampilkan pesan sukses\n",
    "        except Exception as e:\n",
    "            display(HTML(f\"❌ Error reading {file}: {str(e)}\"))  # Menampilkan pesan kesalahan jika terjadi error\n",
    "    \n",
    "    return \"\\n\\n\".join(all_text)  # Menggabungkan semua isi file dengan pemisah newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fungsi process_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(chunk_folder):\n",
    "    \"\"\"Fungsi utama dengan parameter folder\"\"\"\n",
    "    try:\n",
    "        # Baca dokumen\n",
    "        display(Markdown(\"### 1. Membaca dokumen...\"))  # Menampilkan pesan bahwa dokumen sedang dibaca\n",
    "        if not os.path.exists(chunk_folder):  # Memeriksa apakah folder ada\n",
    "            raise FileNotFoundError(f\"Folder tidak ditemukan: {chunk_folder}\")  # Mengangkat kesalahan jika folder tidak ditemukan\n",
    "        \n",
    "        document_text = read_chunks_ordered(chunk_folder)  # Membaca semua chunk dari folder\n",
    "        \n",
    "        # Initialize model dengan temperature 0 untuk hasil konsisten\n",
    "        display(Markdown(\"### 2. Inisialisasi model...\"))  # Menampilkan pesan bahwa model sedang diinisialisasi\n",
    "        model = OllamaLLM(  # Menginisialisasi model LLM\n",
    "            model=\"llama3.2\",  # Menentukan model yang digunakan\n",
    "            temperature=0.0,  # Set ke 0 untuk hasil yang konsisten\n",
    "            num_ctx=4096      # Konteks yang lebih besar\n",
    "        )\n",
    "        \n",
    "        # Prompt yang lebih spesifik\n",
    "        display(Markdown(\"### 3. Memproses dokumen...\"))  # Menampilkan pesan bahwa dokumen sedang diproses\n",
    "        template = \"\"\"\n",
    "        Anda adalah asisten yang SANGAT TELITI. Baca dokumen berikut dengan seksama dan jawab pertanyaan-pertanyaan berdasarkan HANYA informasi yang ada dalam dokumen.\n",
    "        \n",
    "        ATURAN PENTING:\n",
    "        1. Gunakan HANYA informasi dari dokumen\n",
    "        2. Jika informasi tidak ada, tulis \"Informasi tidak ditemukan dalam dokumen\"\n",
    "        3. Jika informasi ditemukan, KUTIP LANGSUNG dari dokumen\n",
    "        4. JANGAN menambahkan interpretasi atau asumsi\n",
    "        \n",
    "        DOKUMEN:\n",
    "        {document}\n",
    "\n",
    "        PERTANYAAN DAN JAWABAN:\n",
    "\n",
    "        **1. Apa tujuan dari tata kerja organisasi?**\n",
    "        [Kutip PERSIS dari dokumen atau tulis \"Informasi tidak ditemukan dalam dokumen\"]\n",
    "\n",
    "        **2. Apa pengertian Onsite Support dan Aplikasi Upstream?**\n",
    "        [Kutip PERSIS dari dokumen atau tulis \"Informasi tidak ditemukan dalam dokumen\"]\n",
    "\n",
    "        **3. Apa dokumen dan referensi terkait Tata Kerja Organisasi?**\n",
    "        [Kutip PERSIS dari dokumen atau tulis \"Informasi tidak ditemukan dalam dokumen\"]\n",
    "\n",
    "        **4. Bagaimana prosedur permintaan baru akun aplikasi?**\n",
    "        [Kutip PERSIS dari dokumen atau tulis \"Informasi tidak ditemukan dalam dokumen\"]\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(template)  # Membuat template prompt dari string\n",
    "        chain = prompt | model  # Menggabungkan prompt dengan model LLM\n",
    "        \n",
    "        # Proses dokumen\n",
    "        response = chain.invoke({\"document\": document_text})  # Mengirimkan teks dokumen ke model untuk diproses\n",
    "        \n",
    "        # Tampilkan hasil\n",
    "        display(Markdown(\"### 4. Hasil:\"))  # Menampilkan judul untuk hasil\n",
    "        display(Markdown(response))  # Menampilkan hasil dari model\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(HTML(f\"<b style='color: red;'>Error:</b> {str(e)}\"))  # Menampilkan pesan kesalahan jika terjadi error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ekseskusi Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"output_chunks/overlap\"  # Menentukan jalur folder yang berisi chunk\n",
    "process_document(folder_path)  # Memanggil fungsi untuk memproses dokumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Keseluruhan Kode Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 1. Membaca dokumen..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_001.txt: 940 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_002.txt: 891 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_003.txt: 1089 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_004.txt: 1444 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_005.txt: 927 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_006.txt: 1308 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_007.txt: 960 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_008.txt: 1138 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_009.txt: 1420 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "✅ Read chunk_010.txt: 754 characters"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 2. Inisialisasi model..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 3. Memproses dokumen..."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### 4. Hasil:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Berikut adalah jawaban untuk pertanyaan-pertanyaan tersebut:\n",
       "\n",
       "**1. Apa tujuan dari tata kerja organisasi?**\n",
       "\n",
       "Tujuan dari Tata Kerja Organisasi (TKO) ini adalah untuk menjelaskan proses pengelolaan akun-akun yang memerlukan akses ke aplikasi Non-ERP yang meliputi aplikasi Non-ERP bisnis, aplikasi upstream dan infrastruktur pendukung lainnya di lingkungan PT. Pertamina Hulu Rokan – Zona Rokan.\n",
       "\n",
       "**2. Apa pengertian Onsite Support dan Aplikasi Upstream?**\n",
       "\n",
       "Pengertian dari Onsite Support adalah personel yang ditunjuk untuk mengontrol akses aplikasi dan/atau data dari suatu aplikasi vendor. Pengertian dari Aplikasi Upstream adalah perangkat lunak yang digunakan oleh Perusahaan untuk mendukung proses geology, geophysics, reservoir, production, facility, drilling, engineering (GGRPFDE).\n",
       "\n",
       "**3. Apa dokumen dan referensi terkait Tata Kerja Organisasi?**\n",
       "\n",
       "Dokumen dan referensi terkait Tata Kerja Organisasi adalah Undang-Undang Republik Indonesia No. 11 Tahun 2008 tentang Informasi dan Transaksi Elektronik, Pedoman PT Pertamina Hulu Energi No. A10-001/PHE53000/2021-S9 Rev. 0 tentang Penyelenggaraan Tata Kelola Teknologi Informasi Perusahaan, Pedoman PT Pertamina Hulu Energi No. A10-002/PHE53000/2021-S9 Rev. 0 tentang Pedoman Perencanaan Teknologi Informasi, dan Pedoman SKK Migas No. KEP-00008./SKO0000/2013/S0 tentang Tata Kerja Pengelolaan Teknologi Informasi dan Komunikasi Pada kontraktor Kontrak Kerja Sama.\n",
       "\n",
       "**4. Bagaimana prosedur permintaan baru akun aplikasi?**\n",
       "\n",
       "Prosedur permintaan baru akun aplikasi melalui proses User Access Review adalah sebagai berikut:\n",
       "\n",
       "a. Permintaan Baru Akun Aplikasi\n",
       "- Application Programmer membuat daftar akun yang berisikan kepemilikan akun dan hak akses yang sudah terdaftar untuk masing-masing aplikasi.\n",
       "- Technical Writer mengirimkan daftar akun tersebut ke Information Steward melalui email dengan format email \"User Access Review\" berupa peninjauan daftar akun untuk akun yang perlu diperbarui dari aplikasi.\n",
       "\n",
       "b. Peninjauan Daftar Akun\n",
       "- Information Steward meninjau daftar akun aplikasi yang diterima kemudian mengirimkan kembali ke Technical Writer email UAR hasil tinjauan berupa daftar akun baru yang perlu ditambahkan akses ke aplikasi.\n",
       "\n",
       "c. Persetujuan Daftar Hasil Peninjauan Akun\n",
       "- Technical Writer menginfokan hasil peninjauan akun aplikasi yang sudah selesai dilakukan oleh Information Steward ke Business Analyst, untuk kemudian meminta persetujuan dari daftar hasil peninjauan akun yang telah selesai ditinjau oleh Information Steward.\n",
       "\n",
       "d. Penyetujuan Daftar Hasil Peninjauan Akun\n",
       "- Business Analyst menyetujui daftar hasil peninjauan akun dan meminta Technical Writer untuk membuat tiket terkait akun-akun yang perlu ditambahkan aksesnya ke aplikasi.\n",
       "\n",
       "e. Pengecekan Terhadap Ketersediaan Lisensi\n",
       "- Technical Writer membuat tiket untuk pengecekan terhadap ketersediaan lisensi kepada Onsite Support.\n",
       "- Onsite Support melakukan pengecekan terhadap ketersediaan lisensi dan memberitahu ke Technical Writer jika lisensi tidak tersedia.\n",
       "\n",
       "f. Penambahan Akses Akun Baru\n",
       "- Jika aplikasi tersebut memerlukan akses database, maka lanjut ke proses 1j.\n",
       "- Jika aplikasi tersebut tidak memerlukan akses database, maka lanjut ke proses 1k.\n",
       "- IT IO – Database Administrator melakukan penambahan akun baru pada database aplikasi yang diminta dan memberitahu penambahan akses akun baru ke Technical Writer.\n",
       "\n",
       "g. Penambahan Akses Akun Baru\n",
       "- Application Programmer memberikan akses ke aplikasi yang diminta dan memberitahu penambahan akses akun baru ke Technical Writer.\n",
       "\n",
       "h. Penutupan Tiket Permintaan Penambahan Akun Baru\n",
       "- Technical Writer menutup tiket permintaan penambahan akun baru setelah mendapat konfirmasi dari masing-masing pemilik Akun (Pengguna)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os  # Mengimpor modul os untuk operasi sistem file\n",
    "from IPython.display import display, Markdown, HTML  # Mengimpor fungsi untuk menampilkan output di Jupyter Notebook\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Mengimpor ChatPromptTemplate untuk membuat template prompt\n",
    "from langchain_ollama.llms import OllamaLLM  # Mengimpor OllamaLLM untuk menggunakan model LLM\n",
    "\n",
    "def read_chunks_ordered(path):\n",
    "    \"\"\"Membaca file chunks dengan urutan yang benar\"\"\"\n",
    "    files = []  # Inisialisasi daftar untuk menyimpan nama file\n",
    "    for file in os.listdir(path):  # Iterasi melalui setiap file dalam folder\n",
    "        if file.endswith('.txt') and file.startswith('chunk_'):  # Memeriksa apakah file adalah chunk\n",
    "            files.append(file)  # Menambahkan file ke daftar jika memenuhi syarat\n",
    "    \n",
    "    # Sort files naturally (chunk_1.txt, chunk_2.txt, etc.)\n",
    "    files.sort(key=lambda x: int(x.split('_')[1].split('.')[0]))  # Mengurutkan file berdasarkan nomor chunk\n",
    "\n",
    "    all_text = []  # Inisialisasi daftar untuk menyimpan konten dari semua chunk\n",
    "    for file in files:  # Iterasi melalui setiap file yang telah diurutkan\n",
    "        file_path = os.path.join(path, file)  # Menentukan jalur lengkap untuk file\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:  # Membuka file dalam mode baca\n",
    "                content = f.read()  # Membaca isi file\n",
    "                all_text.append(content)  # Menambahkan konten ke daftar\n",
    "                display(HTML(f\"✅ Read {file}: {len(content)} characters\"))  # Menampilkan pesan sukses\n",
    "        except Exception as e:\n",
    "            display(HTML(f\"❌ Error reading {file}: {str(e)}\"))  # Menampilkan pesan kesalahan jika terjadi error\n",
    "    \n",
    "    return \"\\n\\n\".join(all_text)  # Menggabungkan semua isi file dengan pemisah newline\n",
    "\n",
    "def process_document(chunk_folder):\n",
    "    \"\"\"Fungsi utama dengan parameter folder\"\"\"\n",
    "    try:\n",
    "        # Baca dokumen\n",
    "        display(Markdown(\"### 1. Membaca dokumen...\"))  # Menampilkan pesan bahwa dokumen sedang dibaca\n",
    "        if not os.path.exists(chunk_folder):  # Memeriksa apakah folder ada\n",
    "            raise FileNotFoundError(f\"Folder tidak ditemukan: {chunk_folder}\")  # Mengangkat kesalahan jika folder tidak ditemukan\n",
    "        \n",
    "        document_text = read_chunks_ordered(chunk_folder)  # Membaca semua chunk dari folder\n",
    "        \n",
    "        # Initialize model dengan temperature 0 untuk hasil konsisten\n",
    "        display(Markdown(\"### 2. Inisialisasi model...\"))  # Menampilkan pesan bahwa model sedang diinisialisasi\n",
    "        model = OllamaLLM(  # Menginisialisasi model LLM\n",
    "            model=\"llama3.2\",  # Menentukan model yang digunakan\n",
    "            temperature=0.0,  # Set ke 0 untuk hasil yang konsisten\n",
    "            num_ctx=4096      # Konteks yang lebih besar\n",
    "        )\n",
    "        \n",
    "        # Prompt yang lebih spesifik\n",
    "        display(Markdown(\"### 3. Memproses dokumen...\"))  # Menampilkan pesan bahwa dokumen sedang diproses\n",
    "        template = \"\"\"\n",
    "        Anda adalah asisten yang SANGAT TELITI. Baca dokumen berikut dengan seksama dan jawab pertanyaan-pertanyaan berdasarkan HANYA informasi yang ada dalam dokumen.\n",
    "        \n",
    "        ATURAN PENTING:\n",
    "        1. Gunakan HANYA informasi dari dokumen\n",
    "        2. Jika informasi tidak ada, tulis \"Informasi tidak ditemukan dalam dokumen\"\n",
    "        3. Jika informasi ditemukan, KUTIP LANGSUNG dari dokumen\n",
    "        4. JANGAN menambahkan interpretasi atau asumsi\n",
    "        \n",
    "        DOKUMEN:\n",
    "        {document}\n",
    "\n",
    "        PERTANYAAN DAN JAWABAN:\n",
    "\n",
    "        **1. Apa tujuan dari tata kerja organisasi?**\n",
    "        [Kutip PERSIS dari dokumen atau tulis \"Informasi tidak ditemukan dalam dokumen\"]\n",
    "\n",
    "        **2. Apa pengertian Onsite Support dan Aplikasi Upstream?**\n",
    "        [Kutip PERSIS dari dokumen atau tulis \"Informasi tidak ditemukan dalam dokumen\"]\n",
    "\n",
    "        **3. Apa dokumen dan referensi terkait Tata Kerja Organisasi?**\n",
    "        [Kutip PERSIS dari dokumen atau tulis \"Informasi tidak ditemukan dalam dokumen\"]\n",
    "\n",
    "        **4. Bagaimana prosedur permintaan baru akun aplikasi?**\n",
    "        [Kutip PERSIS dari dokumen atau tulis \"Informasi tidak ditemukan dalam dokumen\"]\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_template(template)  # Membuat template prompt dari string\n",
    "        chain = prompt | model  # Menggabungkan prompt dengan model LLM\n",
    "        \n",
    "        # Proses dokumen\n",
    "        response = chain.invoke({\"document\": document_text})  # Mengirimkan teks dokumen ke model untuk diproses\n",
    "        \n",
    "        # Tampilkan hasil\n",
    "        display(Markdown(\"### 4. Hasil:\"))  # Menampilkan judul untuk hasil\n",
    "        display(Markdown(response))  # Menampilkan hasil dari model\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(HTML(f\"<b style='color: red;'>Error:</b> {str(e)}\"))  # Menampilkan pesan kesalahan jika terjadi error\n",
    "\n",
    "# Jalankan dengan folder yang spesifik\n",
    "folder_path = \"output_chunks/overlap\"  # Menentukan jalur folder yang berisi chunk\n",
    "process_document(folder_path)  # Memanggil fungsi untuk memproses dokumen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
