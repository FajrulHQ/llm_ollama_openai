{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESS DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from utils.document_processor import DocumentProcessor\n",
    "from chromadb import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"./data\"\n",
    "VALID_EXTENSIONS = ('.pdf', '.docx', '.txt')\n",
    "\n",
    "docs = DocumentProcessor()\n",
    "chroma_client = Client()\n",
    "collection = chroma_client.get_or_create_collection(name=\"document_vectors\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def load_documents():\n",
    "    for filename in filter(lambda f: f.lower().endswith(VALID_EXTENSIONS), os.listdir(DATA_PATH)):\n",
    "        filepath = os.path.join(DATA_PATH, filename)\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            yield docs.process_document(f.read(), filename)\n",
    "\n",
    "def split_documents(extracted_docs, chunk_size=500):\n",
    "    for doc_index, (content, _, _, _) in enumerate(extracted_docs):\n",
    "        for page_number, page_content in enumerate(content or []):\n",
    "            for i in range(0, len(page_content), chunk_size):\n",
    "                chunk = page_content[i:i+chunk_size].strip()\n",
    "                if chunk:\n",
    "                    yield f\"doc{doc_index}_page{page_number}_chunk{i//chunk_size}\", chunk, doc_index, page_number\n",
    "\n",
    "def add_to_chroma(chunks):\n",
    "    existing_ids = set(collection.get(include=[]).get(\"ids\", []))\n",
    "    new_chunks = [(cid, embedding_model.encode([chunk])[0].tolist(), doc_idx, page)\n",
    "                  for cid, chunk, doc_idx, page in chunks if cid not in existing_ids]\n",
    "\n",
    "    if new_chunks:\n",
    "        ids, embeddings, metadata = zip(*[(cid, emb, {\"doc_index\": doc_idx, \"page\": page})\n",
    "                                           for cid, emb, doc_idx, page in new_chunks])\n",
    "        collection.add(ids=list(ids), embeddings=list(embeddings), metadatas=list(metadata))\n",
    "\n",
    "def main(reset=False):\n",
    "    if reset and os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    add_to_chroma(split_documents(load_documents()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLLAMA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "COLLECTION_NAME = \"ollama_vectore_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST OLLAMA MODEL WITH GRAPHRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self, documents, model, prompt_template):\n",
    "        self.documents = documents\n",
    "        self.model = model\n",
    "        self.prompt_template = prompt_template\n",
    "        self.graph = nx.Graph()\n",
    "        \n",
    "    def create_document_embeddings(self):\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        doc_texts = [\n",
    "            \"\\n\".join(map(str, doc[0])) if isinstance(doc[0], list) else str(doc[0])\n",
    "            for doc in self.documents\n",
    "        ]\n",
    "        embeddings = vectorizer.fit_transform(doc_texts)\n",
    "        return embeddings\n",
    "    \n",
    "    def build_document_graph(self, embeddings, similarity_threshold=0.2):\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        for i in range(len(self.documents)):\n",
    "            self.graph.add_node(i, text=self.documents[i])\n",
    "            \n",
    "        for i in range(len(self.documents)):\n",
    "            for j in range(i+1, len(self.documents)):\n",
    "                if similarity_matrix[i, j] > similarity_threshold:\n",
    "                    self.graph.add_edge(i, j, weight=similarity_matrix[i, j])\n",
    "    \n",
    "    def summarize_documents(self):\n",
    "        chain = self.prompt_template | self.model\n",
    "        \n",
    "        for doc_index, doc_data in enumerate(self.documents):\n",
    "            if not doc_data or len(doc_data) < 4:\n",
    "                continue\n",
    "            \n",
    "            content, _, _, _ = doc_data\n",
    "            full_text = \"\\n\".join(\n",
    "                \" \".join(map(str, page)) if isinstance(page, list) else str(page)\n",
    "                for page in content\n",
    "            ) if isinstance(content, list) else str(content)\n",
    "            \n",
    "            if not full_text.strip():\n",
    "                continue\n",
    "            \n",
    "            summary = chain.invoke({\"document\": full_text})\n",
    "            \n",
    "            self.graph.nodes[doc_index]['summary'] = summary \n",
    "    \n",
    "    def rank_documents(self):\n",
    "        pagerank = nx.pagerank(self.graph)\n",
    "        ranked_docs = sorted(\n",
    "            [(idx, score) for idx, score in pagerank.items()], \n",
    "            key=lambda x: x[1], \n",
    "            reverse=True\n",
    "        )\n",
    "        return ranked_docs\n",
    "    \n",
    "    def generate_insights(self):\n",
    "        insights = []\n",
    "        ranked_docs = self.rank_documents()\n",
    "        \n",
    "        for idx, rank_score in ranked_docs[:1]: \n",
    "            node_data = self.graph.nodes[idx]\n",
    "            summary = node_data.get('summary', '')\n",
    "            \n",
    "            insight_prompt = f\"\"\"\n",
    "            Based on the summary of highly rated documents, \n",
    "            provide important insights\n",
    "            \n",
    "            Summary: {summary}\n",
    "            \"\"\"\n",
    "            \n",
    "            insight = self.model.invoke(insight_prompt)\n",
    "            insights.append(insight)\n",
    "        \n",
    "        return insights\n",
    "\n",
    "template = \"\"\"\n",
    "You are a helpful assistant for text summarization. \n",
    "Only include information that is part of the document. \n",
    "Do not include your own opinion or analysis.\n",
    "\n",
    "Document: \n",
    "\"{document}\"\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model=OLLAMA_MODEL)\n",
    "\n",
    "documents = list(load_documents())\n",
    "\n",
    "graph_rag = GraphRAG(documents, model, prompt)\n",
    "\n",
    "embeddings = graph_rag.create_document_embeddings()\n",
    "graph_rag.build_document_graph(embeddings)\n",
    "\n",
    "graph_rag.summarize_documents() \n",
    "\n",
    "insights = graph_rag.generate_insights()\n",
    "for insight in insights:\n",
    "    display(Markdown(insight))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
