{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECURSIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengekstrak teks dari PDF...\n",
      "Membagi teks menjadi chunks secara rekursif...\n",
      "Selesai membagi menjadi 11 chunks.\n",
      "Semua chunks telah disimpan dalam folder: ./output_chunks/recursive\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Membersihkan dan merapikan teks hasil ekstraksi.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"\n",
    "    Load dan ekstrak teks dari PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = \"\"\n",
    "        \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            page_text = page.get_text(\"text\")\n",
    "            page_text = clean_text(page_text)\n",
    "            full_text += page_text + \" \"\n",
    "        \n",
    "        return clean_text(full_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def recursive_chunk(text, chunk_size, separators=None):\n",
    "    \"\"\"\n",
    "    Membagi teks secara rekursif menjadi chunk dengan panjang maksimum menggunakan separator yang ditentukan.\n",
    "    \"\"\"\n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    if separators is None:\n",
    "        separators = [\".\"]\n",
    "\n",
    "    for separator in separators:\n",
    "        if separator in text[:chunk_size]:\n",
    "            split_index = text[:chunk_size].rfind(separator) + len(separator)\n",
    "            chunk = text[:split_index]\n",
    "            remaining_text = text[split_index:].strip()\n",
    "            return [chunk] + recursive_chunk(remaining_text, chunk_size, separators)\n",
    "\n",
    "    chunk = text[:chunk_size]\n",
    "    remaining_text = text[chunk_size:].strip()\n",
    "    return [chunk] + recursive_chunk(remaining_text, chunk_size, separators)\n",
    "\n",
    "def save_chunks_to_file(chunks, output_file):\n",
    "    \"\"\"\n",
    "    Menyimpan semua chunk ke dalam satu file.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, chunk in enumerate(chunks, 1):\n",
    "            f.write(f\"Chunk {idx:03d}:\\n\")\n",
    "            f.write(chunk)\n",
    "            f.write(\"\\n\\n\")\n",
    "    \n",
    "    print(f\"Semua chunks telah disimpan ke dalam file: {output_file}\")\n",
    "\n",
    "def main():\n",
    "    # Konfigurasi\n",
    "    pdf_path = \"./data/tko.pdf\"\n",
    "    output_file = \"./output_chunks/output_recursive.txt\"  # File untuk menyimpan hasil chunking\n",
    "    chunk_size = 1000\n",
    "\n",
    "    # Load PDF dan ekstrak teks\n",
    "    print(\"Mengekstrak teks dari PDF...\")\n",
    "    full_text = load_pdf_text(pdf_path)\n",
    "\n",
    "    if not full_text:\n",
    "        print(\"Tidak ada teks yang bisa diekstrak. Program dihentikan.\")\n",
    "        return\n",
    "\n",
    "    # Chunk teks secara rekursif\n",
    "    print(\"Membagi teks menjadi chunks secara rekursif...\")\n",
    "    chunks = recursive_chunk(full_text, chunk_size)\n",
    "\n",
    "    print(f\"Selesai membagi menjadi {len(chunks)} chunks.\")\n",
    "\n",
    "    # Simpan semua chunk ke satu file\n",
    "    save_chunks_to_file(chunks, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunk 1 saved: chunk_001.txt\n",
      "âœ… Chunk 2 saved: chunk_002.txt\n",
      "âœ… Chunk 3 saved: chunk_003.txt\n",
      "âœ… Chunk 4 saved: chunk_004.txt\n",
      "âœ… Chunk 5 saved: chunk_005.txt\n",
      "âœ… Chunk 6 saved: chunk_006.txt\n",
      "âœ… Chunk 7 saved: chunk_007.txt\n",
      "âœ… Chunk 8 saved: chunk_008.txt\n",
      "âœ… Chunk 9 saved: chunk_009.txt\n",
      "âœ… Chunk 10 saved: chunk_010.txt\n",
      "\n",
      "âœ… 10 chunks successfully processed and saved in './output_chunks/overlap'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz\n",
    "import nltk\n",
    "import re\n",
    "import textwrap\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class PDFChunker:\n",
    "    def __init__(self, chunk_size=1000, chunk_overlap=200, line_width=80):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.line_width = line_width\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract text from PDF file with better formatting.\"\"\"\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            text_blocks = []\n",
    "            \n",
    "            for page in doc:\n",
    "                blocks = page.get_text(\"blocks\")\n",
    "                for block in blocks:\n",
    "                    clean_text = block[4].strip()\n",
    "                    if clean_text:\n",
    "                        text_blocks.append(clean_text)\n",
    "            \n",
    "            text = \" \".join(text_blocks)\n",
    "            return self.clean_text(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if 'doc' in locals():\n",
    "                doc.close()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text while preserving structure.\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'([.!?])\\s*', r'\\1 ', text)\n",
    "        text = re.sub(r'\\s+([.!?])', r'\\1', text)\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    def create_improved_chunks(self, text):\n",
    "        \"\"\"Create chunks with improved sentence handling.\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            sentence_length = len(sentence)\n",
    "\n",
    "            if not current_chunk:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length = sentence_length\n",
    "                continue\n",
    "\n",
    "            if current_length + len(\" \") + sentence_length <= self.chunk_size:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += len(\" \") + sentence_length\n",
    "            else:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = sentence_length\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        final_chunks = []\n",
    "        for i in range(len(chunks)):\n",
    "            if i > 0:\n",
    "                prev_chunk_sentences = sent_tokenize(chunks[i-1])\n",
    "                overlap_sentences = prev_chunk_sentences[-2:] if len(prev_chunk_sentences) > 2 else prev_chunk_sentences\n",
    "                current_chunk = \" \".join(overlap_sentences) + \" \" + chunks[i]\n",
    "                final_chunks.append(current_chunk)\n",
    "            else:\n",
    "                final_chunks.append(chunks[i])\n",
    "\n",
    "        return final_chunks\n",
    "\n",
    "    def wrap_text(self, text):\n",
    "        \"\"\"Wrap text to specified line width while preserving paragraphs.\"\"\"\n",
    "        # Split text into paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        \n",
    "        # Wrap each paragraph\n",
    "        wrapped_paragraphs = []\n",
    "        for paragraph in paragraphs:\n",
    "            # Wrap the paragraph text\n",
    "            wrapped = textwrap.fill(paragraph.strip(), width=self.line_width)\n",
    "            wrapped_paragraphs.append(wrapped)\n",
    "        \n",
    "        # Join paragraphs with double newlines\n",
    "        return '\\n\\n'.join(wrapped_paragraphs)\n",
    "\n",
    "    def save_chunks_to_file(self, chunks, output_folder=\"chunks_output\"):\n",
    "        \"\"\"Save chunks to a single file with proper text wrapping.\"\"\"\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "        output_file = os.path.join(output_folder, \"output_overlap.txt\")\n",
    "        \n",
    "        try:\n",
    "            with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "                for i, chunk in enumerate(chunks, 1):\n",
    "                    # Write chunk header\n",
    "                    header = f\"Chunk {i}\"\n",
    "                    file.write(f\"{header}\\n\")\n",
    "                    file.write(\"=\"* len(header) + \"\\n\\n\")\n",
    "                    \n",
    "                    # Write wrapped chunk content\n",
    "                    wrapped_content = self.wrap_text(chunk)\n",
    "                    file.write(wrapped_content)\n",
    "                    file.write(\"\\n\\n\\n\")  # Add extra spacing between chunks\n",
    "\n",
    "            print(f\"âœ… {len(chunks)} chunks successfully saved to '{output_file}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving chunks: {e}\")\n",
    "\n",
    "    def process_pdf(self, pdf_path):\n",
    "        \"\"\"Process PDF and create better chunks.\"\"\"\n",
    "        text = self.extract_text_from_pdf(pdf_path)\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        return self.create_improved_chunks(text)\n",
    "\n",
    "    def analyze_chunks(self, chunks):\n",
    "        \"\"\"Analyze chunks with improved metrics.\"\"\"\n",
    "        if not chunks:\n",
    "            return {\n",
    "                \"total_chunks\": 0,\n",
    "                \"average_size\": 0,\n",
    "                \"size_stats\": {},\n",
    "                \"sentence_stats\": {}\n",
    "            }\n",
    "\n",
    "        chunk_sizes = [len(chunk) for chunk in chunks]\n",
    "        sentences_per_chunk = [len(sent_tokenize(chunk)) for chunk in chunks]\n",
    "\n",
    "        analysis = {\n",
    "            \"total_chunks\": len(chunks),\n",
    "            \"average_size\": sum(chunk_sizes) / len(chunks),\n",
    "            \"size_stats\": {\n",
    "                \"min\": min(chunk_sizes),\n",
    "                \"max\": max(chunk_sizes),\n",
    "                \"avg\": sum(chunk_sizes) / len(chunks)\n",
    "            },\n",
    "            \"sentence_stats\": {\n",
    "                \"min_sentences\": min(sentences_per_chunk),\n",
    "                \"max_sentences\": max(sentences_per_chunk),\n",
    "                \"avg_sentences\": sum(sentences_per_chunk) / len(sentences_per_chunk)\n",
    "            }\n",
    "        }\n",
    "        return analysis\n",
    "\n",
    "def main():\n",
    "    # Initialize chunker with line width for text wrapping\n",
    "    chunker = PDFChunker(chunk_size=1000, chunk_overlap=200, line_width=80)\n",
    "    \n",
    "    # Configuration\n",
    "    pdf_path = \"./data/tko.pdf\"\n",
    "    output_folder = \"./output_chunks/\"\n",
    "    \n",
    "    # Process PDF\n",
    "    chunks = chunker.process_pdf(pdf_path)\n",
    "    \n",
    "    if chunks:\n",
    "        # Save results\n",
    "        chunker.save_chunks_to_file(chunks, output_folder)\n",
    "        \n",
    "        # Analyze and display results\n",
    "        analysis = chunker.analyze_chunks(chunks)\n",
    "        print(\"\\nChunking Analysis:\")\n",
    "        print(f\"Total chunks: {analysis['total_chunks']}\")\n",
    "        print(f\"Average chunk size: {analysis['size_stats']['avg']:.0f} characters\")\n",
    "        print(f\"Min/Max chunk size: {analysis['size_stats']['min']}/{analysis['size_stats']['max']} characters\")\n",
    "        print(f\"\\nSentences per chunk:\")\n",
    "        print(f\"Min: {analysis['sentence_stats']['min_sentences']:.0f}\")\n",
    "        print(f\"Max: {analysis['sentence_stats']['max_sentences']:.0f}\")\n",
    "        print(f\"Average: {analysis['sentence_stats']['avg_sentences']:.1f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKEN BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Agus\n",
      "[nltk_data]     Syuhada\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Memulai proses membaca PDF...\n",
      "Teks PDF terbaca (50 karakter pertama): TATA KERJA ORGANISASI PENGELOLAAN AKUN APLIKASI No...\n",
      "ðŸ” Memulai proses chunking...\n",
      "Total chunk yang dibuat: 6\n",
      "ðŸ” Menyimpan hasil chunking ke folder output...\n",
      "âœ… Semua chunk tersimpan di folder: ./output_chunks/token_base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import fitz  # PyMuPDF\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "import logging\n",
    "import gc\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename='pdf_processing.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "class SentenceChunkProcessor:\n",
    "    def __init__(self, chunk_size=500):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    def read_pdf_and_clean(self, file_path: str) -> str:\n",
    "        logging.info(f\"Started processing PDF: {file_path}\")\n",
    "        doc = fitz.open(file_path)\n",
    "        text = \" \".join([page.get_text(\"text\") for page in doc])\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Menghindari pemisahan angka yang salah (4. 3.)\n",
    "        text = re.sub(r'(\\d+)\\.\\s(?=\\d+)', r'\\1|', text)\n",
    "\n",
    "        logging.info(f\"Successfully read and cleaned PDF: {file_path}\")\n",
    "        return text\n",
    "\n",
    "    def restore_numbering(self, text: str) -> str:\n",
    "        \"\"\"Mengembalikan angka yang sebelumnya diganti (4|3 -> 4. 3.)\"\"\"\n",
    "        return text.replace('|', '. ')\n",
    "\n",
    "    def chunk_text_by_sentences(self, text: str) -> List[Tuple[str, int]]:\n",
    "        text = self.restore_numbering(text)  # Kembalikan format angka\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks = []\n",
    "        current_tokens = []\n",
    "        current_text = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.encoder.encode(sentence)\n",
    "            \n",
    "            if len(current_tokens) + len(sentence_tokens) > self.chunk_size:\n",
    "                if current_tokens:\n",
    "                    chunks.append((self.encoder.decode(current_tokens), len(current_tokens)))\n",
    "                    current_tokens = []  # Reset tanpa overlap\n",
    "                    current_text = \"\"\n",
    "            \n",
    "            current_tokens.extend(sentence_tokens)\n",
    "            current_text += sentence\n",
    "            \n",
    "            gc.collect()\n",
    "        \n",
    "        if current_tokens:\n",
    "            chunks.append((self.encoder.decode(current_tokens), len(current_tokens)))\n",
    "\n",
    "        logging.info(f\"Processed and chunked text into {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "\n",
    "    def save_chunks(self, content_chunks: List[Tuple[str, int]], output_path: str) -> None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = f\"{os.path.splitext(output_path)[0]}_{timestamp}.txt\"\n",
    "\n",
    "        try:\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Processing Date: {datetime.now()}\\n\")\n",
    "                f.write(f\"Chunk Size: {self.chunk_size} tokens\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "                for i, (chunk, token_count) in enumerate(content_chunks, 1):\n",
    "                    f.write(f\"=== Chunk {i} ===\\n\")\n",
    "                    f.write(f\"Tokens: {token_count}\\n\")\n",
    "                    f.write(chunk + \"\\n\\n\")\n",
    "                    f.write(\"-\" * 30 + \"\\n\\n\")\n",
    "\n",
    "            logging.info(f\"Successfully saved chunks to: {output_path}\")\n",
    "            logging.info(f\"Total chunks: {len(content_chunks)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving chunks: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"./data/tko.pdf\"\n",
    "    output_path = \"./output_chunks/output_token_base.txt\"\n",
    "\n",
    "    processor = SentenceChunkProcessor(chunk_size=500)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File '{file_path}' tidak ditemukan!\")\n",
    "    else:\n",
    "        try:\n",
    "            print(\"ðŸ” Memulai proses membaca PDF...\")\n",
    "            text = processor.read_pdf_and_clean(file_path)\n",
    "            print(f\"Teks PDF terbaca (50 karakter pertama): {text[:50]}...\")\n",
    "\n",
    "            print(\"ðŸ” Memulai proses chunking...\")\n",
    "            content_chunks = processor.chunk_text_by_sentences(text)\n",
    "            print(f\"Total chunk yang dibuat: {len(content_chunks)}\")\n",
    "\n",
    "            print(\"ðŸ” Menyimpan hasil chunking ke output file...\")\n",
    "            processor.save_chunks(content_chunks, output_path)\n",
    "            print(f\"âœ… File output tersimpan di: {output_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Terjadi error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTEXT ENRICHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Memproses file: dokumen_pdf.pdf\n",
      "==================================================\n",
      "Hasil chunking disimpan di: output_chunks\\dokumen_pdf.pdf_chunks.txt\n",
      "\n",
      "==================================================\n",
      "Memproses file: tko.pdf\n",
      "==================================================\n",
      "Hasil chunking disimpan di: output_chunks\\tko.pdf_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "class AdvancedChunking:\n",
    "    def __init__(self, ukuran_chunk=1000, overlap=200):\n",
    "        self.ukuran_chunk = ukuran_chunk\n",
    "        self.overlap = overlap\n",
    "        self.output_folder = 'output_chunks'\n",
    "        os.makedirs(self.output_folder, exist_ok=True)\n",
    "\n",
    "    def baca_pdf(self, path_file):\n",
    "        \"\"\"Membaca teks dari file PDF.\"\"\"\n",
    "        pdf_reader = PdfReader(path_file)\n",
    "        teks = \"\".join([halaman.extract_text() or \"\" for halaman in pdf_reader.pages])\n",
    "        return teks\n",
    "\n",
    "    def preprocessing_teks(self, teks):\n",
    "        \"\"\"Membersihkan teks dari spasi berlebih dan karakter tidak perlu.\"\"\"\n",
    "        return re.sub(r'\\s+', ' ', teks).strip()\n",
    "\n",
    "    def chunk_dengan_konteks(self, teks):\n",
    "        \"\"\"Membagi teks menjadi chunk dengan mempertahankan sedikit konteks dari chunk sebelumnya dan berikutnya.\"\"\"\n",
    "        teks_bersih = self.preprocessing_teks(teks)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.ukuran_chunk, chunk_overlap=self.overlap, length_function=len,\n",
    "            separators=[\".\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "\n",
    "        chunks = text_splitter.create_documents([teks_bersih])\n",
    "        chunks_dengan_konteks = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            konteks_sebelum = chunks[i - 1].page_content if i > 0 else \"\"\n",
    "            konteks_sesudah = chunks[i + 1].page_content if i < len(chunks) - 1 else \"\"\n",
    "\n",
    "            chunk_konteks = f\"{konteks_sebelum} {chunk.page_content} {konteks_sesudah}\".strip()\n",
    "            chunk_konteks = chunk_konteks.rstrip('.') + '.'\n",
    "\n",
    "            metadata = {\n",
    "                'chunk_id': i,\n",
    "                'total_chunks': len(chunks),\n",
    "                'panjang_chunk': len(chunk_konteks),\n",
    "                'konteks_sebelum': konteks_sebelum[:50],\n",
    "                'konteks_sesudah': konteks_sesudah[:50]\n",
    "            }\n",
    "\n",
    "            chunks_dengan_konteks.append({'konten': chunk_konteks, 'metadata': metadata})\n",
    "\n",
    "        return chunks_dengan_konteks\n",
    "\n",
    "    def proses_folder_pdf(self, folder_path):\n",
    "        \"\"\"Memproses semua file PDF dalam folder dan membaginya menjadi chunk.\"\"\"\n",
    "        hasil = {}\n",
    "        total_chunk = 0\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.pdf'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Memproses file: {filename}\")\n",
    "                print(f\"{'='*50}\")\n",
    "\n",
    "                teks_pdf = self.baca_pdf(file_path)\n",
    "                chunks = self.chunk_dengan_konteks(teks_pdf)\n",
    "\n",
    "                total_chunk += len(chunks)\n",
    "                hasil[filename] = {'chunks': chunks}\n",
    "\n",
    "                # Menyimpan hasil chunking ke file teks\n",
    "                chunk_file = os.path.join(self.output_folder, f\"{filename}_chunks.txt\")\n",
    "                with open(chunk_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    for i, chunk in enumerate(chunks, 1):\n",
    "                        f.write(f\"Chunk {i}:\\n{chunk['konten']}\\n\\n\")\n",
    "\n",
    "                print(f\"Hasil chunking disimpan di: {chunk_file}\")\n",
    "\n",
    "        return hasil\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chunker = AdvancedChunking(ukuran_chunk=1000, overlap=200)\n",
    "    FOLDER_PATH = \"./data\"\n",
    "\n",
    "    try:\n",
    "        hasil_chunking = chunker.proses_folder_pdf(FOLDER_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEMANTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil telah disimpan dalam ./output_chunks/output_semantic.txt\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Membaca teks dari file PDF.\"\"\"\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "    return text\n",
    "\n",
    "# def clean_text(text):\n",
    "#     \"\"\"Membersihkan teks dari karakter khusus, spasi berlebih, dan format yang tidak diperlukan.\"\"\"\n",
    "#     text = re.sub(r'\\n+', '\\n', text)  # Menghapus newline berlebih\n",
    "#     text = re.sub(r'[^a-zA-Z0-9.,\\n ]+', '', text)  # Menghapus karakter khusus\n",
    "#     text = re.sub(r' +', ' ', text)  # Menghapus spasi berlebih\n",
    "#     return text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Membersihkan teks dari karakter khusus, spasi berlebih, dan format yang tidak diperlukan.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip() \n",
    "    text = \" \".join(text.split())  \n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    \"\"\"Membagi teks menjadi chunk lebih kecil berdasarkan ukuran karakter.\"\"\"\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def save_to_txt(filename, chunks):\n",
    "    \"\"\"Menyimpan hasil ke file .txt.\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(f\"Chunk {i+1}:\\n{chunk}\\n\\n\")\n",
    "\n",
    "# Path ke file PDF\n",
    "pdf_path = \"./data/tko.pdf\"\n",
    "output_txt = \"./output_chunks/output_semantic.txt\"\n",
    "\n",
    "# Membaca, membersihkan, dan membagi teks\n",
    "raw_text = read_pdf(pdf_path)\n",
    "cleaned_text = clean_text(raw_text)\n",
    "chunks = chunk_text(cleaned_text)\n",
    "\n",
    "# Menyimpan output ke file .txt\n",
    "save_to_txt(output_txt, chunks)\n",
    "\n",
    "print(f\"Hasil telah disimpan dalam {output_txt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADVANCE SEMANTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document: ./data\\.gitkeep\n",
      "Unsupported file format: gitkeep\n",
      "Processing document: ./data\\dokumen_pdf.pdf\n",
      "Reading PDF file: ./data\\dokumen_pdf.pdf\n",
      "Preprocessing text: Cleaning and normalizing...\n",
      "Performing semantic chunking...\n",
      "Generated 5 chunks.\n",
      "Processing chunk 1...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 2...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 3...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 4...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 5...\n",
      "Extracting topics using LDA...\n",
      "Finished processing: ./data\\dokumen_pdf.pdf -> ./output_chunks/dokumen_pdf_chunks.txt\n",
      "Processing document: ./data\\tko.pdf\n",
      "Reading PDF file: ./data\\tko.pdf\n",
      "Preprocessing text: Cleaning and normalizing...\n",
      "Performing semantic chunking...\n",
      "Generated 10 chunks.\n",
      "Processing chunk 1...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 2...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 3...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 4...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 5...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 6...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 7...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 8...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 9...\n",
      "Extracting topics using LDA...\n",
      "Processing chunk 10...\n",
      "Extracting topics using LDA...\n",
      "Finished processing: ./data\\tko.pdf -> ./output_chunks/tko_chunks.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import docx\n",
    "import fitz\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "llm = OllamaLLM(model=OLLAMA_MODEL)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def read_txt(file_path):\n",
    "    print(f\"Reading TXT file: {file_path}\")\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    print(f\"Reading PDF file: {file_path}\")\n",
    "    text = \"\"\n",
    "    with fitz.open(file_path) as doc:\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def read_docx(file_path):\n",
    "    print(f\"Reading DOCX file: {file_path}\")\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def clean_text(text):\n",
    "    print(\"Preprocessing text: Cleaning and normalizing...\")\n",
    "    text = text.replace(\"\\n\", \" \").strip() \n",
    "    text = \" \".join(text.split())  \n",
    "    return text\n",
    "\n",
    "def semantic_chunking(text, chunk_size=1000):\n",
    "    print(\"Performing semantic chunking...\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(chunk) + len(sentence) < chunk_size:\n",
    "            chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(chunk.strip())\n",
    "            chunk = sentence\n",
    "    if chunk:\n",
    "        chunks.append(chunk.strip())\n",
    "    \n",
    "    print(f\"Generated {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def extract_topics(text, num_topics=5):\n",
    "    print(\"Extracting topics using LDA...\")\n",
    "    words = [token.lemma_ for token in nlp(text) if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([words])\n",
    "    corpus = [dictionary.doc2bow(words)]\n",
    "    lda_model = gensim.models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4)\n",
    "    topics = lda_model.show_topics(formatted=False)\n",
    "    \n",
    "    return topics[0] if topics else None\n",
    "\n",
    "def process_document(file_path, output_folder):\n",
    "    print(f\"Processing document: {file_path}\")\n",
    "    ext = file_path.split(\".\")[-1].lower()\n",
    "    \n",
    "    if ext == \"txt\":\n",
    "        text = read_txt(file_path)\n",
    "    elif ext == \"pdf\":\n",
    "        text = read_pdf(file_path)\n",
    "    elif ext == \"docx\":\n",
    "        text = read_docx(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {ext}\")\n",
    "        return\n",
    "    \n",
    "    cleaned_text = clean_text(text)\n",
    "    chunks = semantic_chunking(cleaned_text)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, os.path.basename(file_path).split(\".\")[0] + \"_chunks.txt\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Processing chunk {i+1}...\")\n",
    "            best_topic = extract_topics(chunk)\n",
    "            if best_topic:\n",
    "                topic_words = \", \".join([word for word, _ in best_topic[1]])\n",
    "                topic_str = f\"Best Topic: {topic_words}\"\n",
    "            else:\n",
    "                topic_str = \"Best Topic: No topics found\"\n",
    "            f.write(f\"--- Chunk {i+1} ---\\n{chunk}\\n{topic_str}\\n\\n\")\n",
    "    \n",
    "    print(f\"Finished processing: {file_path} -> {output_file}\")\n",
    "\n",
    "def main():\n",
    "    input_folder = \"./data\"\n",
    "    output_folder = \"./output_chunks/\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    for file_name in os.listdir(input_folder):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            process_document(file_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGENTIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] dokumen_pdf.pdf - Panjang teks sebelum pemrosesan: 3843\n",
      "[INFO] Total chunks generated for dokumen_pdf.pdf: 5\n",
      "[INFO] tko.pdf - Panjang teks sebelum pemrosesan: 8838\n",
      "[INFO] Total chunks generated for tko.pdf: 10\n",
      "[SUCCESS] Proses chunking selesai. Hasilnya disimpan dalam 'chunked_data' dan metadata di 'metadata'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from IPython.display import Markdown\n",
    "from phi.agent import Agent\n",
    "from phi.model.ollama import Ollama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from utils.document_processor import DocumentProcessor  \n",
    "\n",
    "# Inisialisasi path dan model\n",
    "DATA_PATH = \"./data\"\n",
    "INDEX_PATH = \"faiss_index\"\n",
    "CHUNKED_DATA_PATH = \"./output_chunks\"  \n",
    "METADATA_PATH = \"./metadata\"  \n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "\n",
    "# Pastikan folder tersedia\n",
    "os.makedirs(CHUNKED_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(METADATA_PATH, exist_ok=True)\n",
    "\n",
    "# Inisialisasi model Ollama\n",
    "llm = Ollama(id=OLLAMA_MODEL)\n",
    "\n",
    "# Inisialisasi embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Inisialisasi document processor\n",
    "docs = DocumentProcessor()\n",
    "\n",
    "# Inisialisasi Agent dengan model Ollama\n",
    "agent = Agent(model=llm, show_tool_calls=True, markdown=True)\n",
    "\n",
    "# Parameter chunking\n",
    "CHUNK_SIZE = 1200  # Ditingkatkan untuk efisiensi\n",
    "MIN_CHUNK_SIZE = 500  # Gabungkan chunk kecil\n",
    "MAX_CHUNKS = 30  # Batasi jumlah chunk\n",
    "\n",
    "extracted_docs = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" Membersihkan teks dari karakter kosong, whitespace berlebih, dan simbol aneh \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  \n",
    "    return text\n",
    "\n",
    "def clean_agent_output(text):\n",
    "    \"\"\" Membersihkan output dari agent agar tidak mengandung pemisah yang tidak perlu \"\"\"\n",
    "    text = re.sub(r'\\n?###.*?\\n', '\\n', text)  # Hapus judul markdown seperti ### Section\n",
    "    text = re.sub(r'\\n?\\*\\*\\*.*?\\n', '\\n', text)  # Hapus pemisah ***\n",
    "    text = re.sub(r'\\n?-{3,}\\n?', '\\n', text)  # Hapus garis pemisah ---\n",
    "    text = re.sub(r'(\\s*-{2,}\\s*)', ' ', text)  # Hapus pemisah --\n",
    "    text = re.sub(r'(\\s*\\*{2,}\\s*)', ' ', text)  # Hapus ** pemisah tebal\n",
    "    text = re.sub(r'(\\s*\\*\\s*)', ' ', text)  # Hapus * pemisah tunggal\n",
    "    text = re.sub(r'(\\s*-\\s*)', ' ', text)  # Hapus pemisah -\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text).strip() \n",
    "    return text\n",
    "\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    valid_extensions = ('.pdf', '.docx', '.txt')\n",
    "    if not filename.lower().endswith(valid_extensions):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(DATA_PATH, filename)\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            document = f.read()\n",
    "            result = docs.process_document(document, filename)\n",
    "\n",
    "        if not result or len(result) < 4:\n",
    "            print(f\"[WARNING] Gagal memproses {filename}, melewati file ini.\")\n",
    "            continue\n",
    "\n",
    "        plain_text = clean_text(result[3])  \n",
    "        print(f\"[INFO] {filename} - Panjang teks sebelum pemrosesan: {len(plain_text)}\")\n",
    "\n",
    "        # Gunakan agent untuk chunking secara bertahap\n",
    "        structured_text = \"\"\n",
    "        start_idx = 0\n",
    "        chunk_count = 0\n",
    "\n",
    "        while start_idx < len(plain_text) and chunk_count < MAX_CHUNKS:\n",
    "            chunk_text = plain_text[start_idx:start_idx + CHUNK_SIZE]\n",
    "            response = agent.run(\n",
    "                f\"Split the following text into meaningful segments ensuring logical separation:\\n{chunk_text}\",\n",
    "                max_tokens=8000\n",
    "            )\n",
    "            \n",
    "            if isinstance(response, str):\n",
    "                structured_text += clean_agent_output(response) + \"\\n\\n\"\n",
    "            elif isinstance(response, dict):\n",
    "                structured_text += clean_agent_output(response.get(\"text\", \"\")) + \"\\n\\n\"\n",
    "            else:\n",
    "                structured_text += clean_agent_output(getattr(response, \"content\", str(response))) + \"\\n\\n\"\n",
    "            \n",
    "            start_idx += CHUNK_SIZE\n",
    "            chunk_count += 1\n",
    "\n",
    "        structured_text = structured_text.strip()\n",
    "        chunked_texts = structured_text.split(\"\\n\\n\")\n",
    "\n",
    "        # Gabungkan chunk yang terlalu pendek\n",
    "        optimized_chunks = []\n",
    "        temp_chunk = \"\"\n",
    "\n",
    "        for chunk in chunked_texts:\n",
    "            chunk = chunk.strip()\n",
    "            if len(chunk) < MIN_CHUNK_SIZE:\n",
    "                temp_chunk += \" \" + chunk\n",
    "            else:\n",
    "                if temp_chunk:\n",
    "                    optimized_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = \"\"\n",
    "                optimized_chunks.append(chunk)\n",
    "\n",
    "        if temp_chunk:\n",
    "            optimized_chunks.append(temp_chunk.strip())\n",
    "\n",
    "        chunk_data = [{\"chunk_id\": i+1, \"text\": chunk.strip()} \n",
    "                      for i, chunk in enumerate(optimized_chunks[:MAX_CHUNKS]) if chunk.strip()]\n",
    "\n",
    "        metadata = {\n",
    "            \"filename\": filename,\n",
    "            \"total_chunks\": len(chunk_data),\n",
    "            \"total_length\": len(plain_text)\n",
    "        }\n",
    "\n",
    "        extracted_docs.extend([\n",
    "            Document(page_content=chunk[\"text\"], metadata={\"chunk_id\": chunk[\"chunk_id\"], **metadata}) \n",
    "            for chunk in chunk_data\n",
    "        ])\n",
    "\n",
    "        # Simpan hasil chunking ke file\n",
    "        chunked_filepath = os.path.join(CHUNKED_DATA_PATH, f\"chunked_{filename}.txt\")\n",
    "        with open(chunked_filepath, \"w\", encoding=\"utf-8\") as chunked_file:\n",
    "            for chunk in chunk_data:\n",
    "                chunked_file.write(f\"Chunk {chunk['chunk_id']}:\\n\")\n",
    "                chunked_file.write(f\"{chunk['text']}\\n\")\n",
    "                chunked_file.write(\"\\n---\\n\\n\")  \n",
    "\n",
    "        metadata_filepath = os.path.join(METADATA_PATH, f\"metadata_{filename}.json\")\n",
    "        with open(metadata_filepath, \"w\", encoding=\"utf-8\") as metadata_file:\n",
    "            json.dump(metadata, metadata_file, indent=4)\n",
    "\n",
    "        print(f\"[INFO] Total chunks generated for {filename}: {len(chunk_data)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error processing {filename}: {e}\")\n",
    "\n",
    "# Simpan ke FAISS hanya jika ada dokumen yang diproses\n",
    "if extracted_docs:\n",
    "    vector_store = FAISS.from_documents(extracted_docs, embedding_model)\n",
    "    vector_store.save_local(INDEX_PATH)\n",
    "    print(\"[SUCCESS] Proses chunking selesai. Hasilnya disimpan dalam 'chunked_data' dan metadata di 'metadata'.\")\n",
    "else:\n",
    "    print(\"[INFO] Tidak ada dokumen yang berhasil diproses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided text, I'll answer your questions:\n",
       "\n",
       "1. Apa tujuan dari tata kerja organisasi?\n",
       "\n",
       "The purpose of organizational workflow is to manage and control the processes within an organization. In this case, it seems that the main goal is to ensure proper management of user access and account creation in an IT system.\n",
       "\n",
       "2. Apa apakah yang dimaksud dengan User Access Review (UAR)?\n",
       "\n",
       "From the text, it appears that UAR refers to a review process for new account requests or updates to existing accounts. This process involves various roles, such as Application Programmer, Technical Writer, Information Steward, and Business Analyst, working together to ensure that user access is properly managed.\n",
       "\n",
       "In summary:\n",
       "\n",
       "* The organizational workflow aims to manage and control user access in an IT system.\n",
       "* User Access Review (UAR) is a process for reviewing new account requests or updates to existing accounts, involving multiple roles."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Markdown\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "\n",
    "# Load LLM model\n",
    "model = OllamaLLM(model=OLLAMA_MODEL)\n",
    "\n",
    "# Template prompt untuk ringkasan\n",
    "template = \"\"\"\n",
    "1. Apa tujuan dari tata kerja organisasi?\n",
    "2. Apa pengertian Onsite Support dan Aplikasi Upstream?\n",
    "3. Apa dokumen dan referensi terkait Tata Kerja Organisasi?\n",
    "4. Bagaimana prosedur permintaan baru akun aplikasi?\n",
    "Gunakan hanya informasi yang terdapat dalam dokumen.  \n",
    "\n",
    "Dokumen:\n",
    "\"{document}\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def summarize_text(text):\n",
    "    \"\"\"Meringkas teks menggunakan model LLM\"\"\"\n",
    "    chain = prompt | model\n",
    "    response = chain.invoke({\"document\": text})\n",
    "    return response\n",
    "\n",
    "def read_all_chunks(folder):\n",
    "    \"\"\"Membaca semua file chunk dalam folder dan menggabungkan isinya\"\"\"\n",
    "    all_text = []\n",
    "    \n",
    "    # Ambil semua file dalam folder dan urutkan berdasarkan nama file\n",
    "    files = sorted([f for f in os.listdir(folder) if f.startswith(\"chunk_\") and f.endswith(\".txt\")])\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_text.append(f.read())  # Tambahkan isi file ke list\n",
    "    \n",
    "    return \"\\n\\n\".join(all_text)  # Gabungkan semua isi file dengan pemisah newline\n",
    "\n",
    "# Baca semua chunk dari folder\n",
    "output_folder = \"output_chunks/recursive\"\n",
    "document_text = read_all_chunks(output_folder)\n",
    "\n",
    "# Buat ringkasan\n",
    "summary = summarize_text(document_text)\n",
    "\n",
    "# Tampilkan hasil sebagai Markdown\n",
    "Markdown(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
