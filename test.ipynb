{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESS DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from utils.document_processor import DocumentProcessor\n",
    "import os\n",
    "\n",
    "docs = DocumentProcessor()\n",
    "\n",
    "DATA_PATH = \"./data\"\n",
    "extracted_docs = []\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "  valid_extensions = ('.pdf', '.docx', '.txt')\n",
    "  if not filename.lower().endswith(valid_extensions):\n",
    "    continue # exclude not supported file type\n",
    "  \n",
    "  filepath = os.path.join(DATA_PATH, filename)\n",
    "  with open(filepath, \"rb\") as f:\n",
    "    document = f.read()\n",
    "    result = docs.process_document(document, filename)\n",
    "    extracted_docs.append(result)\n",
    "    \n",
    "content, images, tables, plain_text = extracted_docs[0] #1st file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST OLLAMA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "COLLECTION_NAME = \"ollama_vectore_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "  You are a helpful assistant for text summarization. \n",
    "  Only include information that is part of the document. \n",
    "  Do not include your own opinion or analysis.\n",
    "\n",
    "  Document: \n",
    "  \"{document}\"\n",
    "  Summary:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=OLLAMA_MODEL)\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"document\": plain_text})\n",
    "Markdown(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST OPENAI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import openai\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "model = \"gpt-35-turbo\"\n",
    "response = openai.ChatCompletion.create(\n",
    "  engine=model,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\", \n",
    "      \"content\": \"\"\"\n",
    "        You are a helpful assistant for text summarization.\n",
    "        Only include information that is part of the document. \n",
    "        Do not include your own opinion or analysis.\n",
    "      \"\"\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": plain_text\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "Markdown(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
